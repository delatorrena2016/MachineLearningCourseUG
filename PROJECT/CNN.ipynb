{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:(8163, 8, 8), and categorical beta:(8163, 100), beta:(10000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('conf.txt').reshape(-1, 8, 8)\n",
    "Temp = np.loadtxt('invBeta.txt')\n",
    "beta = 1 / Temp  # beta\n",
    "N = 100          # category number\n",
    "range = 1./(N-2) # interval range\n",
    "# hot_one representation of size (beta samples x total category)\n",
    "beta_hot = np.empty((len(beta), N)) \n",
    "for count, beta_n in enumerate(beta):\n",
    "    countt = 0\n",
    "    hot_one = np.zeros(N)\n",
    "    for inf in np.arange(0, 0.99, range): #por ser ranchero, para que no sobrepase el intervalo de 1.\n",
    "        if beta_n < 0:\n",
    "            hot_one[0] = 1\n",
    "        elif inf <= beta_n < (inf + range):\n",
    "            hot_one[countt + 1] = 1 #there's a previous category of neg values\n",
    "        elif beta_n >= 1:\n",
    "            hot_one[N-1] = 1\n",
    "        countt+=1\n",
    "    beta_hot[count] = hot_one\n",
    "\n",
    "data, beta_hot = data[1837:].astype(int), beta_hot[1837:].astype(int)\n",
    "print('data shape:{}, and categorical beta:{}, beta:{}'.format(data.shape, beta_hot.shape, beta.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20001960192098825 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "a = -1\n",
    "print(beta[a],beta_hot[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with the whole dataset: (8163, 100)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 1)           10        \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               1700      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710\n",
      "Trainable params: 1,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 4.2833 - accuracy: 0.0282\n",
      "Epoch 1: loss improved from inf to 4.28333, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 4.2833 - accuracy: 0.0282 - val_loss: 3.9581 - val_accuracy: 0.0447\n",
      "Epoch 2/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.9154 - accuracy: 0.0486\n",
      "Epoch 2: loss improved from 4.28333 to 3.91677, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.9168 - accuracy: 0.0480 - val_loss: 3.8283 - val_accuracy: 0.0521\n",
      "Epoch 3/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.8391 - accuracy: 0.0487\n",
      "Epoch 3: loss improved from 3.91677 to 3.83700, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.8370 - accuracy: 0.0489 - val_loss: 3.7610 - val_accuracy: 0.0570\n",
      "Epoch 4/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.7782 - accuracy: 0.0536\n",
      "Epoch 4: loss improved from 3.83700 to 3.77977, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.7798 - accuracy: 0.0532 - val_loss: 3.7088 - val_accuracy: 0.0680\n",
      "Epoch 5/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.7382 - accuracy: 0.0561\n",
      "Epoch 5: loss improved from 3.77977 to 3.73798, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.7380 - accuracy: 0.0564 - val_loss: 3.6742 - val_accuracy: 0.0680\n",
      "Epoch 6/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.7077 - accuracy: 0.0581\n",
      "Epoch 6: loss improved from 3.73798 to 3.70766, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.7077 - accuracy: 0.0581 - val_loss: 3.6483 - val_accuracy: 0.0723\n",
      "Epoch 7/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.6843 - accuracy: 0.0602\n",
      "Epoch 7: loss improved from 3.70766 to 3.68442, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.6844 - accuracy: 0.0601 - val_loss: 3.6297 - val_accuracy: 0.0698\n",
      "Epoch 8/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.6667 - accuracy: 0.0610\n",
      "Epoch 8: loss improved from 3.68442 to 3.66624, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.6662 - accuracy: 0.0614 - val_loss: 3.6131 - val_accuracy: 0.0716\n",
      "Epoch 9/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.6509 - accuracy: 0.0624\n",
      "Epoch 9: loss improved from 3.66624 to 3.65069, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.6507 - accuracy: 0.0626 - val_loss: 3.5986 - val_accuracy: 0.0729\n",
      "Epoch 10/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.6390 - accuracy: 0.0651\n",
      "Epoch 10: loss improved from 3.65069 to 3.63700, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.6370 - accuracy: 0.0650 - val_loss: 3.5867 - val_accuracy: 0.0747\n",
      "Epoch 11/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.6268 - accuracy: 0.0666\n",
      "Epoch 11: loss improved from 3.63700 to 3.62549, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.6255 - accuracy: 0.0671 - val_loss: 3.5781 - val_accuracy: 0.0753\n",
      "Epoch 12/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.6162 - accuracy: 0.0674\n",
      "Epoch 12: loss improved from 3.62549 to 3.61602, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.6160 - accuracy: 0.0673 - val_loss: 3.5703 - val_accuracy: 0.0784\n",
      "Epoch 13/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.6066 - accuracy: 0.0673\n",
      "Epoch 13: loss improved from 3.61602 to 3.60695, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.6070 - accuracy: 0.0673 - val_loss: 3.5614 - val_accuracy: 0.0765\n",
      "Epoch 14/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.5995 - accuracy: 0.0678\n",
      "Epoch 14: loss improved from 3.60695 to 3.59928, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.5993 - accuracy: 0.0679 - val_loss: 3.5546 - val_accuracy: 0.0765\n",
      "Epoch 15/5000\n",
      "235/256 [==========================>...] - ETA: 0s - loss: 3.5958 - accuracy: 0.0666\n",
      "Epoch 15: loss improved from 3.59928 to 3.59200, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.5920 - accuracy: 0.0669 - val_loss: 3.5482 - val_accuracy: 0.0759\n",
      "Epoch 16/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.5852 - accuracy: 0.0702\n",
      "Epoch 16: loss improved from 3.59200 to 3.58535, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5853 - accuracy: 0.0699 - val_loss: 3.5411 - val_accuracy: 0.0833\n",
      "Epoch 17/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.5787 - accuracy: 0.0708\n",
      "Epoch 17: loss improved from 3.58535 to 3.57937, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.5794 - accuracy: 0.0704 - val_loss: 3.5361 - val_accuracy: 0.0778\n",
      "Epoch 18/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.5732 - accuracy: 0.0694\n",
      "Epoch 18: loss improved from 3.57937 to 3.57296, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5730 - accuracy: 0.0696 - val_loss: 3.5316 - val_accuracy: 0.0821\n",
      "Epoch 19/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.5677 - accuracy: 0.0710\n",
      "Epoch 19: loss improved from 3.57296 to 3.56767, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5677 - accuracy: 0.0706 - val_loss: 3.5249 - val_accuracy: 0.0765\n",
      "Epoch 20/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.5617 - accuracy: 0.0699\n",
      "Epoch 20: loss improved from 3.56767 to 3.56171, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5617 - accuracy: 0.0711 - val_loss: 3.5214 - val_accuracy: 0.0759\n",
      "Epoch 21/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.5574 - accuracy: 0.0684\n",
      "Epoch 21: loss improved from 3.56171 to 3.55659, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.5566 - accuracy: 0.0690 - val_loss: 3.5201 - val_accuracy: 0.0710\n",
      "Epoch 22/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.5457 - accuracy: 0.0736\n",
      "Epoch 22: loss improved from 3.55659 to 3.55134, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5513 - accuracy: 0.0726 - val_loss: 3.5105 - val_accuracy: 0.0753\n",
      "Epoch 23/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.5469 - accuracy: 0.0715\n",
      "Epoch 23: loss improved from 3.55134 to 3.54656, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5466 - accuracy: 0.0714 - val_loss: 3.5060 - val_accuracy: 0.0759\n",
      "Epoch 24/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.5417 - accuracy: 0.0720\n",
      "Epoch 24: loss improved from 3.54656 to 3.54172, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5417 - accuracy: 0.0714 - val_loss: 3.5018 - val_accuracy: 0.0808\n",
      "Epoch 25/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.5359 - accuracy: 0.0696\n",
      "Epoch 25: loss improved from 3.54172 to 3.53686, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.5369 - accuracy: 0.0706 - val_loss: 3.4968 - val_accuracy: 0.0784\n",
      "Epoch 26/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.5302 - accuracy: 0.0707\n",
      "Epoch 26: loss improved from 3.53686 to 3.53206, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.5321 - accuracy: 0.0701 - val_loss: 3.4937 - val_accuracy: 0.0796\n",
      "Epoch 27/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.5272 - accuracy: 0.0716\n",
      "Epoch 27: loss improved from 3.53206 to 3.52717, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5272 - accuracy: 0.0715 - val_loss: 3.4886 - val_accuracy: 0.0759\n",
      "Epoch 28/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.5233 - accuracy: 0.0724\n",
      "Epoch 28: loss improved from 3.52717 to 3.52222, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5222 - accuracy: 0.0720 - val_loss: 3.4840 - val_accuracy: 0.0759\n",
      "Epoch 29/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.5167 - accuracy: 0.0708\n",
      "Epoch 29: loss improved from 3.52222 to 3.51728, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5173 - accuracy: 0.0711 - val_loss: 3.4790 - val_accuracy: 0.0802\n",
      "Epoch 30/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.5120 - accuracy: 0.0728\n",
      "Epoch 30: loss improved from 3.51728 to 3.51185, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.5118 - accuracy: 0.0730 - val_loss: 3.4759 - val_accuracy: 0.0759\n",
      "Epoch 31/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.5076 - accuracy: 0.0737\n",
      "Epoch 31: loss improved from 3.51185 to 3.50694, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.5069 - accuracy: 0.0735 - val_loss: 3.4706 - val_accuracy: 0.0802\n",
      "Epoch 32/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.5047 - accuracy: 0.0733\n",
      "Epoch 32: loss improved from 3.50694 to 3.50337, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.5034 - accuracy: 0.0736 - val_loss: 3.4671 - val_accuracy: 0.0790\n",
      "Epoch 33/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4993 - accuracy: 0.0760\n",
      "Epoch 33: loss improved from 3.50337 to 3.49934, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4993 - accuracy: 0.0760 - val_loss: 3.4641 - val_accuracy: 0.0802\n",
      "Epoch 34/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.4958 - accuracy: 0.0747\n",
      "Epoch 34: loss improved from 3.49934 to 3.49670, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4967 - accuracy: 0.0746 - val_loss: 3.4610 - val_accuracy: 0.0802\n",
      "Epoch 35/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4914 - accuracy: 0.0743\n",
      "Epoch 35: loss improved from 3.49670 to 3.49311, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4931 - accuracy: 0.0737 - val_loss: 3.4580 - val_accuracy: 0.0802\n",
      "Epoch 36/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.4892 - accuracy: 0.0745\n",
      "Epoch 36: loss improved from 3.49311 to 3.48977, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.4898 - accuracy: 0.0747 - val_loss: 3.4549 - val_accuracy: 0.0784\n",
      "Epoch 37/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4887 - accuracy: 0.0748\n",
      "Epoch 37: loss improved from 3.48977 to 3.48685, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.4869 - accuracy: 0.0750 - val_loss: 3.4519 - val_accuracy: 0.0796\n",
      "Epoch 38/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.4830 - accuracy: 0.0756\n",
      "Epoch 38: loss improved from 3.48685 to 3.48389, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4839 - accuracy: 0.0752 - val_loss: 3.4492 - val_accuracy: 0.0821\n",
      "Epoch 39/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.4801 - accuracy: 0.0740\n",
      "Epoch 39: loss improved from 3.48389 to 3.48126, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4813 - accuracy: 0.0748 - val_loss: 3.4457 - val_accuracy: 0.0839\n",
      "Epoch 40/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4784 - accuracy: 0.0746\n",
      "Epoch 40: loss improved from 3.48126 to 3.47838, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.4784 - accuracy: 0.0746 - val_loss: 3.4439 - val_accuracy: 0.0827\n",
      "Epoch 41/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.4755 - accuracy: 0.0750\n",
      "Epoch 41: loss improved from 3.47838 to 3.47569, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4757 - accuracy: 0.0755 - val_loss: 3.4424 - val_accuracy: 0.0778\n",
      "Epoch 42/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.4715 - accuracy: 0.0755\n",
      "Epoch 42: loss improved from 3.47569 to 3.47263, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.4726 - accuracy: 0.0746 - val_loss: 3.4383 - val_accuracy: 0.0821\n",
      "Epoch 43/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.4696 - accuracy: 0.0733\n",
      "Epoch 43: loss improved from 3.47263 to 3.46960, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.4696 - accuracy: 0.0733 - val_loss: 3.4356 - val_accuracy: 0.0808\n",
      "Epoch 44/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.4681 - accuracy: 0.0746\n",
      "Epoch 44: loss improved from 3.46960 to 3.46675, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.4668 - accuracy: 0.0746 - val_loss: 3.4338 - val_accuracy: 0.0765\n",
      "Epoch 45/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.4643 - accuracy: 0.0749\n",
      "Epoch 45: loss improved from 3.46675 to 3.46423, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.4642 - accuracy: 0.0746 - val_loss: 3.4310 - val_accuracy: 0.0814\n",
      "Epoch 46/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.4608 - accuracy: 0.0770\n",
      "Epoch 46: loss improved from 3.46423 to 3.46160, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.4616 - accuracy: 0.0757 - val_loss: 3.4279 - val_accuracy: 0.0833\n",
      "Epoch 47/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.4594 - accuracy: 0.0741\n",
      "Epoch 47: loss improved from 3.46160 to 3.45944, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4594 - accuracy: 0.0736 - val_loss: 3.4259 - val_accuracy: 0.0827\n",
      "Epoch 48/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.4608 - accuracy: 0.0744\n",
      "Epoch 48: loss improved from 3.45944 to 3.45759, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4576 - accuracy: 0.0752 - val_loss: 3.4279 - val_accuracy: 0.0796\n",
      "Epoch 49/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.4559 - accuracy: 0.0764\n",
      "Epoch 49: loss improved from 3.45759 to 3.45614, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4561 - accuracy: 0.0757 - val_loss: 3.4210 - val_accuracy: 0.0821\n",
      "Epoch 50/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.4531 - accuracy: 0.0746\n",
      "Epoch 50: loss improved from 3.45614 to 3.45362, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4536 - accuracy: 0.0745 - val_loss: 3.4201 - val_accuracy: 0.0821\n",
      "Epoch 51/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.4506 - accuracy: 0.0750\n",
      "Epoch 51: loss improved from 3.45362 to 3.45157, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4516 - accuracy: 0.0753 - val_loss: 3.4192 - val_accuracy: 0.0790\n",
      "Epoch 52/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.4502 - accuracy: 0.0742\n",
      "Epoch 52: loss improved from 3.45157 to 3.44970, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4497 - accuracy: 0.0737 - val_loss: 3.4165 - val_accuracy: 0.0833\n",
      "Epoch 53/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.4462 - accuracy: 0.0741\n",
      "Epoch 53: loss improved from 3.44970 to 3.44775, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4477 - accuracy: 0.0739 - val_loss: 3.4144 - val_accuracy: 0.0833\n",
      "Epoch 54/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.4484 - accuracy: 0.0737\n",
      "Epoch 54: loss improved from 3.44775 to 3.44724, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4472 - accuracy: 0.0741 - val_loss: 3.4134 - val_accuracy: 0.0784\n",
      "Epoch 55/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.4457 - accuracy: 0.0729\n",
      "Epoch 55: loss improved from 3.44724 to 3.44449, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.4445 - accuracy: 0.0736 - val_loss: 3.4116 - val_accuracy: 0.0796\n",
      "Epoch 56/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4431 - accuracy: 0.0741\n",
      "Epoch 56: loss improved from 3.44449 to 3.44306, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4431 - accuracy: 0.0741 - val_loss: 3.4101 - val_accuracy: 0.0821\n",
      "Epoch 57/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4433 - accuracy: 0.0733\n",
      "Epoch 57: loss improved from 3.44306 to 3.44156, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.4416 - accuracy: 0.0736 - val_loss: 3.4094 - val_accuracy: 0.0808\n",
      "Epoch 58/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.4409 - accuracy: 0.0764\n",
      "Epoch 58: loss improved from 3.44156 to 3.43999, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4400 - accuracy: 0.0767 - val_loss: 3.4080 - val_accuracy: 0.0778\n",
      "Epoch 59/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4384 - accuracy: 0.0741\n",
      "Epoch 59: loss improved from 3.43999 to 3.43847, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4385 - accuracy: 0.0740 - val_loss: 3.4058 - val_accuracy: 0.0790\n",
      "Epoch 60/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4368 - accuracy: 0.0763\n",
      "Epoch 60: loss improved from 3.43847 to 3.43653, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.4365 - accuracy: 0.0763 - val_loss: 3.4056 - val_accuracy: 0.0772\n",
      "Epoch 61/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.4331 - accuracy: 0.0724\n",
      "Epoch 61: loss improved from 3.43653 to 3.43573, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4357 - accuracy: 0.0719 - val_loss: 3.4024 - val_accuracy: 0.0802\n",
      "Epoch 62/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.4337 - accuracy: 0.0740\n",
      "Epoch 62: loss improved from 3.43573 to 3.43429, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4343 - accuracy: 0.0741 - val_loss: 3.4031 - val_accuracy: 0.0808\n",
      "Epoch 63/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4355 - accuracy: 0.0745\n",
      "Epoch 63: loss improved from 3.43429 to 3.43362, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4336 - accuracy: 0.0747 - val_loss: 3.4024 - val_accuracy: 0.0790\n",
      "Epoch 64/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.4306 - accuracy: 0.0750\n",
      "Epoch 64: loss improved from 3.43362 to 3.43166, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4317 - accuracy: 0.0751 - val_loss: 3.3991 - val_accuracy: 0.0814\n",
      "Epoch 65/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.4292 - accuracy: 0.0752\n",
      "Epoch 65: loss improved from 3.43166 to 3.43057, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4306 - accuracy: 0.0748 - val_loss: 3.3977 - val_accuracy: 0.0814\n",
      "Epoch 66/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.4295 - accuracy: 0.0738\n",
      "Epoch 66: loss improved from 3.43057 to 3.42933, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4293 - accuracy: 0.0741 - val_loss: 3.3968 - val_accuracy: 0.0814\n",
      "Epoch 67/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4294 - accuracy: 0.0712\n",
      "Epoch 67: loss improved from 3.42933 to 3.42824, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4282 - accuracy: 0.0720 - val_loss: 3.3950 - val_accuracy: 0.0851\n",
      "Epoch 68/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.4274 - accuracy: 0.0767\n",
      "Epoch 68: loss improved from 3.42824 to 3.42715, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4271 - accuracy: 0.0764 - val_loss: 3.3960 - val_accuracy: 0.0796\n",
      "Epoch 69/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.4277 - accuracy: 0.0755\n",
      "Epoch 69: loss improved from 3.42715 to 3.42619, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.4262 - accuracy: 0.0762 - val_loss: 3.3940 - val_accuracy: 0.0814\n",
      "Epoch 70/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.4251 - accuracy: 0.0737\n",
      "Epoch 70: loss improved from 3.42619 to 3.42510, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4251 - accuracy: 0.0737 - val_loss: 3.4118 - val_accuracy: 0.0765\n",
      "Epoch 71/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.4263 - accuracy: 0.0741\n",
      "Epoch 71: loss did not improve from 3.42510\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4253 - accuracy: 0.0746 - val_loss: 3.3901 - val_accuracy: 0.0839\n",
      "Epoch 72/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4228 - accuracy: 0.0746\n",
      "Epoch 72: loss improved from 3.42510 to 3.42282, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4228 - accuracy: 0.0746 - val_loss: 3.3898 - val_accuracy: 0.0821\n",
      "Epoch 73/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.4195 - accuracy: 0.0716\n",
      "Epoch 73: loss improved from 3.42282 to 3.42065, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4207 - accuracy: 0.0714 - val_loss: 3.3894 - val_accuracy: 0.0802\n",
      "Epoch 74/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4175 - accuracy: 0.0757\n",
      "Epoch 74: loss improved from 3.42065 to 3.41896, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4190 - accuracy: 0.0748 - val_loss: 3.3866 - val_accuracy: 0.0833\n",
      "Epoch 75/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.4179 - accuracy: 0.0768\n",
      "Epoch 75: loss improved from 3.41896 to 3.41684, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4168 - accuracy: 0.0760 - val_loss: 3.3845 - val_accuracy: 0.0857\n",
      "Epoch 76/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.4169 - accuracy: 0.0774\n",
      "Epoch 76: loss improved from 3.41684 to 3.41564, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.4156 - accuracy: 0.0782 - val_loss: 3.3835 - val_accuracy: 0.0888\n",
      "Epoch 77/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.4142 - accuracy: 0.0765\n",
      "Epoch 77: loss improved from 3.41564 to 3.41437, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4144 - accuracy: 0.0764 - val_loss: 3.3836 - val_accuracy: 0.0821\n",
      "Epoch 78/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4127 - accuracy: 0.0749\n",
      "Epoch 78: loss improved from 3.41437 to 3.41264, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4126 - accuracy: 0.0748 - val_loss: 3.3824 - val_accuracy: 0.0845\n",
      "Epoch 79/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.4101 - accuracy: 0.0755\n",
      "Epoch 79: loss improved from 3.41264 to 3.41183, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4118 - accuracy: 0.0760 - val_loss: 3.3802 - val_accuracy: 0.0814\n",
      "Epoch 80/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.4128 - accuracy: 0.0755\n",
      "Epoch 80: loss improved from 3.41183 to 3.41051, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.4105 - accuracy: 0.0767 - val_loss: 3.3800 - val_accuracy: 0.0845\n",
      "Epoch 81/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.4106 - accuracy: 0.0746\n",
      "Epoch 81: loss improved from 3.41051 to 3.40900, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4090 - accuracy: 0.0750 - val_loss: 3.3779 - val_accuracy: 0.0833\n",
      "Epoch 82/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.4051 - accuracy: 0.0770\n",
      "Epoch 82: loss improved from 3.40900 to 3.40756, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4076 - accuracy: 0.0757 - val_loss: 3.3764 - val_accuracy: 0.0808\n",
      "Epoch 83/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.4068 - accuracy: 0.0749\n",
      "Epoch 83: loss improved from 3.40756 to 3.40647, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.4065 - accuracy: 0.0747 - val_loss: 3.3748 - val_accuracy: 0.0839\n",
      "Epoch 84/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4055 - accuracy: 0.0762\n",
      "Epoch 84: loss improved from 3.40647 to 3.40540, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.4054 - accuracy: 0.0763 - val_loss: 3.3749 - val_accuracy: 0.0845\n",
      "Epoch 85/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.4047 - accuracy: 0.0764\n",
      "Epoch 85: loss improved from 3.40540 to 3.40365, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4037 - accuracy: 0.0763 - val_loss: 3.3733 - val_accuracy: 0.0833\n",
      "Epoch 86/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.4038 - accuracy: 0.0768\n",
      "Epoch 86: loss improved from 3.40365 to 3.40365, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4036 - accuracy: 0.0768 - val_loss: 3.3725 - val_accuracy: 0.0863\n",
      "Epoch 87/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.4032 - accuracy: 0.0786\n",
      "Epoch 87: loss improved from 3.40365 to 3.40201, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.4020 - accuracy: 0.0783 - val_loss: 3.3710 - val_accuracy: 0.0839\n",
      "Epoch 88/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.4013 - accuracy: 0.0749\n",
      "Epoch 88: loss improved from 3.40201 to 3.40066, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.4007 - accuracy: 0.0747 - val_loss: 3.3697 - val_accuracy: 0.0790\n",
      "Epoch 89/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.4008 - accuracy: 0.0764\n",
      "Epoch 89: loss improved from 3.40066 to 3.39942, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3994 - accuracy: 0.0773 - val_loss: 3.3674 - val_accuracy: 0.0851\n",
      "Epoch 90/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.4001 - accuracy: 0.0739\n",
      "Epoch 90: loss improved from 3.39942 to 3.39896, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3990 - accuracy: 0.0744 - val_loss: 3.3683 - val_accuracy: 0.0833\n",
      "Epoch 91/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3962 - accuracy: 0.0789\n",
      "Epoch 91: loss improved from 3.39896 to 3.39751, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3975 - accuracy: 0.0788 - val_loss: 3.3663 - val_accuracy: 0.0827\n",
      "Epoch 92/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3957 - accuracy: 0.0760\n",
      "Epoch 92: loss improved from 3.39751 to 3.39562, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3956 - accuracy: 0.0761 - val_loss: 3.3646 - val_accuracy: 0.0808\n",
      "Epoch 93/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3937 - accuracy: 0.0768\n",
      "Epoch 93: loss improved from 3.39562 to 3.39480, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3948 - accuracy: 0.0764 - val_loss: 3.3627 - val_accuracy: 0.0814\n",
      "Epoch 94/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3913 - accuracy: 0.0756\n",
      "Epoch 94: loss improved from 3.39480 to 3.39407, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3941 - accuracy: 0.0751 - val_loss: 3.3625 - val_accuracy: 0.0839\n",
      "Epoch 95/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3923 - accuracy: 0.0760\n",
      "Epoch 95: loss improved from 3.39407 to 3.39236, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3924 - accuracy: 0.0760 - val_loss: 3.3612 - val_accuracy: 0.0772\n",
      "Epoch 96/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3910 - accuracy: 0.0766\n",
      "Epoch 96: loss improved from 3.39236 to 3.39117, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3912 - accuracy: 0.0764 - val_loss: 3.3612 - val_accuracy: 0.0796\n",
      "Epoch 97/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3930 - accuracy: 0.0752\n",
      "Epoch 97: loss improved from 3.39117 to 3.39090, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3909 - accuracy: 0.0752 - val_loss: 3.3597 - val_accuracy: 0.0802\n",
      "Epoch 98/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3879 - accuracy: 0.0784\n",
      "Epoch 98: loss improved from 3.39090 to 3.38910, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3891 - accuracy: 0.0775 - val_loss: 3.3589 - val_accuracy: 0.0790\n",
      "Epoch 99/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3876 - accuracy: 0.0769\n",
      "Epoch 99: loss improved from 3.38910 to 3.38829, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3883 - accuracy: 0.0766 - val_loss: 3.3572 - val_accuracy: 0.0821\n",
      "Epoch 100/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3893 - accuracy: 0.0771\n",
      "Epoch 100: loss improved from 3.38829 to 3.38628, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3863 - accuracy: 0.0768 - val_loss: 3.3555 - val_accuracy: 0.0827\n",
      "Epoch 101/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3852 - accuracy: 0.0777\n",
      "Epoch 101: loss improved from 3.38628 to 3.38519, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3852 - accuracy: 0.0777 - val_loss: 3.3562 - val_accuracy: 0.0827\n",
      "Epoch 102/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3846 - accuracy: 0.0760\n",
      "Epoch 102: loss improved from 3.38519 to 3.38485, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3849 - accuracy: 0.0756 - val_loss: 3.3546 - val_accuracy: 0.0839\n",
      "Epoch 103/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3848 - accuracy: 0.0757\n",
      "Epoch 103: loss improved from 3.38485 to 3.38369, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3837 - accuracy: 0.0768 - val_loss: 3.3539 - val_accuracy: 0.0796\n",
      "Epoch 104/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3823 - accuracy: 0.0770\n",
      "Epoch 104: loss improved from 3.38369 to 3.38246, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3825 - accuracy: 0.0766 - val_loss: 3.3526 - val_accuracy: 0.0827\n",
      "Epoch 105/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3831 - accuracy: 0.0772\n",
      "Epoch 105: loss improved from 3.38246 to 3.38195, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3820 - accuracy: 0.0780 - val_loss: 3.3526 - val_accuracy: 0.0802\n",
      "Epoch 106/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3816 - accuracy: 0.0768\n",
      "Epoch 106: loss improved from 3.38195 to 3.38096, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3810 - accuracy: 0.0769 - val_loss: 3.3521 - val_accuracy: 0.0814\n",
      "Epoch 107/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3798 - accuracy: 0.0761\n",
      "Epoch 107: loss improved from 3.38096 to 3.38034, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3803 - accuracy: 0.0756 - val_loss: 3.3521 - val_accuracy: 0.0845\n",
      "Epoch 108/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3785 - accuracy: 0.0780\n",
      "Epoch 108: loss improved from 3.38034 to 3.37952, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3795 - accuracy: 0.0777 - val_loss: 3.3503 - val_accuracy: 0.0814\n",
      "Epoch 109/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3796 - accuracy: 0.0764\n",
      "Epoch 109: loss improved from 3.37952 to 3.37837, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3784 - accuracy: 0.0769 - val_loss: 3.3499 - val_accuracy: 0.0808\n",
      "Epoch 110/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3755 - accuracy: 0.0768\n",
      "Epoch 110: loss improved from 3.37837 to 3.37765, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3777 - accuracy: 0.0764 - val_loss: 3.3487 - val_accuracy: 0.0808\n",
      "Epoch 111/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3772 - accuracy: 0.0766\n",
      "Epoch 111: loss improved from 3.37765 to 3.37764, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3776 - accuracy: 0.0766 - val_loss: 3.3491 - val_accuracy: 0.0827\n",
      "Epoch 112/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3770 - accuracy: 0.0756\n",
      "Epoch 112: loss improved from 3.37764 to 3.37641, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3764 - accuracy: 0.0760 - val_loss: 3.3482 - val_accuracy: 0.0808\n",
      "Epoch 113/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3761 - accuracy: 0.0775\n",
      "Epoch 113: loss improved from 3.37641 to 3.37609, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3761 - accuracy: 0.0775 - val_loss: 3.3474 - val_accuracy: 0.0796\n",
      "Epoch 114/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3744 - accuracy: 0.0777\n",
      "Epoch 114: loss improved from 3.37609 to 3.37569, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3757 - accuracy: 0.0778 - val_loss: 3.3475 - val_accuracy: 0.0808\n",
      "Epoch 115/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3704 - accuracy: 0.0805\n",
      "Epoch 115: loss improved from 3.37569 to 3.37463, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3746 - accuracy: 0.0791 - val_loss: 3.3480 - val_accuracy: 0.0759\n",
      "Epoch 116/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3720 - accuracy: 0.0770\n",
      "Epoch 116: loss improved from 3.37463 to 3.37396, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3740 - accuracy: 0.0763 - val_loss: 3.3456 - val_accuracy: 0.0821\n",
      "Epoch 117/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3738 - accuracy: 0.0786\n",
      "Epoch 117: loss improved from 3.37396 to 3.37383, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3738 - accuracy: 0.0784 - val_loss: 3.3448 - val_accuracy: 0.0851\n",
      "Epoch 118/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3722 - accuracy: 0.0794\n",
      "Epoch 118: loss improved from 3.37383 to 3.37258, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3726 - accuracy: 0.0790 - val_loss: 3.3443 - val_accuracy: 0.0735\n",
      "Epoch 119/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3736 - accuracy: 0.0780\n",
      "Epoch 119: loss improved from 3.37258 to 3.37200, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3720 - accuracy: 0.0783 - val_loss: 3.3437 - val_accuracy: 0.0802\n",
      "Epoch 120/5000\n",
      "235/256 [==========================>...] - ETA: 0s - loss: 3.3731 - accuracy: 0.0775\n",
      "Epoch 120: loss improved from 3.37200 to 3.37178, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3718 - accuracy: 0.0780 - val_loss: 3.3435 - val_accuracy: 0.0765\n",
      "Epoch 121/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3711 - accuracy: 0.0760\n",
      "Epoch 121: loss improved from 3.37178 to 3.37117, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3712 - accuracy: 0.0760 - val_loss: 3.3440 - val_accuracy: 0.0827\n",
      "Epoch 122/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3714 - accuracy: 0.0783\n",
      "Epoch 122: loss improved from 3.37117 to 3.37028, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3703 - accuracy: 0.0785 - val_loss: 3.3428 - val_accuracy: 0.0796\n",
      "Epoch 123/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3719 - accuracy: 0.0784\n",
      "Epoch 123: loss did not improve from 3.37028\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3704 - accuracy: 0.0782 - val_loss: 3.3431 - val_accuracy: 0.0759\n",
      "Epoch 124/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3701 - accuracy: 0.0764\n",
      "Epoch 124: loss improved from 3.37028 to 3.36963, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3696 - accuracy: 0.0774 - val_loss: 3.3429 - val_accuracy: 0.0772\n",
      "Epoch 125/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3683 - accuracy: 0.0774\n",
      "Epoch 125: loss improved from 3.36963 to 3.36899, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3690 - accuracy: 0.0771 - val_loss: 3.3420 - val_accuracy: 0.0821\n",
      "Epoch 126/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3687 - accuracy: 0.0783\n",
      "Epoch 126: loss improved from 3.36899 to 3.36862, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3686 - accuracy: 0.0784 - val_loss: 3.3417 - val_accuracy: 0.0814\n",
      "Epoch 127/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3666 - accuracy: 0.0774\n",
      "Epoch 127: loss improved from 3.36862 to 3.36838, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3684 - accuracy: 0.0777 - val_loss: 3.3422 - val_accuracy: 0.0802\n",
      "Epoch 128/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3681 - accuracy: 0.0787\n",
      "Epoch 128: loss improved from 3.36838 to 3.36744, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3674 - accuracy: 0.0788 - val_loss: 3.3405 - val_accuracy: 0.0778\n",
      "Epoch 129/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3679 - accuracy: 0.0775\n",
      "Epoch 129: loss did not improve from 3.36744\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3677 - accuracy: 0.0775 - val_loss: 3.3406 - val_accuracy: 0.0759\n",
      "Epoch 130/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3671 - accuracy: 0.0762\n",
      "Epoch 130: loss improved from 3.36744 to 3.36721, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3672 - accuracy: 0.0762 - val_loss: 3.3408 - val_accuracy: 0.0784\n",
      "Epoch 131/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3673 - accuracy: 0.0770\n",
      "Epoch 131: loss improved from 3.36721 to 3.36671, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3667 - accuracy: 0.0766 - val_loss: 3.3402 - val_accuracy: 0.0765\n",
      "Epoch 132/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3641 - accuracy: 0.0784\n",
      "Epoch 132: loss improved from 3.36671 to 3.36656, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3666 - accuracy: 0.0788 - val_loss: 3.3400 - val_accuracy: 0.0796\n",
      "Epoch 133/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3651 - accuracy: 0.0768\n",
      "Epoch 133: loss improved from 3.36656 to 3.36596, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3660 - accuracy: 0.0764 - val_loss: 3.3381 - val_accuracy: 0.0790\n",
      "Epoch 134/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3656 - accuracy: 0.0781\n",
      "Epoch 134: loss improved from 3.36596 to 3.36517, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3652 - accuracy: 0.0778 - val_loss: 3.3381 - val_accuracy: 0.0808\n",
      "Epoch 135/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3655 - accuracy: 0.0799\n",
      "Epoch 135: loss did not improve from 3.36517\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3652 - accuracy: 0.0799 - val_loss: 3.3374 - val_accuracy: 0.0790\n",
      "Epoch 136/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3648 - accuracy: 0.0781\n",
      "Epoch 136: loss improved from 3.36517 to 3.36490, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3649 - accuracy: 0.0785 - val_loss: 3.3391 - val_accuracy: 0.0796\n",
      "Epoch 137/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3644 - accuracy: 0.0792\n",
      "Epoch 137: loss improved from 3.36490 to 3.36452, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3645 - accuracy: 0.0791 - val_loss: 3.3388 - val_accuracy: 0.0772\n",
      "Epoch 138/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3629 - accuracy: 0.0784\n",
      "Epoch 138: loss did not improve from 3.36452\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3646 - accuracy: 0.0782 - val_loss: 3.3369 - val_accuracy: 0.0790\n",
      "Epoch 139/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3668 - accuracy: 0.0783\n",
      "Epoch 139: loss improved from 3.36452 to 3.36330, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3633 - accuracy: 0.0780 - val_loss: 3.3366 - val_accuracy: 0.0747\n",
      "Epoch 140/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3644 - accuracy: 0.0771\n",
      "Epoch 140: loss did not improve from 3.36330\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3640 - accuracy: 0.0773 - val_loss: 3.3369 - val_accuracy: 0.0796\n",
      "Epoch 141/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3635 - accuracy: 0.0792\n",
      "Epoch 141: loss did not improve from 3.36330\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3634 - accuracy: 0.0793 - val_loss: 3.3367 - val_accuracy: 0.0808\n",
      "Epoch 142/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3623 - accuracy: 0.0792\n",
      "Epoch 142: loss improved from 3.36330 to 3.36220, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3622 - accuracy: 0.0791 - val_loss: 3.3356 - val_accuracy: 0.0814\n",
      "Epoch 143/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3617 - accuracy: 0.0766\n",
      "Epoch 143: loss did not improve from 3.36220\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3623 - accuracy: 0.0766 - val_loss: 3.3346 - val_accuracy: 0.0784\n",
      "Epoch 144/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3612 - accuracy: 0.0780\n",
      "Epoch 144: loss improved from 3.36220 to 3.36168, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3617 - accuracy: 0.0782 - val_loss: 3.3382 - val_accuracy: 0.0784\n",
      "Epoch 145/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3620 - accuracy: 0.0755\n",
      "Epoch 145: loss did not improve from 3.36168\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3620 - accuracy: 0.0756 - val_loss: 3.3355 - val_accuracy: 0.0790\n",
      "Epoch 146/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3603 - accuracy: 0.0766\n",
      "Epoch 146: loss improved from 3.36168 to 3.36091, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3609 - accuracy: 0.0771 - val_loss: 3.3339 - val_accuracy: 0.0784\n",
      "Epoch 147/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3617 - accuracy: 0.0788\n",
      "Epoch 147: loss did not improve from 3.36091\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3616 - accuracy: 0.0789 - val_loss: 3.3366 - val_accuracy: 0.0747\n",
      "Epoch 148/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3586 - accuracy: 0.0774\n",
      "Epoch 148: loss improved from 3.36091 to 3.36048, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3605 - accuracy: 0.0786 - val_loss: 3.3338 - val_accuracy: 0.0796\n",
      "Epoch 149/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3599 - accuracy: 0.0762\n",
      "Epoch 149: loss improved from 3.36048 to 3.35994, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3599 - accuracy: 0.0762 - val_loss: 3.3336 - val_accuracy: 0.0778\n",
      "Epoch 150/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3595 - accuracy: 0.0773\n",
      "Epoch 150: loss improved from 3.35994 to 3.35939, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3594 - accuracy: 0.0774 - val_loss: 3.3337 - val_accuracy: 0.0753\n",
      "Epoch 151/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3589 - accuracy: 0.0777\n",
      "Epoch 151: loss did not improve from 3.35939\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3597 - accuracy: 0.0774 - val_loss: 3.3330 - val_accuracy: 0.0796\n",
      "Epoch 152/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3561 - accuracy: 0.0807\n",
      "Epoch 152: loss improved from 3.35939 to 3.35880, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3588 - accuracy: 0.0800 - val_loss: 3.3334 - val_accuracy: 0.0765\n",
      "Epoch 153/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3587 - accuracy: 0.0792\n",
      "Epoch 153: loss improved from 3.35880 to 3.35868, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3587 - accuracy: 0.0791 - val_loss: 3.3323 - val_accuracy: 0.0759\n",
      "Epoch 154/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3580 - accuracy: 0.0793\n",
      "Epoch 154: loss improved from 3.35868 to 3.35831, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3583 - accuracy: 0.0784 - val_loss: 3.3321 - val_accuracy: 0.0784\n",
      "Epoch 155/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3583 - accuracy: 0.0777\n",
      "Epoch 155: loss improved from 3.35831 to 3.35827, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3583 - accuracy: 0.0775 - val_loss: 3.3320 - val_accuracy: 0.0790\n",
      "Epoch 156/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3608 - accuracy: 0.0767\n",
      "Epoch 156: loss improved from 3.35827 to 3.35754, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3575 - accuracy: 0.0772 - val_loss: 3.3294 - val_accuracy: 0.0808\n",
      "Epoch 157/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3603 - accuracy: 0.0807\n",
      "Epoch 157: loss improved from 3.35754 to 3.35722, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3572 - accuracy: 0.0807 - val_loss: 3.3305 - val_accuracy: 0.0808\n",
      "Epoch 158/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3575 - accuracy: 0.0782\n",
      "Epoch 158: loss did not improve from 3.35722\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3574 - accuracy: 0.0782 - val_loss: 3.3303 - val_accuracy: 0.0784\n",
      "Epoch 159/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3564 - accuracy: 0.0791\n",
      "Epoch 159: loss improved from 3.35722 to 3.35620, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3562 - accuracy: 0.0798 - val_loss: 3.3299 - val_accuracy: 0.0796\n",
      "Epoch 160/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3576 - accuracy: 0.0786\n",
      "Epoch 160: loss did not improve from 3.35620\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3562 - accuracy: 0.0796 - val_loss: 3.3304 - val_accuracy: 0.0790\n",
      "Epoch 161/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3574 - accuracy: 0.0755\n",
      "Epoch 161: loss did not improve from 3.35620\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3565 - accuracy: 0.0755 - val_loss: 3.3300 - val_accuracy: 0.0802\n",
      "Epoch 162/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3567 - accuracy: 0.0800\n",
      "Epoch 162: loss did not improve from 3.35620\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3564 - accuracy: 0.0799 - val_loss: 3.3290 - val_accuracy: 0.0765\n",
      "Epoch 163/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3554 - accuracy: 0.0769\n",
      "Epoch 163: loss improved from 3.35620 to 3.35535, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3554 - accuracy: 0.0769 - val_loss: 3.3281 - val_accuracy: 0.0772\n",
      "Epoch 164/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3569 - accuracy: 0.0800\n",
      "Epoch 164: loss did not improve from 3.35535\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3568 - accuracy: 0.0789 - val_loss: 3.3286 - val_accuracy: 0.0790\n",
      "Epoch 165/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3523 - accuracy: 0.0777\n",
      "Epoch 165: loss improved from 3.35535 to 3.35457, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3546 - accuracy: 0.0783 - val_loss: 3.3271 - val_accuracy: 0.0808\n",
      "Epoch 166/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3543 - accuracy: 0.0806\n",
      "Epoch 166: loss improved from 3.35457 to 3.35428, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3543 - accuracy: 0.0802 - val_loss: 3.3273 - val_accuracy: 0.0778\n",
      "Epoch 167/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3536 - accuracy: 0.0785\n",
      "Epoch 167: loss improved from 3.35428 to 3.35423, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3542 - accuracy: 0.0775 - val_loss: 3.3277 - val_accuracy: 0.0802\n",
      "Epoch 168/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3541 - accuracy: 0.0790\n",
      "Epoch 168: loss improved from 3.35423 to 3.35395, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3540 - accuracy: 0.0791 - val_loss: 3.3265 - val_accuracy: 0.0802\n",
      "Epoch 169/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3538 - accuracy: 0.0780\n",
      "Epoch 169: loss improved from 3.35395 to 3.35390, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3539 - accuracy: 0.0782 - val_loss: 3.3269 - val_accuracy: 0.0790\n",
      "Epoch 170/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3537 - accuracy: 0.0789\n",
      "Epoch 170: loss improved from 3.35390 to 3.35366, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3537 - accuracy: 0.0789 - val_loss: 3.3279 - val_accuracy: 0.0765\n",
      "Epoch 171/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3535 - accuracy: 0.0805\n",
      "Epoch 171: loss did not improve from 3.35366\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3537 - accuracy: 0.0804 - val_loss: 3.3271 - val_accuracy: 0.0790\n",
      "Epoch 172/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3524 - accuracy: 0.0791\n",
      "Epoch 172: loss improved from 3.35366 to 3.35341, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3534 - accuracy: 0.0791 - val_loss: 3.3267 - val_accuracy: 0.0778\n",
      "Epoch 173/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3529 - accuracy: 0.0775\n",
      "Epoch 173: loss improved from 3.35341 to 3.35301, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3530 - accuracy: 0.0778 - val_loss: 3.3250 - val_accuracy: 0.0759\n",
      "Epoch 174/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3540 - accuracy: 0.0801\n",
      "Epoch 174: loss improved from 3.35301 to 3.35243, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3524 - accuracy: 0.0796 - val_loss: 3.3252 - val_accuracy: 0.0753\n",
      "Epoch 175/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3538 - accuracy: 0.0796\n",
      "Epoch 175: loss improved from 3.35243 to 3.35208, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3521 - accuracy: 0.0789 - val_loss: 3.3243 - val_accuracy: 0.0796\n",
      "Epoch 176/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3501 - accuracy: 0.0781\n",
      "Epoch 176: loss improved from 3.35208 to 3.35184, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3518 - accuracy: 0.0774 - val_loss: 3.3246 - val_accuracy: 0.0759\n",
      "Epoch 177/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3517 - accuracy: 0.0807\n",
      "Epoch 177: loss improved from 3.35184 to 3.35125, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3512 - accuracy: 0.0812 - val_loss: 3.3282 - val_accuracy: 0.0759\n",
      "Epoch 178/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3523 - accuracy: 0.0791\n",
      "Epoch 178: loss did not improve from 3.35125\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3520 - accuracy: 0.0791 - val_loss: 3.3255 - val_accuracy: 0.0784\n",
      "Epoch 179/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3512 - accuracy: 0.0794\n",
      "Epoch 179: loss did not improve from 3.35125\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3513 - accuracy: 0.0794 - val_loss: 3.3241 - val_accuracy: 0.0784\n",
      "Epoch 180/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3460 - accuracy: 0.0806\n",
      "Epoch 180: loss improved from 3.35125 to 3.35084, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3508 - accuracy: 0.0801 - val_loss: 3.3231 - val_accuracy: 0.0778\n",
      "Epoch 181/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3542 - accuracy: 0.0775\n",
      "Epoch 181: loss did not improve from 3.35084\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3509 - accuracy: 0.0773 - val_loss: 3.3238 - val_accuracy: 0.0796\n",
      "Epoch 182/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3500 - accuracy: 0.0793\n",
      "Epoch 182: loss improved from 3.35084 to 3.35052, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3505 - accuracy: 0.0788 - val_loss: 3.3229 - val_accuracy: 0.0784\n",
      "Epoch 183/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3508 - accuracy: 0.0798\n",
      "Epoch 183: loss improved from 3.35052 to 3.35014, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3501 - accuracy: 0.0800 - val_loss: 3.3226 - val_accuracy: 0.0778\n",
      "Epoch 184/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3497 - accuracy: 0.0793\n",
      "Epoch 184: loss improved from 3.35014 to 3.34998, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3500 - accuracy: 0.0793 - val_loss: 3.3226 - val_accuracy: 0.0796\n",
      "Epoch 185/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3500 - accuracy: 0.0797\n",
      "Epoch 185: loss improved from 3.34998 to 3.34977, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3498 - accuracy: 0.0798 - val_loss: 3.3227 - val_accuracy: 0.0796\n",
      "Epoch 186/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3485 - accuracy: 0.0804\n",
      "Epoch 186: loss did not improve from 3.34977\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3502 - accuracy: 0.0789 - val_loss: 3.3217 - val_accuracy: 0.0808\n",
      "Epoch 187/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3517 - accuracy: 0.0779\n",
      "Epoch 187: loss did not improve from 3.34977\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3502 - accuracy: 0.0789 - val_loss: 3.3221 - val_accuracy: 0.0790\n",
      "Epoch 188/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3494 - accuracy: 0.0795\n",
      "Epoch 188: loss improved from 3.34977 to 3.34931, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3493 - accuracy: 0.0793 - val_loss: 3.3224 - val_accuracy: 0.0790\n",
      "Epoch 189/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3488 - accuracy: 0.0790\n",
      "Epoch 189: loss improved from 3.34931 to 3.34881, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3488 - accuracy: 0.0790 - val_loss: 3.3218 - val_accuracy: 0.0778\n",
      "Epoch 190/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3517 - accuracy: 0.0775\n",
      "Epoch 190: loss did not improve from 3.34881\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3492 - accuracy: 0.0778 - val_loss: 3.3217 - val_accuracy: 0.0808\n",
      "Epoch 191/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3477 - accuracy: 0.0800\n",
      "Epoch 191: loss improved from 3.34881 to 3.34812, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3481 - accuracy: 0.0793 - val_loss: 3.3208 - val_accuracy: 0.0796\n",
      "Epoch 192/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3496 - accuracy: 0.0779\n",
      "Epoch 192: loss did not improve from 3.34812\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3488 - accuracy: 0.0782 - val_loss: 3.3209 - val_accuracy: 0.0784\n",
      "Epoch 193/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3502 - accuracy: 0.0791\n",
      "Epoch 193: loss improved from 3.34812 to 3.34789, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3479 - accuracy: 0.0795 - val_loss: 3.3230 - val_accuracy: 0.0821\n",
      "Epoch 194/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3472 - accuracy: 0.0827\n",
      "Epoch 194: loss did not improve from 3.34789\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3488 - accuracy: 0.0817 - val_loss: 3.3222 - val_accuracy: 0.0827\n",
      "Epoch 195/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3480 - accuracy: 0.0794\n",
      "Epoch 195: loss did not improve from 3.34789\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3480 - accuracy: 0.0794 - val_loss: 3.3203 - val_accuracy: 0.0808\n",
      "Epoch 196/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3477 - accuracy: 0.0788\n",
      "Epoch 196: loss improved from 3.34789 to 3.34760, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3476 - accuracy: 0.0789 - val_loss: 3.3196 - val_accuracy: 0.0833\n",
      "Epoch 197/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3489 - accuracy: 0.0782\n",
      "Epoch 197: loss did not improve from 3.34760\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3477 - accuracy: 0.0782 - val_loss: 3.3196 - val_accuracy: 0.0808\n",
      "Epoch 198/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3478 - accuracy: 0.0813\n",
      "Epoch 198: loss did not improve from 3.34760\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3479 - accuracy: 0.0811 - val_loss: 3.3205 - val_accuracy: 0.0784\n",
      "Epoch 199/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3471 - accuracy: 0.0799\n",
      "Epoch 199: loss improved from 3.34760 to 3.34726, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3473 - accuracy: 0.0799 - val_loss: 3.3211 - val_accuracy: 0.0759\n",
      "Epoch 200/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3465 - accuracy: 0.0807\n",
      "Epoch 200: loss improved from 3.34726 to 3.34679, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3468 - accuracy: 0.0804 - val_loss: 3.3206 - val_accuracy: 0.0753\n",
      "Epoch 201/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3463 - accuracy: 0.0789\n",
      "Epoch 201: loss improved from 3.34679 to 3.34614, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3461 - accuracy: 0.0789 - val_loss: 3.3196 - val_accuracy: 0.0790\n",
      "Epoch 202/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3466 - accuracy: 0.0796\n",
      "Epoch 202: loss did not improve from 3.34614\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3465 - accuracy: 0.0799 - val_loss: 3.3201 - val_accuracy: 0.0796\n",
      "Epoch 203/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3462 - accuracy: 0.0778\n",
      "Epoch 203: loss did not improve from 3.34614\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3463 - accuracy: 0.0777 - val_loss: 3.3199 - val_accuracy: 0.0753\n",
      "Epoch 204/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3447 - accuracy: 0.0817\n",
      "Epoch 204: loss improved from 3.34614 to 3.34570, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3457 - accuracy: 0.0806 - val_loss: 3.3190 - val_accuracy: 0.0814\n",
      "Epoch 205/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3445 - accuracy: 0.0813\n",
      "Epoch 205: loss improved from 3.34570 to 3.34564, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3456 - accuracy: 0.0802 - val_loss: 3.3199 - val_accuracy: 0.0796\n",
      "Epoch 206/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3455 - accuracy: 0.0786\n",
      "Epoch 206: loss did not improve from 3.34564\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3458 - accuracy: 0.0786 - val_loss: 3.3192 - val_accuracy: 0.0765\n",
      "Epoch 207/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3447 - accuracy: 0.0793\n",
      "Epoch 207: loss did not improve from 3.34564\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3457 - accuracy: 0.0786 - val_loss: 3.3174 - val_accuracy: 0.0784\n",
      "Epoch 208/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3452 - accuracy: 0.0783\n",
      "Epoch 208: loss improved from 3.34564 to 3.34507, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3451 - accuracy: 0.0784 - val_loss: 3.3202 - val_accuracy: 0.0772\n",
      "Epoch 209/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3446 - accuracy: 0.0789\n",
      "Epoch 209: loss did not improve from 3.34507\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3455 - accuracy: 0.0786 - val_loss: 3.3172 - val_accuracy: 0.0821\n",
      "Epoch 210/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3459 - accuracy: 0.0800\n",
      "Epoch 210: loss did not improve from 3.34507\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3456 - accuracy: 0.0796 - val_loss: 3.3182 - val_accuracy: 0.0790\n",
      "Epoch 211/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3448 - accuracy: 0.0816\n",
      "Epoch 211: loss improved from 3.34507 to 3.34488, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3449 - accuracy: 0.0816 - val_loss: 3.3175 - val_accuracy: 0.0796\n",
      "Epoch 212/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3433 - accuracy: 0.0829\n",
      "Epoch 212: loss did not improve from 3.34488\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3450 - accuracy: 0.0824 - val_loss: 3.3175 - val_accuracy: 0.0784\n",
      "Epoch 213/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3446 - accuracy: 0.0801\n",
      "Epoch 213: loss improved from 3.34488 to 3.34446, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3445 - accuracy: 0.0801 - val_loss: 3.3179 - val_accuracy: 0.0790\n",
      "Epoch 214/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3443 - accuracy: 0.0800\n",
      "Epoch 214: loss improved from 3.34446 to 3.34414, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3441 - accuracy: 0.0800 - val_loss: 3.3170 - val_accuracy: 0.0796\n",
      "Epoch 215/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3440 - accuracy: 0.0786\n",
      "Epoch 215: loss improved from 3.34414 to 3.34410, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3441 - accuracy: 0.0784 - val_loss: 3.3179 - val_accuracy: 0.0759\n",
      "Epoch 216/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3455 - accuracy: 0.0776\n",
      "Epoch 216: loss did not improve from 3.34410\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3456 - accuracy: 0.0775 - val_loss: 3.3174 - val_accuracy: 0.0796\n",
      "Epoch 217/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3441 - accuracy: 0.0793\n",
      "Epoch 217: loss did not improve from 3.34410\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3443 - accuracy: 0.0793 - val_loss: 3.3169 - val_accuracy: 0.0778\n",
      "Epoch 218/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3434 - accuracy: 0.0787\n",
      "Epoch 218: loss did not improve from 3.34410\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3446 - accuracy: 0.0773 - val_loss: 3.3169 - val_accuracy: 0.0765\n",
      "Epoch 219/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3454 - accuracy: 0.0781\n",
      "Epoch 219: loss improved from 3.34410 to 3.34406, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3441 - accuracy: 0.0778 - val_loss: 3.3179 - val_accuracy: 0.0802\n",
      "Epoch 220/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3402 - accuracy: 0.0794\n",
      "Epoch 220: loss improved from 3.34406 to 3.34363, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3436 - accuracy: 0.0798 - val_loss: 3.3165 - val_accuracy: 0.0784\n",
      "Epoch 221/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3414 - accuracy: 0.0796\n",
      "Epoch 221: loss improved from 3.34363 to 3.34258, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3426 - accuracy: 0.0799 - val_loss: 3.3166 - val_accuracy: 0.0845\n",
      "Epoch 222/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3438 - accuracy: 0.0802\n",
      "Epoch 222: loss did not improve from 3.34258\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3442 - accuracy: 0.0800 - val_loss: 3.3166 - val_accuracy: 0.0802\n",
      "Epoch 223/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3437 - accuracy: 0.0789\n",
      "Epoch 223: loss did not improve from 3.34258\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3430 - accuracy: 0.0789 - val_loss: 3.3166 - val_accuracy: 0.0778\n",
      "Epoch 224/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3433 - accuracy: 0.0815\n",
      "Epoch 224: loss improved from 3.34258 to 3.34250, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3425 - accuracy: 0.0804 - val_loss: 3.3153 - val_accuracy: 0.0790\n",
      "Epoch 225/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3446 - accuracy: 0.0786\n",
      "Epoch 225: loss did not improve from 3.34250\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3431 - accuracy: 0.0786 - val_loss: 3.3168 - val_accuracy: 0.0814\n",
      "Epoch 226/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3466 - accuracy: 0.0796\n",
      "Epoch 226: loss did not improve from 3.34250\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3438 - accuracy: 0.0801 - val_loss: 3.3154 - val_accuracy: 0.0772\n",
      "Epoch 227/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3371 - accuracy: 0.0773\n",
      "Epoch 227: loss improved from 3.34250 to 3.34212, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3421 - accuracy: 0.0772 - val_loss: 3.3153 - val_accuracy: 0.0808\n",
      "Epoch 228/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3417 - accuracy: 0.0788\n",
      "Epoch 228: loss did not improve from 3.34212\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3424 - accuracy: 0.0789 - val_loss: 3.3140 - val_accuracy: 0.0802\n",
      "Epoch 229/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3414 - accuracy: 0.0813\n",
      "Epoch 229: loss did not improve from 3.34212\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3422 - accuracy: 0.0811 - val_loss: 3.3145 - val_accuracy: 0.0808\n",
      "Epoch 230/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3434 - accuracy: 0.0819\n",
      "Epoch 230: loss did not improve from 3.34212\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3429 - accuracy: 0.0816 - val_loss: 3.3172 - val_accuracy: 0.0796\n",
      "Epoch 231/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3416 - accuracy: 0.0776\n",
      "Epoch 231: loss did not improve from 3.34212\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3424 - accuracy: 0.0774 - val_loss: 3.3157 - val_accuracy: 0.0839\n",
      "Epoch 232/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3410 - accuracy: 0.0780\n",
      "Epoch 232: loss did not improve from 3.34212\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3422 - accuracy: 0.0773 - val_loss: 3.3147 - val_accuracy: 0.0814\n",
      "Epoch 233/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3430 - accuracy: 0.0806\n",
      "Epoch 233: loss improved from 3.34212 to 3.34198, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3420 - accuracy: 0.0801 - val_loss: 3.3139 - val_accuracy: 0.0845\n",
      "Epoch 234/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3405 - accuracy: 0.0828\n",
      "Epoch 234: loss improved from 3.34198 to 3.34074, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3407 - accuracy: 0.0822 - val_loss: 3.3138 - val_accuracy: 0.0790\n",
      "Epoch 235/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3396 - accuracy: 0.0803\n",
      "Epoch 235: loss improved from 3.34074 to 3.33988, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3399 - accuracy: 0.0801 - val_loss: 3.3133 - val_accuracy: 0.0772\n",
      "Epoch 236/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3390 - accuracy: 0.0794\n",
      "Epoch 236: loss improved from 3.33988 to 3.33914, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3391 - accuracy: 0.0794 - val_loss: 3.3121 - val_accuracy: 0.0796\n",
      "Epoch 237/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3411 - accuracy: 0.0800\n",
      "Epoch 237: loss improved from 3.33914 to 3.33913, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3391 - accuracy: 0.0806 - val_loss: 3.3115 - val_accuracy: 0.0814\n",
      "Epoch 238/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3388 - accuracy: 0.0822\n",
      "Epoch 238: loss improved from 3.33913 to 3.33853, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3385 - accuracy: 0.0820 - val_loss: 3.3111 - val_accuracy: 0.0796\n",
      "Epoch 239/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3357 - accuracy: 0.0814\n",
      "Epoch 239: loss improved from 3.33853 to 3.33835, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3384 - accuracy: 0.0801 - val_loss: 3.3110 - val_accuracy: 0.0778\n",
      "Epoch 240/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3387 - accuracy: 0.0784\n",
      "Epoch 240: loss did not improve from 3.33835\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3386 - accuracy: 0.0784 - val_loss: 3.3117 - val_accuracy: 0.0827\n",
      "Epoch 241/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3381 - accuracy: 0.0794\n",
      "Epoch 241: loss improved from 3.33835 to 3.33774, saving model to best_model.h5\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3377 - accuracy: 0.0793 - val_loss: 3.3121 - val_accuracy: 0.0765\n",
      "Epoch 242/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3373 - accuracy: 0.0776\n",
      "Epoch 242: loss improved from 3.33774 to 3.33726, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 14ms/step - loss: 3.3373 - accuracy: 0.0777 - val_loss: 3.3120 - val_accuracy: 0.0759\n",
      "Epoch 243/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3360 - accuracy: 0.0799\n",
      "Epoch 243: loss did not improve from 3.33726\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3374 - accuracy: 0.0801 - val_loss: 3.3120 - val_accuracy: 0.0765\n",
      "Epoch 244/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3362 - accuracy: 0.0787\n",
      "Epoch 244: loss improved from 3.33726 to 3.33663, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3366 - accuracy: 0.0788 - val_loss: 3.3113 - val_accuracy: 0.0808\n",
      "Epoch 245/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3364 - accuracy: 0.0813\n",
      "Epoch 245: loss did not improve from 3.33663\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3368 - accuracy: 0.0813 - val_loss: 3.3113 - val_accuracy: 0.0790\n",
      "Epoch 246/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3360 - accuracy: 0.0812\n",
      "Epoch 246: loss improved from 3.33663 to 3.33592, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3359 - accuracy: 0.0818 - val_loss: 3.3115 - val_accuracy: 0.0759\n",
      "Epoch 247/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3359 - accuracy: 0.0815\n",
      "Epoch 247: loss did not improve from 3.33592\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3371 - accuracy: 0.0812 - val_loss: 3.3118 - val_accuracy: 0.0790\n",
      "Epoch 248/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3366 - accuracy: 0.0810\n",
      "Epoch 248: loss did not improve from 3.33592\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3362 - accuracy: 0.0806 - val_loss: 3.3111 - val_accuracy: 0.0759\n",
      "Epoch 249/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3338 - accuracy: 0.0822\n",
      "Epoch 249: loss improved from 3.33592 to 3.33543, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3354 - accuracy: 0.0816 - val_loss: 3.3109 - val_accuracy: 0.0765\n",
      "Epoch 250/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3358 - accuracy: 0.0778\n",
      "Epoch 250: loss did not improve from 3.33543\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3363 - accuracy: 0.0777 - val_loss: 3.3112 - val_accuracy: 0.0723\n",
      "Epoch 251/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3343 - accuracy: 0.0810\n",
      "Epoch 251: loss did not improve from 3.33543\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3357 - accuracy: 0.0813 - val_loss: 3.3112 - val_accuracy: 0.0772\n",
      "Epoch 252/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3354 - accuracy: 0.0804\n",
      "Epoch 252: loss improved from 3.33543 to 3.33534, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3353 - accuracy: 0.0804 - val_loss: 3.3097 - val_accuracy: 0.0765\n",
      "Epoch 253/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3363 - accuracy: 0.0797\n",
      "Epoch 253: loss did not improve from 3.33534\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3369 - accuracy: 0.0802 - val_loss: 3.3110 - val_accuracy: 0.0753\n",
      "Epoch 254/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3360 - accuracy: 0.0793\n",
      "Epoch 254: loss improved from 3.33534 to 3.33479, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3348 - accuracy: 0.0788 - val_loss: 3.3111 - val_accuracy: 0.0784\n",
      "Epoch 255/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3353 - accuracy: 0.0799\n",
      "Epoch 255: loss did not improve from 3.33479\n",
      "256/256 [==============================] - 4s 13ms/step - loss: 3.3353 - accuracy: 0.0799 - val_loss: 3.3108 - val_accuracy: 0.0772\n",
      "Epoch 256/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3349 - accuracy: 0.0799\n",
      "Epoch 256: loss did not improve from 3.33479\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3349 - accuracy: 0.0806 - val_loss: 3.3114 - val_accuracy: 0.0765\n",
      "Epoch 257/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3361 - accuracy: 0.0769\n",
      "Epoch 257: loss did not improve from 3.33479\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3356 - accuracy: 0.0782 - val_loss: 3.3108 - val_accuracy: 0.0784\n",
      "Epoch 258/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3338 - accuracy: 0.0793\n",
      "Epoch 258: loss did not improve from 3.33479\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3351 - accuracy: 0.0791 - val_loss: 3.3092 - val_accuracy: 0.0808\n",
      "Epoch 259/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3343 - accuracy: 0.0790\n",
      "Epoch 259: loss improved from 3.33479 to 3.33398, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3340 - accuracy: 0.0791 - val_loss: 3.3098 - val_accuracy: 0.0821\n",
      "Epoch 260/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3337 - accuracy: 0.0800\n",
      "Epoch 260: loss improved from 3.33398 to 3.33371, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3337 - accuracy: 0.0800 - val_loss: 3.3101 - val_accuracy: 0.0778\n",
      "Epoch 261/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3339 - accuracy: 0.0806\n",
      "Epoch 261: loss did not improve from 3.33371\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3352 - accuracy: 0.0801 - val_loss: 3.3106 - val_accuracy: 0.0808\n",
      "Epoch 262/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3344 - accuracy: 0.0779\n",
      "Epoch 262: loss did not improve from 3.33371\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3345 - accuracy: 0.0779 - val_loss: 3.3090 - val_accuracy: 0.0765\n",
      "Epoch 263/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3339 - accuracy: 0.0796\n",
      "Epoch 263: loss improved from 3.33371 to 3.33359, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3336 - accuracy: 0.0790 - val_loss: 3.3104 - val_accuracy: 0.0808\n",
      "Epoch 264/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3338 - accuracy: 0.0786\n",
      "Epoch 264: loss did not improve from 3.33359\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3338 - accuracy: 0.0786 - val_loss: 3.3094 - val_accuracy: 0.0796\n",
      "Epoch 265/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3360 - accuracy: 0.0798\n",
      "Epoch 265: loss did not improve from 3.33359\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3337 - accuracy: 0.0790 - val_loss: 3.3099 - val_accuracy: 0.0765\n",
      "Epoch 266/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3341 - accuracy: 0.0790\n",
      "Epoch 266: loss did not improve from 3.33359\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3342 - accuracy: 0.0789 - val_loss: 3.3088 - val_accuracy: 0.0784\n",
      "Epoch 267/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3343 - accuracy: 0.0792\n",
      "Epoch 267: loss did not improve from 3.33359\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3343 - accuracy: 0.0791 - val_loss: 3.3093 - val_accuracy: 0.0790\n",
      "Epoch 268/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3330 - accuracy: 0.0805\n",
      "Epoch 268: loss improved from 3.33359 to 3.33308, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3331 - accuracy: 0.0805 - val_loss: 3.3080 - val_accuracy: 0.0753\n",
      "Epoch 269/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3334 - accuracy: 0.0788\n",
      "Epoch 269: loss did not improve from 3.33308\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3333 - accuracy: 0.0788 - val_loss: 3.3086 - val_accuracy: 0.0808\n",
      "Epoch 270/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3337 - accuracy: 0.0785\n",
      "Epoch 270: loss did not improve from 3.33308\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3335 - accuracy: 0.0784 - val_loss: 3.3086 - val_accuracy: 0.0735\n",
      "Epoch 271/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3321 - accuracy: 0.0809\n",
      "Epoch 271: loss improved from 3.33308 to 3.33262, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3326 - accuracy: 0.0805 - val_loss: 3.3071 - val_accuracy: 0.0808\n",
      "Epoch 272/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3336 - accuracy: 0.0799\n",
      "Epoch 272: loss did not improve from 3.33262\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3335 - accuracy: 0.0796 - val_loss: 3.3100 - val_accuracy: 0.0772\n",
      "Epoch 273/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3374 - accuracy: 0.0786\n",
      "Epoch 273: loss did not improve from 3.33262\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3336 - accuracy: 0.0790 - val_loss: 3.3083 - val_accuracy: 0.0778\n",
      "Epoch 274/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3312 - accuracy: 0.0803\n",
      "Epoch 274: loss improved from 3.33262 to 3.33245, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3324 - accuracy: 0.0798 - val_loss: 3.3084 - val_accuracy: 0.0814\n",
      "Epoch 275/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3320 - accuracy: 0.0795\n",
      "Epoch 275: loss improved from 3.33245 to 3.33224, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3322 - accuracy: 0.0789 - val_loss: 3.3066 - val_accuracy: 0.0784\n",
      "Epoch 276/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3331 - accuracy: 0.0807\n",
      "Epoch 276: loss did not improve from 3.33224\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3322 - accuracy: 0.0807 - val_loss: 3.3081 - val_accuracy: 0.0778\n",
      "Epoch 277/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3320 - accuracy: 0.0778\n",
      "Epoch 277: loss improved from 3.33224 to 3.33197, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3320 - accuracy: 0.0779 - val_loss: 3.3081 - val_accuracy: 0.0765\n",
      "Epoch 278/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3320 - accuracy: 0.0789\n",
      "Epoch 278: loss did not improve from 3.33197\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3323 - accuracy: 0.0789 - val_loss: 3.3082 - val_accuracy: 0.0796\n",
      "Epoch 279/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3345 - accuracy: 0.0815\n",
      "Epoch 279: loss did not improve from 3.33197\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3322 - accuracy: 0.0796 - val_loss: 3.3074 - val_accuracy: 0.0814\n",
      "Epoch 280/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3323 - accuracy: 0.0816\n",
      "Epoch 280: loss did not improve from 3.33197\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3320 - accuracy: 0.0817 - val_loss: 3.3088 - val_accuracy: 0.0839\n",
      "Epoch 281/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3319 - accuracy: 0.0799\n",
      "Epoch 281: loss improved from 3.33197 to 3.33191, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3319 - accuracy: 0.0799 - val_loss: 3.3066 - val_accuracy: 0.0808\n",
      "Epoch 282/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3316 - accuracy: 0.0785\n",
      "Epoch 282: loss improved from 3.33191 to 3.33160, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3316 - accuracy: 0.0785 - val_loss: 3.3077 - val_accuracy: 0.0839\n",
      "Epoch 283/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3312 - accuracy: 0.0789\n",
      "Epoch 283: loss did not improve from 3.33160\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3317 - accuracy: 0.0786 - val_loss: 3.3066 - val_accuracy: 0.0796\n",
      "Epoch 284/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3342 - accuracy: 0.0790\n",
      "Epoch 284: loss did not improve from 3.33160\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3322 - accuracy: 0.0796 - val_loss: 3.3070 - val_accuracy: 0.0796\n",
      "Epoch 285/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3296 - accuracy: 0.0816\n",
      "Epoch 285: loss improved from 3.33160 to 3.33112, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3311 - accuracy: 0.0809 - val_loss: 3.3061 - val_accuracy: 0.0802\n",
      "Epoch 286/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3287 - accuracy: 0.0807\n",
      "Epoch 286: loss improved from 3.33112 to 3.33094, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3309 - accuracy: 0.0800 - val_loss: 3.3061 - val_accuracy: 0.0759\n",
      "Epoch 287/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3274 - accuracy: 0.0782\n",
      "Epoch 287: loss did not improve from 3.33094\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3327 - accuracy: 0.0778 - val_loss: 3.3059 - val_accuracy: 0.0790\n",
      "Epoch 288/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3310 - accuracy: 0.0799\n",
      "Epoch 288: loss did not improve from 3.33094\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3310 - accuracy: 0.0799 - val_loss: 3.3059 - val_accuracy: 0.0796\n",
      "Epoch 289/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3305 - accuracy: 0.0790\n",
      "Epoch 289: loss improved from 3.33094 to 3.33068, saving model to best_model.h5\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3307 - accuracy: 0.0790 - val_loss: 3.3055 - val_accuracy: 0.0784\n",
      "Epoch 290/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3307 - accuracy: 0.0819\n",
      "Epoch 290: loss did not improve from 3.33068\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3316 - accuracy: 0.0810 - val_loss: 3.3061 - val_accuracy: 0.0778\n",
      "Epoch 291/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3291 - accuracy: 0.0806\n",
      "Epoch 291: loss improved from 3.33068 to 3.33036, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3304 - accuracy: 0.0800 - val_loss: 3.3058 - val_accuracy: 0.0802\n",
      "Epoch 292/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3306 - accuracy: 0.0792\n",
      "Epoch 292: loss improved from 3.33036 to 3.33021, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3302 - accuracy: 0.0798 - val_loss: 3.3056 - val_accuracy: 0.0808\n",
      "Epoch 293/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3309 - accuracy: 0.0810\n",
      "Epoch 293: loss did not improve from 3.33021\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3304 - accuracy: 0.0810 - val_loss: 3.3050 - val_accuracy: 0.0784\n",
      "Epoch 294/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3302 - accuracy: 0.0758\n",
      "Epoch 294: loss improved from 3.33021 to 3.33011, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3301 - accuracy: 0.0761 - val_loss: 3.3052 - val_accuracy: 0.0814\n",
      "Epoch 295/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3304 - accuracy: 0.0794\n",
      "Epoch 295: loss did not improve from 3.33011\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3302 - accuracy: 0.0794 - val_loss: 3.3052 - val_accuracy: 0.0796\n",
      "Epoch 296/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3295 - accuracy: 0.0792\n",
      "Epoch 296: loss improved from 3.33011 to 3.33006, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3301 - accuracy: 0.0789 - val_loss: 3.3055 - val_accuracy: 0.0778\n",
      "Epoch 297/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3327 - accuracy: 0.0794\n",
      "Epoch 297: loss did not improve from 3.33006\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3328 - accuracy: 0.0800 - val_loss: 3.3057 - val_accuracy: 0.0833\n",
      "Epoch 298/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3297 - accuracy: 0.0784\n",
      "Epoch 298: loss did not improve from 3.33006\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3302 - accuracy: 0.0783 - val_loss: 3.3051 - val_accuracy: 0.0802\n",
      "Epoch 299/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3298 - accuracy: 0.0805\n",
      "Epoch 299: loss improved from 3.33006 to 3.32960, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3296 - accuracy: 0.0811 - val_loss: 3.3040 - val_accuracy: 0.0821\n",
      "Epoch 300/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3303 - accuracy: 0.0790\n",
      "Epoch 300: loss did not improve from 3.32960\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3304 - accuracy: 0.0790 - val_loss: 3.3048 - val_accuracy: 0.0839\n",
      "Epoch 301/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3279 - accuracy: 0.0766\n",
      "Epoch 301: loss improved from 3.32960 to 3.32951, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3295 - accuracy: 0.0769 - val_loss: 3.3040 - val_accuracy: 0.0784\n",
      "Epoch 302/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3295 - accuracy: 0.0803\n",
      "Epoch 302: loss improved from 3.32951 to 3.32883, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3288 - accuracy: 0.0809 - val_loss: 3.3032 - val_accuracy: 0.0778\n",
      "Epoch 303/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3289 - accuracy: 0.0799\n",
      "Epoch 303: loss did not improve from 3.32883\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3292 - accuracy: 0.0805 - val_loss: 3.3037 - val_accuracy: 0.0814\n",
      "Epoch 304/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3295 - accuracy: 0.0800\n",
      "Epoch 304: loss did not improve from 3.32883\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3301 - accuracy: 0.0794 - val_loss: 3.3040 - val_accuracy: 0.0796\n",
      "Epoch 305/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3286 - accuracy: 0.0789\n",
      "Epoch 305: loss did not improve from 3.32883\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3296 - accuracy: 0.0788 - val_loss: 3.3044 - val_accuracy: 0.0833\n",
      "Epoch 306/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3292 - accuracy: 0.0807\n",
      "Epoch 306: loss did not improve from 3.32883\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3290 - accuracy: 0.0802 - val_loss: 3.3044 - val_accuracy: 0.0839\n",
      "Epoch 307/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3269 - accuracy: 0.0789\n",
      "Epoch 307: loss did not improve from 3.32883\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3291 - accuracy: 0.0789 - val_loss: 3.3038 - val_accuracy: 0.0814\n",
      "Epoch 308/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3288 - accuracy: 0.0804\n",
      "Epoch 308: loss improved from 3.32883 to 3.32876, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3288 - accuracy: 0.0804 - val_loss: 3.3039 - val_accuracy: 0.0845\n",
      "Epoch 309/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3285 - accuracy: 0.0817\n",
      "Epoch 309: loss improved from 3.32876 to 3.32849, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3285 - accuracy: 0.0817 - val_loss: 3.3037 - val_accuracy: 0.0814\n",
      "Epoch 310/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3286 - accuracy: 0.0763\n",
      "Epoch 310: loss did not improve from 3.32849\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3285 - accuracy: 0.0763 - val_loss: 3.3039 - val_accuracy: 0.0784\n",
      "Epoch 311/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3282 - accuracy: 0.0811\n",
      "Epoch 311: loss did not improve from 3.32849\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3285 - accuracy: 0.0801 - val_loss: 3.3044 - val_accuracy: 0.0808\n",
      "Epoch 312/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3280 - accuracy: 0.0821\n",
      "Epoch 312: loss improved from 3.32849 to 3.32818, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3282 - accuracy: 0.0818 - val_loss: 3.3031 - val_accuracy: 0.0845\n",
      "Epoch 313/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3289 - accuracy: 0.0787\n",
      "Epoch 313: loss did not improve from 3.32818\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3284 - accuracy: 0.0790 - val_loss: 3.3036 - val_accuracy: 0.0839\n",
      "Epoch 314/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3286 - accuracy: 0.0816\n",
      "Epoch 314: loss did not improve from 3.32818\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3287 - accuracy: 0.0816 - val_loss: 3.3039 - val_accuracy: 0.0821\n",
      "Epoch 315/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3282 - accuracy: 0.0794\n",
      "Epoch 315: loss improved from 3.32818 to 3.32798, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3280 - accuracy: 0.0795 - val_loss: 3.3043 - val_accuracy: 0.0772\n",
      "Epoch 316/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3283 - accuracy: 0.0806\n",
      "Epoch 316: loss did not improve from 3.32798\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3280 - accuracy: 0.0807 - val_loss: 3.3043 - val_accuracy: 0.0796\n",
      "Epoch 317/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3302 - accuracy: 0.0806\n",
      "Epoch 317: loss did not improve from 3.32798\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3281 - accuracy: 0.0806 - val_loss: 3.3037 - val_accuracy: 0.0827\n",
      "Epoch 318/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3282 - accuracy: 0.0795\n",
      "Epoch 318: loss improved from 3.32798 to 3.32792, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3279 - accuracy: 0.0795 - val_loss: 3.3040 - val_accuracy: 0.0796\n",
      "Epoch 319/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3282 - accuracy: 0.0787\n",
      "Epoch 319: loss did not improve from 3.32792\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3281 - accuracy: 0.0793 - val_loss: 3.3049 - val_accuracy: 0.0827\n",
      "Epoch 320/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3276 - accuracy: 0.0790\n",
      "Epoch 320: loss improved from 3.32792 to 3.32769, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3277 - accuracy: 0.0789 - val_loss: 3.3030 - val_accuracy: 0.0784\n",
      "Epoch 321/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3285 - accuracy: 0.0793\n",
      "Epoch 321: loss did not improve from 3.32769\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3284 - accuracy: 0.0793 - val_loss: 3.3036 - val_accuracy: 0.0796\n",
      "Epoch 322/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3279 - accuracy: 0.0798\n",
      "Epoch 322: loss did not improve from 3.32769\n",
      "256/256 [==============================] - 3s 14ms/step - loss: 3.3277 - accuracy: 0.0801 - val_loss: 3.3044 - val_accuracy: 0.0765\n",
      "Epoch 323/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3285 - accuracy: 0.0789\n",
      "Epoch 323: loss did not improve from 3.32769\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3282 - accuracy: 0.0788 - val_loss: 3.3039 - val_accuracy: 0.0790\n",
      "Epoch 324/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3278 - accuracy: 0.0784\n",
      "Epoch 324: loss improved from 3.32769 to 3.32755, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3275 - accuracy: 0.0782 - val_loss: 3.3029 - val_accuracy: 0.0802\n",
      "Epoch 325/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3271 - accuracy: 0.0832\n",
      "Epoch 325: loss improved from 3.32755 to 3.32710, saving model to best_model.h5\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3271 - accuracy: 0.0833 - val_loss: 3.3044 - val_accuracy: 0.0747\n",
      "Epoch 326/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3284 - accuracy: 0.0796\n",
      "Epoch 326: loss did not improve from 3.32710\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3272 - accuracy: 0.0799 - val_loss: 3.3028 - val_accuracy: 0.0827\n",
      "Epoch 327/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3277 - accuracy: 0.0811\n",
      "Epoch 327: loss did not improve from 3.32710\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3275 - accuracy: 0.0811 - val_loss: 3.3038 - val_accuracy: 0.0790\n",
      "Epoch 328/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3292 - accuracy: 0.0794\n",
      "Epoch 328: loss improved from 3.32710 to 3.32703, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3270 - accuracy: 0.0795 - val_loss: 3.3035 - val_accuracy: 0.0802\n",
      "Epoch 329/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3272 - accuracy: 0.0814\n",
      "Epoch 329: loss did not improve from 3.32703\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3270 - accuracy: 0.0813 - val_loss: 3.3029 - val_accuracy: 0.0821\n",
      "Epoch 330/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3271 - accuracy: 0.0808\n",
      "Epoch 330: loss improved from 3.32703 to 3.32697, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3270 - accuracy: 0.0807 - val_loss: 3.3021 - val_accuracy: 0.0839\n",
      "Epoch 331/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3278 - accuracy: 0.0801\n",
      "Epoch 331: loss did not improve from 3.32697\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3277 - accuracy: 0.0802 - val_loss: 3.3044 - val_accuracy: 0.0851\n",
      "Epoch 332/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3271 - accuracy: 0.0813\n",
      "Epoch 332: loss did not improve from 3.32697\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3274 - accuracy: 0.0816 - val_loss: 3.3021 - val_accuracy: 0.0833\n",
      "Epoch 333/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3274 - accuracy: 0.0795\n",
      "Epoch 333: loss did not improve from 3.32697\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3273 - accuracy: 0.0796 - val_loss: 3.3026 - val_accuracy: 0.0821\n",
      "Epoch 334/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3239 - accuracy: 0.0820\n",
      "Epoch 334: loss improved from 3.32697 to 3.32616, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3262 - accuracy: 0.0817 - val_loss: 3.3022 - val_accuracy: 0.0784\n",
      "Epoch 335/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3276 - accuracy: 0.0793\n",
      "Epoch 335: loss did not improve from 3.32616\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3276 - accuracy: 0.0794 - val_loss: 3.3027 - val_accuracy: 0.0772\n",
      "Epoch 336/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3259 - accuracy: 0.0792\n",
      "Epoch 336: loss did not improve from 3.32616\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3263 - accuracy: 0.0793 - val_loss: 3.3017 - val_accuracy: 0.0808\n",
      "Epoch 337/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3274 - accuracy: 0.0808\n",
      "Epoch 337: loss did not improve from 3.32616\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3276 - accuracy: 0.0806 - val_loss: 3.3049 - val_accuracy: 0.0839\n",
      "Epoch 338/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3302 - accuracy: 0.0806\n",
      "Epoch 338: loss did not improve from 3.32616\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3268 - accuracy: 0.0807 - val_loss: 3.3025 - val_accuracy: 0.0827\n",
      "Epoch 339/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3255 - accuracy: 0.0807\n",
      "Epoch 339: loss improved from 3.32616 to 3.32579, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3258 - accuracy: 0.0806 - val_loss: 3.3025 - val_accuracy: 0.0802\n",
      "Epoch 340/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3262 - accuracy: 0.0816\n",
      "Epoch 340: loss did not improve from 3.32579\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3261 - accuracy: 0.0818 - val_loss: 3.3014 - val_accuracy: 0.0839\n",
      "Epoch 341/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3261 - accuracy: 0.0814\n",
      "Epoch 341: loss did not improve from 3.32579\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3260 - accuracy: 0.0815 - val_loss: 3.3017 - val_accuracy: 0.0790\n",
      "Epoch 342/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3267 - accuracy: 0.0811\n",
      "Epoch 342: loss did not improve from 3.32579\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3268 - accuracy: 0.0811 - val_loss: 3.3019 - val_accuracy: 0.0814\n",
      "Epoch 343/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3266 - accuracy: 0.0792\n",
      "Epoch 343: loss did not improve from 3.32579\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3262 - accuracy: 0.0793 - val_loss: 3.3040 - val_accuracy: 0.0839\n",
      "Epoch 344/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3260 - accuracy: 0.0815\n",
      "Epoch 344: loss did not improve from 3.32579\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3262 - accuracy: 0.0812 - val_loss: 3.3019 - val_accuracy: 0.0821\n",
      "Epoch 345/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3266 - accuracy: 0.0794\n",
      "Epoch 345: loss did not improve from 3.32579\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3266 - accuracy: 0.0794 - val_loss: 3.3032 - val_accuracy: 0.0772\n",
      "Epoch 346/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3269 - accuracy: 0.0795\n",
      "Epoch 346: loss did not improve from 3.32579\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3268 - accuracy: 0.0796 - val_loss: 3.3018 - val_accuracy: 0.0839\n",
      "Epoch 347/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3257 - accuracy: 0.0816\n",
      "Epoch 347: loss improved from 3.32579 to 3.32561, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3256 - accuracy: 0.0816 - val_loss: 3.3019 - val_accuracy: 0.0833\n",
      "Epoch 348/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3250 - accuracy: 0.0797\n",
      "Epoch 348: loss improved from 3.32561 to 3.32537, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3254 - accuracy: 0.0798 - val_loss: 3.3009 - val_accuracy: 0.0827\n",
      "Epoch 349/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3245 - accuracy: 0.0814\n",
      "Epoch 349: loss did not improve from 3.32537\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3255 - accuracy: 0.0810 - val_loss: 3.3012 - val_accuracy: 0.0833\n",
      "Epoch 350/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3261 - accuracy: 0.0800\n",
      "Epoch 350: loss did not improve from 3.32537\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3264 - accuracy: 0.0796 - val_loss: 3.3009 - val_accuracy: 0.0845\n",
      "Epoch 351/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3264 - accuracy: 0.0797\n",
      "Epoch 351: loss did not improve from 3.32537\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3255 - accuracy: 0.0798 - val_loss: 3.3007 - val_accuracy: 0.0833\n",
      "Epoch 352/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3253 - accuracy: 0.0801\n",
      "Epoch 352: loss did not improve from 3.32537\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3254 - accuracy: 0.0801 - val_loss: 3.3003 - val_accuracy: 0.0802\n",
      "Epoch 353/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3259 - accuracy: 0.0784\n",
      "Epoch 353: loss improved from 3.32537 to 3.32506, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3251 - accuracy: 0.0786 - val_loss: 3.3014 - val_accuracy: 0.0808\n",
      "Epoch 354/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3253 - accuracy: 0.0795\n",
      "Epoch 354: loss did not improve from 3.32506\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3252 - accuracy: 0.0795 - val_loss: 3.3013 - val_accuracy: 0.0814\n",
      "Epoch 355/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3258 - accuracy: 0.0803\n",
      "Epoch 355: loss did not improve from 3.32506\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3256 - accuracy: 0.0802 - val_loss: 3.3017 - val_accuracy: 0.0808\n",
      "Epoch 356/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3253 - accuracy: 0.0814\n",
      "Epoch 356: loss did not improve from 3.32506\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3252 - accuracy: 0.0804 - val_loss: 3.3008 - val_accuracy: 0.0802\n",
      "Epoch 357/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3251 - accuracy: 0.0789\n",
      "Epoch 357: loss did not improve from 3.32506\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3251 - accuracy: 0.0789 - val_loss: 3.3002 - val_accuracy: 0.0796\n",
      "Epoch 358/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3242 - accuracy: 0.0799\n",
      "Epoch 358: loss improved from 3.32506 to 3.32472, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3247 - accuracy: 0.0799 - val_loss: 3.2999 - val_accuracy: 0.0784\n",
      "Epoch 359/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3223 - accuracy: 0.0807\n",
      "Epoch 359: loss did not improve from 3.32472\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3259 - accuracy: 0.0802 - val_loss: 3.2999 - val_accuracy: 0.0814\n",
      "Epoch 360/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3250 - accuracy: 0.0812\n",
      "Epoch 360: loss did not improve from 3.32472\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3251 - accuracy: 0.0816 - val_loss: 3.2996 - val_accuracy: 0.0784\n",
      "Epoch 361/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3225 - accuracy: 0.0837\n",
      "Epoch 361: loss did not improve from 3.32472\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3251 - accuracy: 0.0834 - val_loss: 3.3004 - val_accuracy: 0.0778\n",
      "Epoch 362/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3242 - accuracy: 0.0807\n",
      "Epoch 362: loss improved from 3.32472 to 3.32465, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3246 - accuracy: 0.0807 - val_loss: 3.3008 - val_accuracy: 0.0808\n",
      "Epoch 363/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3252 - accuracy: 0.0810\n",
      "Epoch 363: loss improved from 3.32465 to 3.32452, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3245 - accuracy: 0.0811 - val_loss: 3.3044 - val_accuracy: 0.0784\n",
      "Epoch 364/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3260 - accuracy: 0.0809\n",
      "Epoch 364: loss did not improve from 3.32452\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3257 - accuracy: 0.0817 - val_loss: 3.3016 - val_accuracy: 0.0802\n",
      "Epoch 365/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3243 - accuracy: 0.0816\n",
      "Epoch 365: loss did not improve from 3.32452\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3250 - accuracy: 0.0816 - val_loss: 3.2989 - val_accuracy: 0.0802\n",
      "Epoch 366/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3240 - accuracy: 0.0787\n",
      "Epoch 366: loss improved from 3.32452 to 3.32412, saving model to best_model.h5\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3241 - accuracy: 0.0786 - val_loss: 3.2995 - val_accuracy: 0.0808\n",
      "Epoch 367/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3250 - accuracy: 0.0814\n",
      "Epoch 367: loss did not improve from 3.32412\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3252 - accuracy: 0.0817 - val_loss: 3.3004 - val_accuracy: 0.0839\n",
      "Epoch 368/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3251 - accuracy: 0.0796\n",
      "Epoch 368: loss did not improve from 3.32412\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3245 - accuracy: 0.0811 - val_loss: 3.3003 - val_accuracy: 0.0814\n",
      "Epoch 369/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3243 - accuracy: 0.0817\n",
      "Epoch 369: loss did not improve from 3.32412\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3243 - accuracy: 0.0817 - val_loss: 3.2994 - val_accuracy: 0.0827\n",
      "Epoch 370/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3255 - accuracy: 0.0795\n",
      "Epoch 370: loss did not improve from 3.32412\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3254 - accuracy: 0.0799 - val_loss: 3.3018 - val_accuracy: 0.0827\n",
      "Epoch 371/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3245 - accuracy: 0.0820\n",
      "Epoch 371: loss did not improve from 3.32412\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3245 - accuracy: 0.0820 - val_loss: 3.3011 - val_accuracy: 0.0845\n",
      "Epoch 372/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3215 - accuracy: 0.0826\n",
      "Epoch 372: loss did not improve from 3.32412\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3243 - accuracy: 0.0816 - val_loss: 3.2997 - val_accuracy: 0.0821\n",
      "Epoch 373/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3240 - accuracy: 0.0817\n",
      "Epoch 373: loss improved from 3.32412 to 3.32403, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3240 - accuracy: 0.0820 - val_loss: 3.3007 - val_accuracy: 0.0839\n",
      "Epoch 374/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3245 - accuracy: 0.0788\n",
      "Epoch 374: loss did not improve from 3.32403\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3243 - accuracy: 0.0788 - val_loss: 3.2999 - val_accuracy: 0.0814\n",
      "Epoch 375/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3238 - accuracy: 0.0792\n",
      "Epoch 375: loss did not improve from 3.32403\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3242 - accuracy: 0.0789 - val_loss: 3.3007 - val_accuracy: 0.0778\n",
      "Epoch 376/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3230 - accuracy: 0.0818\n",
      "Epoch 376: loss did not improve from 3.32403\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3243 - accuracy: 0.0818 - val_loss: 3.3007 - val_accuracy: 0.0827\n",
      "Epoch 377/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3250 - accuracy: 0.0779\n",
      "Epoch 377: loss did not improve from 3.32403\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3241 - accuracy: 0.0784 - val_loss: 3.2997 - val_accuracy: 0.0845\n",
      "Epoch 378/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3243 - accuracy: 0.0797\n",
      "Epoch 378: loss did not improve from 3.32403\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3246 - accuracy: 0.0800 - val_loss: 3.3008 - val_accuracy: 0.0876\n",
      "Epoch 379/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3248 - accuracy: 0.0808\n",
      "Epoch 379: loss did not improve from 3.32403\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3243 - accuracy: 0.0804 - val_loss: 3.2993 - val_accuracy: 0.0827\n",
      "Epoch 380/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3234 - accuracy: 0.0803\n",
      "Epoch 380: loss improved from 3.32403 to 3.32344, saving model to best_model.h5\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3234 - accuracy: 0.0801 - val_loss: 3.2986 - val_accuracy: 0.0845\n",
      "Epoch 381/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3242 - accuracy: 0.0803\n",
      "Epoch 381: loss did not improve from 3.32344\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3237 - accuracy: 0.0805 - val_loss: 3.2990 - val_accuracy: 0.0839\n",
      "Epoch 382/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3234 - accuracy: 0.0831\n",
      "Epoch 382: loss did not improve from 3.32344\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3236 - accuracy: 0.0820 - val_loss: 3.3002 - val_accuracy: 0.0845\n",
      "Epoch 383/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3239 - accuracy: 0.0836\n",
      "Epoch 383: loss did not improve from 3.32344\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3240 - accuracy: 0.0831 - val_loss: 3.2990 - val_accuracy: 0.0778\n",
      "Epoch 384/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3243 - accuracy: 0.0817\n",
      "Epoch 384: loss did not improve from 3.32344\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3243 - accuracy: 0.0817 - val_loss: 3.3005 - val_accuracy: 0.0814\n",
      "Epoch 385/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3232 - accuracy: 0.0810\n",
      "Epoch 385: loss improved from 3.32344 to 3.32324, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3232 - accuracy: 0.0810 - val_loss: 3.2995 - val_accuracy: 0.0814\n",
      "Epoch 386/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3253 - accuracy: 0.0804\n",
      "Epoch 386: loss did not improve from 3.32324\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3240 - accuracy: 0.0809 - val_loss: 3.3018 - val_accuracy: 0.0772\n",
      "Epoch 387/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3224 - accuracy: 0.0818\n",
      "Epoch 387: loss did not improve from 3.32324\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3239 - accuracy: 0.0815 - val_loss: 3.2993 - val_accuracy: 0.0833\n",
      "Epoch 388/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3230 - accuracy: 0.0831\n",
      "Epoch 388: loss improved from 3.32324 to 3.32308, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3231 - accuracy: 0.0832 - val_loss: 3.2999 - val_accuracy: 0.0790\n",
      "Epoch 389/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3249 - accuracy: 0.0787\n",
      "Epoch 389: loss did not improve from 3.32308\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3251 - accuracy: 0.0788 - val_loss: 3.2998 - val_accuracy: 0.0821\n",
      "Epoch 390/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3233 - accuracy: 0.0824\n",
      "Epoch 390: loss did not improve from 3.32308\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3234 - accuracy: 0.0827 - val_loss: 3.2992 - val_accuracy: 0.0790\n",
      "Epoch 391/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3225 - accuracy: 0.0784\n",
      "Epoch 391: loss improved from 3.32308 to 3.32305, saving model to best_model.h5\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3231 - accuracy: 0.0784 - val_loss: 3.2990 - val_accuracy: 0.0833\n",
      "Epoch 392/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3244 - accuracy: 0.0807\n",
      "Epoch 392: loss did not improve from 3.32305\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3248 - accuracy: 0.0809 - val_loss: 3.3008 - val_accuracy: 0.0814\n",
      "Epoch 393/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3212 - accuracy: 0.0832\n",
      "Epoch 393: loss did not improve from 3.32305\n",
      "256/256 [==============================] - 4s 18ms/step - loss: 3.3235 - accuracy: 0.0822 - val_loss: 3.2986 - val_accuracy: 0.0833\n",
      "Epoch 394/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3233 - accuracy: 0.0800\n",
      "Epoch 394: loss did not improve from 3.32305\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3232 - accuracy: 0.0800 - val_loss: 3.2985 - val_accuracy: 0.0833\n",
      "Epoch 395/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3222 - accuracy: 0.0792\n",
      "Epoch 395: loss did not improve from 3.32305\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3235 - accuracy: 0.0775 - val_loss: 3.2999 - val_accuracy: 0.0821\n",
      "Epoch 396/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3235 - accuracy: 0.0808\n",
      "Epoch 396: loss improved from 3.32305 to 3.32291, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3229 - accuracy: 0.0807 - val_loss: 3.2984 - val_accuracy: 0.0790\n",
      "Epoch 397/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3220 - accuracy: 0.0816\n",
      "Epoch 397: loss did not improve from 3.32291\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3230 - accuracy: 0.0813 - val_loss: 3.2992 - val_accuracy: 0.0821\n",
      "Epoch 398/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3231 - accuracy: 0.0803\n",
      "Epoch 398: loss improved from 3.32291 to 3.32287, saving model to best_model.h5\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3229 - accuracy: 0.0809 - val_loss: 3.2993 - val_accuracy: 0.0808\n",
      "Epoch 399/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3226 - accuracy: 0.0817\n",
      "Epoch 399: loss did not improve from 3.32287\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3229 - accuracy: 0.0818 - val_loss: 3.3005 - val_accuracy: 0.0814\n",
      "Epoch 400/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3208 - accuracy: 0.0817\n",
      "Epoch 400: loss improved from 3.32287 to 3.32282, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3228 - accuracy: 0.0810 - val_loss: 3.2993 - val_accuracy: 0.0833\n",
      "Epoch 401/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3227 - accuracy: 0.0828\n",
      "Epoch 401: loss did not improve from 3.32282\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3232 - accuracy: 0.0824 - val_loss: 3.2989 - val_accuracy: 0.0839\n",
      "Epoch 402/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3238 - accuracy: 0.0810\n",
      "Epoch 402: loss did not improve from 3.32282\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3238 - accuracy: 0.0810 - val_loss: 3.2998 - val_accuracy: 0.0821\n",
      "Epoch 403/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3231 - accuracy: 0.0825\n",
      "Epoch 403: loss improved from 3.32282 to 3.32217, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3222 - accuracy: 0.0823 - val_loss: 3.2984 - val_accuracy: 0.0796\n",
      "Epoch 404/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3230 - accuracy: 0.0789\n",
      "Epoch 404: loss did not improve from 3.32217\n",
      "256/256 [==============================] - 6s 21ms/step - loss: 3.3226 - accuracy: 0.0799 - val_loss: 3.3000 - val_accuracy: 0.0839\n",
      "Epoch 405/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3244 - accuracy: 0.0834\n",
      "Epoch 405: loss did not improve from 3.32217\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3245 - accuracy: 0.0835 - val_loss: 3.2991 - val_accuracy: 0.0821\n",
      "Epoch 406/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3204 - accuracy: 0.0801\n",
      "Epoch 406: loss did not improve from 3.32217\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3226 - accuracy: 0.0805 - val_loss: 3.2986 - val_accuracy: 0.0827\n",
      "Epoch 407/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3218 - accuracy: 0.0813\n",
      "Epoch 407: loss did not improve from 3.32217\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3222 - accuracy: 0.0812 - val_loss: 3.2976 - val_accuracy: 0.0821\n",
      "Epoch 408/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3221 - accuracy: 0.0805\n",
      "Epoch 408: loss improved from 3.32217 to 3.32208, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3221 - accuracy: 0.0805 - val_loss: 3.2982 - val_accuracy: 0.0839\n",
      "Epoch 409/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3232 - accuracy: 0.0805\n",
      "Epoch 409: loss did not improve from 3.32208\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3233 - accuracy: 0.0807 - val_loss: 3.2985 - val_accuracy: 0.0845\n",
      "Epoch 410/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3255 - accuracy: 0.0830\n",
      "Epoch 410: loss did not improve from 3.32208\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3238 - accuracy: 0.0827 - val_loss: 3.2984 - val_accuracy: 0.0851\n",
      "Epoch 411/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3223 - accuracy: 0.0828\n",
      "Epoch 411: loss did not improve from 3.32208\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3229 - accuracy: 0.0826 - val_loss: 3.2981 - val_accuracy: 0.0827\n",
      "Epoch 412/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3236 - accuracy: 0.0782\n",
      "Epoch 412: loss did not improve from 3.32208\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3228 - accuracy: 0.0794 - val_loss: 3.2984 - val_accuracy: 0.0870\n",
      "Epoch 413/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3216 - accuracy: 0.0816\n",
      "Epoch 413: loss improved from 3.32208 to 3.32182, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3218 - accuracy: 0.0816 - val_loss: 3.2984 - val_accuracy: 0.0845\n",
      "Epoch 414/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3225 - accuracy: 0.0806\n",
      "Epoch 414: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3227 - accuracy: 0.0805 - val_loss: 3.2979 - val_accuracy: 0.0839\n",
      "Epoch 415/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3235 - accuracy: 0.0795\n",
      "Epoch 415: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3242 - accuracy: 0.0799 - val_loss: 3.2986 - val_accuracy: 0.0839\n",
      "Epoch 416/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3226 - accuracy: 0.0807\n",
      "Epoch 416: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3226 - accuracy: 0.0807 - val_loss: 3.2985 - val_accuracy: 0.0808\n",
      "Epoch 417/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3217 - accuracy: 0.0833\n",
      "Epoch 417: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3220 - accuracy: 0.0833 - val_loss: 3.2975 - val_accuracy: 0.0876\n",
      "Epoch 418/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3199 - accuracy: 0.0797\n",
      "Epoch 418: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3224 - accuracy: 0.0786 - val_loss: 3.2996 - val_accuracy: 0.0851\n",
      "Epoch 419/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3225 - accuracy: 0.0821\n",
      "Epoch 419: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3224 - accuracy: 0.0821 - val_loss: 3.2986 - val_accuracy: 0.0876\n",
      "Epoch 420/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3270 - accuracy: 0.0812\n",
      "Epoch 420: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3235 - accuracy: 0.0822 - val_loss: 3.3001 - val_accuracy: 0.0808\n",
      "Epoch 421/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3216 - accuracy: 0.0795\n",
      "Epoch 421: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3223 - accuracy: 0.0789 - val_loss: 3.2978 - val_accuracy: 0.0796\n",
      "Epoch 422/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3222 - accuracy: 0.0782\n",
      "Epoch 422: loss did not improve from 3.32182\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3219 - accuracy: 0.0782 - val_loss: 3.2989 - val_accuracy: 0.0790\n",
      "Epoch 423/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3217 - accuracy: 0.0816\n",
      "Epoch 423: loss improved from 3.32182 to 3.32168, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3217 - accuracy: 0.0816 - val_loss: 3.2972 - val_accuracy: 0.0808\n",
      "Epoch 424/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3226 - accuracy: 0.0808\n",
      "Epoch 424: loss did not improve from 3.32168\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3226 - accuracy: 0.0813 - val_loss: 3.2973 - val_accuracy: 0.0845\n",
      "Epoch 425/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3228 - accuracy: 0.0824\n",
      "Epoch 425: loss did not improve from 3.32168\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3217 - accuracy: 0.0823 - val_loss: 3.2983 - val_accuracy: 0.0821\n",
      "Epoch 426/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3218 - accuracy: 0.0804\n",
      "Epoch 426: loss did not improve from 3.32168\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3217 - accuracy: 0.0805 - val_loss: 3.2971 - val_accuracy: 0.0814\n",
      "Epoch 427/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3234 - accuracy: 0.0789\n",
      "Epoch 427: loss did not improve from 3.32168\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3217 - accuracy: 0.0796 - val_loss: 3.2986 - val_accuracy: 0.0808\n",
      "Epoch 428/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3199 - accuracy: 0.0829\n",
      "Epoch 428: loss improved from 3.32168 to 3.32154, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3215 - accuracy: 0.0818 - val_loss: 3.2973 - val_accuracy: 0.0821\n",
      "Epoch 429/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3231 - accuracy: 0.0806\n",
      "Epoch 429: loss did not improve from 3.32154\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3230 - accuracy: 0.0806 - val_loss: 3.3006 - val_accuracy: 0.0851\n",
      "Epoch 430/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3222 - accuracy: 0.0821\n",
      "Epoch 430: loss did not improve from 3.32154\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3222 - accuracy: 0.0821 - val_loss: 3.2976 - val_accuracy: 0.0827\n",
      "Epoch 431/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3212 - accuracy: 0.0833\n",
      "Epoch 431: loss improved from 3.32154 to 3.32109, saving model to best_model.h5\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3211 - accuracy: 0.0833 - val_loss: 3.2974 - val_accuracy: 0.0796\n",
      "Epoch 432/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3219 - accuracy: 0.0793\n",
      "Epoch 432: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3216 - accuracy: 0.0796 - val_loss: 3.2980 - val_accuracy: 0.0821\n",
      "Epoch 433/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3222 - accuracy: 0.0816\n",
      "Epoch 433: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3221 - accuracy: 0.0815 - val_loss: 3.2974 - val_accuracy: 0.0845\n",
      "Epoch 434/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3214 - accuracy: 0.0800\n",
      "Epoch 434: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3215 - accuracy: 0.0800 - val_loss: 3.2965 - val_accuracy: 0.0845\n",
      "Epoch 435/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3214 - accuracy: 0.0812\n",
      "Epoch 435: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3214 - accuracy: 0.0810 - val_loss: 3.2970 - val_accuracy: 0.0851\n",
      "Epoch 436/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3196 - accuracy: 0.0814\n",
      "Epoch 436: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3214 - accuracy: 0.0817 - val_loss: 3.2961 - val_accuracy: 0.0821\n",
      "Epoch 437/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3221 - accuracy: 0.0827\n",
      "Epoch 437: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3218 - accuracy: 0.0829 - val_loss: 3.2967 - val_accuracy: 0.0802\n",
      "Epoch 438/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3223 - accuracy: 0.0829\n",
      "Epoch 438: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3217 - accuracy: 0.0832 - val_loss: 3.2958 - val_accuracy: 0.0814\n",
      "Epoch 439/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3239 - accuracy: 0.0782\n",
      "Epoch 439: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3229 - accuracy: 0.0782 - val_loss: 3.2981 - val_accuracy: 0.0839\n",
      "Epoch 440/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3202 - accuracy: 0.0835\n",
      "Epoch 440: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3218 - accuracy: 0.0824 - val_loss: 3.2965 - val_accuracy: 0.0827\n",
      "Epoch 441/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3220 - accuracy: 0.0817\n",
      "Epoch 441: loss did not improve from 3.32109\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3213 - accuracy: 0.0820 - val_loss: 3.2969 - val_accuracy: 0.0839\n",
      "Epoch 442/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3212 - accuracy: 0.0808\n",
      "Epoch 442: loss improved from 3.32109 to 3.32105, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3211 - accuracy: 0.0809 - val_loss: 3.2973 - val_accuracy: 0.0833\n",
      "Epoch 443/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3214 - accuracy: 0.0797\n",
      "Epoch 443: loss did not improve from 3.32105\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3216 - accuracy: 0.0796 - val_loss: 3.2980 - val_accuracy: 0.0790\n",
      "Epoch 444/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3188 - accuracy: 0.0820\n",
      "Epoch 444: loss did not improve from 3.32105\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3211 - accuracy: 0.0810 - val_loss: 3.2970 - val_accuracy: 0.0870\n",
      "Epoch 445/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3213 - accuracy: 0.0813\n",
      "Epoch 445: loss did not improve from 3.32105\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3212 - accuracy: 0.0821 - val_loss: 3.2968 - val_accuracy: 0.0814\n",
      "Epoch 446/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3234 - accuracy: 0.0824\n",
      "Epoch 446: loss did not improve from 3.32105\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3236 - accuracy: 0.0823 - val_loss: 3.2975 - val_accuracy: 0.0833\n",
      "Epoch 447/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3186 - accuracy: 0.0798\n",
      "Epoch 447: loss did not improve from 3.32105\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3213 - accuracy: 0.0799 - val_loss: 3.2962 - val_accuracy: 0.0821\n",
      "Epoch 448/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3208 - accuracy: 0.0809\n",
      "Epoch 448: loss improved from 3.32105 to 3.32083, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3208 - accuracy: 0.0809 - val_loss: 3.2971 - val_accuracy: 0.0827\n",
      "Epoch 449/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3222 - accuracy: 0.0806\n",
      "Epoch 449: loss did not improve from 3.32083\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3218 - accuracy: 0.0806 - val_loss: 3.2977 - val_accuracy: 0.0796\n",
      "Epoch 450/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3225 - accuracy: 0.0805\n",
      "Epoch 450: loss did not improve from 3.32083\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3211 - accuracy: 0.0805 - val_loss: 3.2981 - val_accuracy: 0.0808\n",
      "Epoch 451/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3208 - accuracy: 0.0823\n",
      "Epoch 451: loss improved from 3.32083 to 3.32056, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3206 - accuracy: 0.0822 - val_loss: 3.2972 - val_accuracy: 0.0845\n",
      "Epoch 452/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3210 - accuracy: 0.0777\n",
      "Epoch 452: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3210 - accuracy: 0.0777 - val_loss: 3.2969 - val_accuracy: 0.0821\n",
      "Epoch 453/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3184 - accuracy: 0.0808\n",
      "Epoch 453: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3210 - accuracy: 0.0791 - val_loss: 3.2968 - val_accuracy: 0.0863\n",
      "Epoch 454/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3219 - accuracy: 0.0802\n",
      "Epoch 454: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3206 - accuracy: 0.0802 - val_loss: 3.2958 - val_accuracy: 0.0863\n",
      "Epoch 455/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3221 - accuracy: 0.0803\n",
      "Epoch 455: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3209 - accuracy: 0.0809 - val_loss: 3.2965 - val_accuracy: 0.0796\n",
      "Epoch 456/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3196 - accuracy: 0.0818\n",
      "Epoch 456: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3210 - accuracy: 0.0813 - val_loss: 3.2973 - val_accuracy: 0.0845\n",
      "Epoch 457/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3167 - accuracy: 0.0791\n",
      "Epoch 457: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3208 - accuracy: 0.0790 - val_loss: 3.2975 - val_accuracy: 0.0833\n",
      "Epoch 458/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3230 - accuracy: 0.0838\n",
      "Epoch 458: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3210 - accuracy: 0.0842 - val_loss: 3.2972 - val_accuracy: 0.0833\n",
      "Epoch 459/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3212 - accuracy: 0.0800\n",
      "Epoch 459: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3212 - accuracy: 0.0800 - val_loss: 3.2976 - val_accuracy: 0.0790\n",
      "Epoch 460/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3212 - accuracy: 0.0805\n",
      "Epoch 460: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3207 - accuracy: 0.0806 - val_loss: 3.2965 - val_accuracy: 0.0821\n",
      "Epoch 461/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3213 - accuracy: 0.0803\n",
      "Epoch 461: loss did not improve from 3.32056\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3212 - accuracy: 0.0802 - val_loss: 3.2972 - val_accuracy: 0.0863\n",
      "Epoch 462/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3193 - accuracy: 0.0786\n",
      "Epoch 462: loss improved from 3.32056 to 3.32054, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3205 - accuracy: 0.0791 - val_loss: 3.2966 - val_accuracy: 0.0833\n",
      "Epoch 463/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3213 - accuracy: 0.0829\n",
      "Epoch 463: loss improved from 3.32054 to 3.32018, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3202 - accuracy: 0.0832 - val_loss: 3.2967 - val_accuracy: 0.0839\n",
      "Epoch 464/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3221 - accuracy: 0.0807\n",
      "Epoch 464: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3221 - accuracy: 0.0807 - val_loss: 3.2963 - val_accuracy: 0.0876\n",
      "Epoch 465/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3182 - accuracy: 0.0843\n",
      "Epoch 465: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3203 - accuracy: 0.0835 - val_loss: 3.2977 - val_accuracy: 0.0870\n",
      "Epoch 466/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3219 - accuracy: 0.0816\n",
      "Epoch 466: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3207 - accuracy: 0.0810 - val_loss: 3.2965 - val_accuracy: 0.0814\n",
      "Epoch 467/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3203 - accuracy: 0.0806\n",
      "Epoch 467: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3202 - accuracy: 0.0806 - val_loss: 3.2956 - val_accuracy: 0.0851\n",
      "Epoch 468/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3202 - accuracy: 0.0797\n",
      "Epoch 468: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3202 - accuracy: 0.0796 - val_loss: 3.2960 - val_accuracy: 0.0814\n",
      "Epoch 469/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3188 - accuracy: 0.0790\n",
      "Epoch 469: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3203 - accuracy: 0.0793 - val_loss: 3.2986 - val_accuracy: 0.0778\n",
      "Epoch 470/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3214 - accuracy: 0.0806\n",
      "Epoch 470: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3215 - accuracy: 0.0806 - val_loss: 3.2972 - val_accuracy: 0.0814\n",
      "Epoch 471/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3203 - accuracy: 0.0825\n",
      "Epoch 471: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3202 - accuracy: 0.0826 - val_loss: 3.2963 - val_accuracy: 0.0814\n",
      "Epoch 472/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3208 - accuracy: 0.0804\n",
      "Epoch 472: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3207 - accuracy: 0.0805 - val_loss: 3.2964 - val_accuracy: 0.0814\n",
      "Epoch 473/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3230 - accuracy: 0.0811\n",
      "Epoch 473: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3227 - accuracy: 0.0812 - val_loss: 3.2968 - val_accuracy: 0.0839\n",
      "Epoch 474/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3213 - accuracy: 0.0804\n",
      "Epoch 474: loss did not improve from 3.32018\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3207 - accuracy: 0.0794 - val_loss: 3.2968 - val_accuracy: 0.0808\n",
      "Epoch 475/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3203 - accuracy: 0.0817\n",
      "Epoch 475: loss improved from 3.32018 to 3.31993, saving model to best_model.h5\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3199 - accuracy: 0.0817 - val_loss: 3.2954 - val_accuracy: 0.0802\n",
      "Epoch 476/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3208 - accuracy: 0.0822\n",
      "Epoch 476: loss did not improve from 3.31993\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3212 - accuracy: 0.0821 - val_loss: 3.2964 - val_accuracy: 0.0814\n",
      "Epoch 477/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3213 - accuracy: 0.0825\n",
      "Epoch 477: loss did not improve from 3.31993\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3204 - accuracy: 0.0822 - val_loss: 3.3150 - val_accuracy: 0.0778\n",
      "Epoch 478/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3192 - accuracy: 0.0800\n",
      "Epoch 478: loss did not improve from 3.31993\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3203 - accuracy: 0.0802 - val_loss: 3.2962 - val_accuracy: 0.0821\n",
      "Epoch 479/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3196 - accuracy: 0.0825\n",
      "Epoch 479: loss did not improve from 3.31993\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3201 - accuracy: 0.0820 - val_loss: 3.2950 - val_accuracy: 0.0833\n",
      "Epoch 480/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3190 - accuracy: 0.0805\n",
      "Epoch 480: loss improved from 3.31993 to 3.31992, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3199 - accuracy: 0.0804 - val_loss: 3.2958 - val_accuracy: 0.0796\n",
      "Epoch 481/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3206 - accuracy: 0.0798\n",
      "Epoch 481: loss did not improve from 3.31992\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3200 - accuracy: 0.0799 - val_loss: 3.2959 - val_accuracy: 0.0808\n",
      "Epoch 482/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3218 - accuracy: 0.0816\n",
      "Epoch 482: loss did not improve from 3.31992\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3208 - accuracy: 0.0822 - val_loss: 3.2977 - val_accuracy: 0.0851\n",
      "Epoch 483/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0819\n",
      "Epoch 483: loss improved from 3.31992 to 3.31976, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3198 - accuracy: 0.0812 - val_loss: 3.2960 - val_accuracy: 0.0814\n",
      "Epoch 484/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3191 - accuracy: 0.0779\n",
      "Epoch 484: loss did not improve from 3.31976\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3199 - accuracy: 0.0788 - val_loss: 3.2959 - val_accuracy: 0.0814\n",
      "Epoch 485/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3154 - accuracy: 0.0821\n",
      "Epoch 485: loss did not improve from 3.31976\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3199 - accuracy: 0.0817 - val_loss: 3.2965 - val_accuracy: 0.0772\n",
      "Epoch 486/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3185 - accuracy: 0.0808\n",
      "Epoch 486: loss did not improve from 3.31976\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3203 - accuracy: 0.0806 - val_loss: 3.2959 - val_accuracy: 0.0833\n",
      "Epoch 487/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3172 - accuracy: 0.0822\n",
      "Epoch 487: loss did not improve from 3.31976\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3200 - accuracy: 0.0821 - val_loss: 3.2985 - val_accuracy: 0.0796\n",
      "Epoch 488/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3209 - accuracy: 0.0840\n",
      "Epoch 488: loss did not improve from 3.31976\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3201 - accuracy: 0.0839 - val_loss: 3.2955 - val_accuracy: 0.0808\n",
      "Epoch 489/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3218 - accuracy: 0.0787\n",
      "Epoch 489: loss did not improve from 3.31976\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3209 - accuracy: 0.0790 - val_loss: 3.2952 - val_accuracy: 0.0833\n",
      "Epoch 490/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3203 - accuracy: 0.0788\n",
      "Epoch 490: loss did not improve from 3.31976\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3199 - accuracy: 0.0786 - val_loss: 3.2949 - val_accuracy: 0.0851\n",
      "Epoch 491/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3201 - accuracy: 0.0791\n",
      "Epoch 491: loss improved from 3.31976 to 3.31930, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3193 - accuracy: 0.0800 - val_loss: 3.2953 - val_accuracy: 0.0827\n",
      "Epoch 492/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3198 - accuracy: 0.0817\n",
      "Epoch 492: loss did not improve from 3.31930\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3200 - accuracy: 0.0816 - val_loss: 3.2967 - val_accuracy: 0.0876\n",
      "Epoch 493/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3213 - accuracy: 0.0805\n",
      "Epoch 493: loss did not improve from 3.31930\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3202 - accuracy: 0.0807 - val_loss: 3.2966 - val_accuracy: 0.0839\n",
      "Epoch 494/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3176 - accuracy: 0.0819\n",
      "Epoch 494: loss improved from 3.31930 to 3.31921, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3192 - accuracy: 0.0820 - val_loss: 3.2965 - val_accuracy: 0.0833\n",
      "Epoch 495/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3178 - accuracy: 0.0819\n",
      "Epoch 495: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3198 - accuracy: 0.0821 - val_loss: 3.2980 - val_accuracy: 0.0802\n",
      "Epoch 496/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3190 - accuracy: 0.0831\n",
      "Epoch 496: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3203 - accuracy: 0.0829 - val_loss: 3.2962 - val_accuracy: 0.0821\n",
      "Epoch 497/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3195 - accuracy: 0.0820\n",
      "Epoch 497: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3197 - accuracy: 0.0820 - val_loss: 3.2952 - val_accuracy: 0.0821\n",
      "Epoch 498/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3185 - accuracy: 0.0798\n",
      "Epoch 498: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3198 - accuracy: 0.0793 - val_loss: 3.2942 - val_accuracy: 0.0839\n",
      "Epoch 499/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3208 - accuracy: 0.0791\n",
      "Epoch 499: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0800 - val_loss: 3.2963 - val_accuracy: 0.0845\n",
      "Epoch 500/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0806\n",
      "Epoch 500: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3193 - accuracy: 0.0802 - val_loss: 3.2967 - val_accuracy: 0.0802\n",
      "Epoch 501/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3194 - accuracy: 0.0809\n",
      "Epoch 501: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0809 - val_loss: 3.2952 - val_accuracy: 0.0814\n",
      "Epoch 502/5000\n",
      "233/256 [==========================>...] - ETA: 0s - loss: 3.3215 - accuracy: 0.0819\n",
      "Epoch 502: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3218 - accuracy: 0.0827 - val_loss: 3.2962 - val_accuracy: 0.0808\n",
      "Epoch 503/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3216 - accuracy: 0.0808\n",
      "Epoch 503: loss did not improve from 3.31921\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0812 - val_loss: 3.2959 - val_accuracy: 0.0851\n",
      "Epoch 504/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3184 - accuracy: 0.0830\n",
      "Epoch 504: loss improved from 3.31921 to 3.31908, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3191 - accuracy: 0.0827 - val_loss: 3.2949 - val_accuracy: 0.0821\n",
      "Epoch 505/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3195 - accuracy: 0.0828\n",
      "Epoch 505: loss did not improve from 3.31908\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3193 - accuracy: 0.0822 - val_loss: 3.2948 - val_accuracy: 0.0808\n",
      "Epoch 506/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3187 - accuracy: 0.0817\n",
      "Epoch 506: loss did not improve from 3.31908\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0810 - val_loss: 3.2949 - val_accuracy: 0.0778\n",
      "Epoch 507/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3210 - accuracy: 0.0806\n",
      "Epoch 507: loss did not improve from 3.31908\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3205 - accuracy: 0.0806 - val_loss: 3.2952 - val_accuracy: 0.0833\n",
      "Epoch 508/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3212 - accuracy: 0.0790\n",
      "Epoch 508: loss did not improve from 3.31908\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 3.3196 - accuracy: 0.0788 - val_loss: 3.2960 - val_accuracy: 0.0827\n",
      "Epoch 509/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3157 - accuracy: 0.0818\n",
      "Epoch 509: loss improved from 3.31908 to 3.31905, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3190 - accuracy: 0.0818 - val_loss: 3.2968 - val_accuracy: 0.0821\n",
      "Epoch 510/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3177 - accuracy: 0.0814\n",
      "Epoch 510: loss did not improve from 3.31905\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3193 - accuracy: 0.0807 - val_loss: 3.2950 - val_accuracy: 0.0833\n",
      "Epoch 511/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3197 - accuracy: 0.0819\n",
      "Epoch 511: loss did not improve from 3.31905\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0818 - val_loss: 3.2952 - val_accuracy: 0.0870\n",
      "Epoch 512/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3186 - accuracy: 0.0794\n",
      "Epoch 512: loss did not improve from 3.31905\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3197 - accuracy: 0.0798 - val_loss: 3.2955 - val_accuracy: 0.0833\n",
      "Epoch 513/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3187 - accuracy: 0.0802\n",
      "Epoch 513: loss did not improve from 3.31905\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3195 - accuracy: 0.0805 - val_loss: 3.2946 - val_accuracy: 0.0851\n",
      "Epoch 514/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3189 - accuracy: 0.0815\n",
      "Epoch 514: loss improved from 3.31905 to 3.31886, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3189 - accuracy: 0.0815 - val_loss: 3.2954 - val_accuracy: 0.0851\n",
      "Epoch 515/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3176 - accuracy: 0.0789\n",
      "Epoch 515: loss improved from 3.31886 to 3.31882, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3188 - accuracy: 0.0791 - val_loss: 3.2949 - val_accuracy: 0.0876\n",
      "Epoch 516/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3201 - accuracy: 0.0823\n",
      "Epoch 516: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3200 - accuracy: 0.0832 - val_loss: 3.2941 - val_accuracy: 0.0876\n",
      "Epoch 517/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3178 - accuracy: 0.0826\n",
      "Epoch 517: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3192 - accuracy: 0.0823 - val_loss: 3.2979 - val_accuracy: 0.0876\n",
      "Epoch 518/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3181 - accuracy: 0.0829\n",
      "Epoch 518: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3195 - accuracy: 0.0824 - val_loss: 3.2948 - val_accuracy: 0.0839\n",
      "Epoch 519/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3192 - accuracy: 0.0808\n",
      "Epoch 519: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 3.3192 - accuracy: 0.0809 - val_loss: 3.2950 - val_accuracy: 0.0845\n",
      "Epoch 520/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3200 - accuracy: 0.0777\n",
      "Epoch 520: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3189 - accuracy: 0.0777 - val_loss: 3.2946 - val_accuracy: 0.0863\n",
      "Epoch 521/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3198 - accuracy: 0.0807\n",
      "Epoch 521: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3195 - accuracy: 0.0811 - val_loss: 3.2949 - val_accuracy: 0.0851\n",
      "Epoch 522/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3177 - accuracy: 0.0807\n",
      "Epoch 522: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3202 - accuracy: 0.0804 - val_loss: 3.2952 - val_accuracy: 0.0839\n",
      "Epoch 523/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3199 - accuracy: 0.0787\n",
      "Epoch 523: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3190 - accuracy: 0.0793 - val_loss: 3.2941 - val_accuracy: 0.0857\n",
      "Epoch 524/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3195 - accuracy: 0.0822\n",
      "Epoch 524: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3208 - accuracy: 0.0816 - val_loss: 3.2962 - val_accuracy: 0.0833\n",
      "Epoch 525/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3182 - accuracy: 0.0815\n",
      "Epoch 525: loss did not improve from 3.31882\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3197 - accuracy: 0.0811 - val_loss: 3.2945 - val_accuracy: 0.0851\n",
      "Epoch 526/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0810\n",
      "Epoch 526: loss improved from 3.31882 to 3.31875, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3187 - accuracy: 0.0807 - val_loss: 3.2953 - val_accuracy: 0.0845\n",
      "Epoch 527/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3177 - accuracy: 0.0811\n",
      "Epoch 527: loss improved from 3.31875 to 3.31854, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3185 - accuracy: 0.0807 - val_loss: 3.2942 - val_accuracy: 0.0827\n",
      "Epoch 528/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3203 - accuracy: 0.0803\n",
      "Epoch 528: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3198 - accuracy: 0.0802 - val_loss: 3.2947 - val_accuracy: 0.0870\n",
      "Epoch 529/5000\n",
      "234/256 [==========================>...] - ETA: 0s - loss: 3.3189 - accuracy: 0.0801\n",
      "Epoch 529: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3189 - accuracy: 0.0793 - val_loss: 3.2949 - val_accuracy: 0.0827\n",
      "Epoch 530/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3171 - accuracy: 0.0823\n",
      "Epoch 530: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3187 - accuracy: 0.0823 - val_loss: 3.2957 - val_accuracy: 0.0796\n",
      "Epoch 531/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3187 - accuracy: 0.0804\n",
      "Epoch 531: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3197 - accuracy: 0.0805 - val_loss: 3.2953 - val_accuracy: 0.0827\n",
      "Epoch 532/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3185 - accuracy: 0.0819\n",
      "Epoch 532: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3188 - accuracy: 0.0818 - val_loss: 3.2944 - val_accuracy: 0.0827\n",
      "Epoch 533/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3229 - accuracy: 0.0814\n",
      "Epoch 533: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3190 - accuracy: 0.0816 - val_loss: 3.2946 - val_accuracy: 0.0894\n",
      "Epoch 534/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3152 - accuracy: 0.0832\n",
      "Epoch 534: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3189 - accuracy: 0.0828 - val_loss: 3.2956 - val_accuracy: 0.0857\n",
      "Epoch 535/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0814\n",
      "Epoch 535: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3186 - accuracy: 0.0816 - val_loss: 3.2952 - val_accuracy: 0.0851\n",
      "Epoch 536/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3185 - accuracy: 0.0810\n",
      "Epoch 536: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3198 - accuracy: 0.0804 - val_loss: 3.2948 - val_accuracy: 0.0839\n",
      "Epoch 537/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3191 - accuracy: 0.0779\n",
      "Epoch 537: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3190 - accuracy: 0.0779 - val_loss: 3.2944 - val_accuracy: 0.0802\n",
      "Epoch 538/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3217 - accuracy: 0.0816\n",
      "Epoch 538: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3185 - accuracy: 0.0817 - val_loss: 3.2949 - val_accuracy: 0.0821\n",
      "Epoch 539/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3123 - accuracy: 0.0824\n",
      "Epoch 539: loss did not improve from 3.31854\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3190 - accuracy: 0.0815 - val_loss: 3.2945 - val_accuracy: 0.0851\n",
      "Epoch 540/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3180 - accuracy: 0.0805\n",
      "Epoch 540: loss improved from 3.31854 to 3.31842, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3184 - accuracy: 0.0806 - val_loss: 3.2942 - val_accuracy: 0.0778\n",
      "Epoch 541/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3244 - accuracy: 0.0812\n",
      "Epoch 541: loss did not improve from 3.31842\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0824 - val_loss: 3.2973 - val_accuracy: 0.0814\n",
      "Epoch 542/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3195 - accuracy: 0.0815\n",
      "Epoch 542: loss did not improve from 3.31842\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0818 - val_loss: 3.2952 - val_accuracy: 0.0839\n",
      "Epoch 543/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3213 - accuracy: 0.0808\n",
      "Epoch 543: loss did not improve from 3.31842\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3193 - accuracy: 0.0810 - val_loss: 3.2950 - val_accuracy: 0.0863\n",
      "Epoch 544/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3192 - accuracy: 0.0822\n",
      "Epoch 544: loss did not improve from 3.31842\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3191 - accuracy: 0.0822 - val_loss: 3.2951 - val_accuracy: 0.0863\n",
      "Epoch 545/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3191 - accuracy: 0.0817\n",
      "Epoch 545: loss did not improve from 3.31842\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3187 - accuracy: 0.0813 - val_loss: 3.2937 - val_accuracy: 0.0851\n",
      "Epoch 546/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3211 - accuracy: 0.0817\n",
      "Epoch 546: loss did not improve from 3.31842\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3191 - accuracy: 0.0817 - val_loss: 3.2946 - val_accuracy: 0.0863\n",
      "Epoch 547/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3203 - accuracy: 0.0798\n",
      "Epoch 547: loss did not improve from 3.31842\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3191 - accuracy: 0.0805 - val_loss: 3.2951 - val_accuracy: 0.0833\n",
      "Epoch 548/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3152 - accuracy: 0.0839\n",
      "Epoch 548: loss improved from 3.31842 to 3.31838, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3184 - accuracy: 0.0832 - val_loss: 3.2959 - val_accuracy: 0.0784\n",
      "Epoch 549/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3178 - accuracy: 0.0813\n",
      "Epoch 549: loss did not improve from 3.31838\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3192 - accuracy: 0.0813 - val_loss: 3.2937 - val_accuracy: 0.0833\n",
      "Epoch 550/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3196 - accuracy: 0.0835\n",
      "Epoch 550: loss did not improve from 3.31838\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3189 - accuracy: 0.0832 - val_loss: 3.2951 - val_accuracy: 0.0808\n",
      "Epoch 551/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3172 - accuracy: 0.0826\n",
      "Epoch 551: loss did not improve from 3.31838\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3190 - accuracy: 0.0818 - val_loss: 3.2962 - val_accuracy: 0.0814\n",
      "Epoch 552/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3133 - accuracy: 0.0846\n",
      "Epoch 552: loss did not improve from 3.31838\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3187 - accuracy: 0.0831 - val_loss: 3.2939 - val_accuracy: 0.0802\n",
      "Epoch 553/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3219 - accuracy: 0.0824\n",
      "Epoch 553: loss did not improve from 3.31838\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3185 - accuracy: 0.0823 - val_loss: 3.2936 - val_accuracy: 0.0814\n",
      "Epoch 554/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3138 - accuracy: 0.0808\n",
      "Epoch 554: loss improved from 3.31838 to 3.31837, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3184 - accuracy: 0.0801 - val_loss: 3.2953 - val_accuracy: 0.0857\n",
      "Epoch 555/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3182 - accuracy: 0.0818\n",
      "Epoch 555: loss did not improve from 3.31837\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3188 - accuracy: 0.0817 - val_loss: 3.2946 - val_accuracy: 0.0821\n",
      "Epoch 556/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3183 - accuracy: 0.0815\n",
      "Epoch 556: loss did not improve from 3.31837\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3184 - accuracy: 0.0815 - val_loss: 3.2961 - val_accuracy: 0.0851\n",
      "Epoch 557/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3151 - accuracy: 0.0809\n",
      "Epoch 557: loss did not improve from 3.31837\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3185 - accuracy: 0.0813 - val_loss: 3.2951 - val_accuracy: 0.0827\n",
      "Epoch 558/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3197 - accuracy: 0.0805\n",
      "Epoch 558: loss did not improve from 3.31837\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3187 - accuracy: 0.0802 - val_loss: 3.2946 - val_accuracy: 0.0851\n",
      "Epoch 559/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3215 - accuracy: 0.0814\n",
      "Epoch 559: loss did not improve from 3.31837\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3197 - accuracy: 0.0816 - val_loss: 3.2949 - val_accuracy: 0.0833\n",
      "Epoch 560/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3187 - accuracy: 0.0833\n",
      "Epoch 560: loss did not improve from 3.31837\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3187 - accuracy: 0.0833 - val_loss: 3.2946 - val_accuracy: 0.0827\n",
      "Epoch 561/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3177 - accuracy: 0.0814\n",
      "Epoch 561: loss improved from 3.31837 to 3.31808, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3181 - accuracy: 0.0813 - val_loss: 3.2944 - val_accuracy: 0.0827\n",
      "Epoch 562/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3189 - accuracy: 0.0809\n",
      "Epoch 562: loss did not improve from 3.31808\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3189 - accuracy: 0.0809 - val_loss: 3.2966 - val_accuracy: 0.0827\n",
      "Epoch 563/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3193 - accuracy: 0.0833\n",
      "Epoch 563: loss did not improve from 3.31808\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3186 - accuracy: 0.0824 - val_loss: 3.2942 - val_accuracy: 0.0821\n",
      "Epoch 564/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3214 - accuracy: 0.0823\n",
      "Epoch 564: loss improved from 3.31808 to 3.31796, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3180 - accuracy: 0.0826 - val_loss: 3.2947 - val_accuracy: 0.0839\n",
      "Epoch 565/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3206 - accuracy: 0.0801\n",
      "Epoch 565: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3206 - accuracy: 0.0809 - val_loss: 3.2954 - val_accuracy: 0.0845\n",
      "Epoch 566/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3188 - accuracy: 0.0795\n",
      "Epoch 566: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3185 - accuracy: 0.0793 - val_loss: 3.2941 - val_accuracy: 0.0876\n",
      "Epoch 567/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3131 - accuracy: 0.0813\n",
      "Epoch 567: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3186 - accuracy: 0.0807 - val_loss: 3.2951 - val_accuracy: 0.0863\n",
      "Epoch 568/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3188 - accuracy: 0.0782\n",
      "Epoch 568: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3190 - accuracy: 0.0784 - val_loss: 3.2933 - val_accuracy: 0.0863\n",
      "Epoch 569/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3210 - accuracy: 0.0815\n",
      "Epoch 569: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3197 - accuracy: 0.0820 - val_loss: 3.2949 - val_accuracy: 0.0851\n",
      "Epoch 570/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3214 - accuracy: 0.0790\n",
      "Epoch 570: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3193 - accuracy: 0.0794 - val_loss: 3.2947 - val_accuracy: 0.0827\n",
      "Epoch 571/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3212 - accuracy: 0.0825\n",
      "Epoch 571: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3184 - accuracy: 0.0823 - val_loss: 3.2948 - val_accuracy: 0.0833\n",
      "Epoch 572/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3192 - accuracy: 0.0810\n",
      "Epoch 572: loss did not improve from 3.31796\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3190 - accuracy: 0.0811 - val_loss: 3.2950 - val_accuracy: 0.0833\n",
      "Epoch 573/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3182 - accuracy: 0.0836\n",
      "Epoch 573: loss improved from 3.31796 to 3.31765, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3177 - accuracy: 0.0833 - val_loss: 3.3020 - val_accuracy: 0.0857\n",
      "Epoch 574/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0818\n",
      "Epoch 574: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3187 - accuracy: 0.0816 - val_loss: 3.2956 - val_accuracy: 0.0821\n",
      "Epoch 575/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3194 - accuracy: 0.0817\n",
      "Epoch 575: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3183 - accuracy: 0.0821 - val_loss: 3.2947 - val_accuracy: 0.0814\n",
      "Epoch 576/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3176 - accuracy: 0.0804\n",
      "Epoch 576: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3179 - accuracy: 0.0800 - val_loss: 3.2930 - val_accuracy: 0.0821\n",
      "Epoch 577/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3210 - accuracy: 0.0820\n",
      "Epoch 577: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3191 - accuracy: 0.0822 - val_loss: 3.2960 - val_accuracy: 0.0851\n",
      "Epoch 578/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0813\n",
      "Epoch 578: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3182 - accuracy: 0.0810 - val_loss: 3.2947 - val_accuracy: 0.0821\n",
      "Epoch 579/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0813\n",
      "Epoch 579: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3185 - accuracy: 0.0807 - val_loss: 3.2943 - val_accuracy: 0.0851\n",
      "Epoch 580/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3186 - accuracy: 0.0821\n",
      "Epoch 580: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3182 - accuracy: 0.0828 - val_loss: 3.2950 - val_accuracy: 0.0851\n",
      "Epoch 581/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0827\n",
      "Epoch 581: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3183 - accuracy: 0.0831 - val_loss: 3.2936 - val_accuracy: 0.0833\n",
      "Epoch 582/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3181 - accuracy: 0.0801\n",
      "Epoch 582: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3182 - accuracy: 0.0795 - val_loss: 3.2950 - val_accuracy: 0.0870\n",
      "Epoch 583/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3226 - accuracy: 0.0822\n",
      "Epoch 583: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3184 - accuracy: 0.0823 - val_loss: 3.2939 - val_accuracy: 0.0851\n",
      "Epoch 584/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0815\n",
      "Epoch 584: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3184 - accuracy: 0.0807 - val_loss: 3.2951 - val_accuracy: 0.0814\n",
      "Epoch 585/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3177 - accuracy: 0.0816\n",
      "Epoch 585: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3184 - accuracy: 0.0816 - val_loss: 3.2931 - val_accuracy: 0.0833\n",
      "Epoch 586/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3186 - accuracy: 0.0823\n",
      "Epoch 586: loss did not improve from 3.31765\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3183 - accuracy: 0.0820 - val_loss: 3.2945 - val_accuracy: 0.0833\n",
      "Epoch 587/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3164 - accuracy: 0.0819\n",
      "Epoch 587: loss improved from 3.31765 to 3.31739, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3174 - accuracy: 0.0822 - val_loss: 3.2933 - val_accuracy: 0.0870\n",
      "Epoch 588/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3169 - accuracy: 0.0816\n",
      "Epoch 588: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3197 - accuracy: 0.0816 - val_loss: 3.2947 - val_accuracy: 0.0857\n",
      "Epoch 589/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3162 - accuracy: 0.0803\n",
      "Epoch 589: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3181 - accuracy: 0.0806 - val_loss: 3.2932 - val_accuracy: 0.0839\n",
      "Epoch 590/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3174 - accuracy: 0.0823\n",
      "Epoch 590: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3179 - accuracy: 0.0821 - val_loss: 3.2933 - val_accuracy: 0.0827\n",
      "Epoch 591/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3197 - accuracy: 0.0826\n",
      "Epoch 591: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3191 - accuracy: 0.0829 - val_loss: 3.2955 - val_accuracy: 0.0851\n",
      "Epoch 592/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3191 - accuracy: 0.0812\n",
      "Epoch 592: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3182 - accuracy: 0.0817 - val_loss: 3.2943 - val_accuracy: 0.0888\n",
      "Epoch 593/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0825\n",
      "Epoch 593: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3176 - accuracy: 0.0817 - val_loss: 3.2945 - val_accuracy: 0.0821\n",
      "Epoch 594/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3184 - accuracy: 0.0802\n",
      "Epoch 594: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3184 - accuracy: 0.0802 - val_loss: 3.2954 - val_accuracy: 0.0851\n",
      "Epoch 595/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3188 - accuracy: 0.0820\n",
      "Epoch 595: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3186 - accuracy: 0.0815 - val_loss: 3.2963 - val_accuracy: 0.0821\n",
      "Epoch 596/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0812\n",
      "Epoch 596: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3179 - accuracy: 0.0812 - val_loss: 3.2945 - val_accuracy: 0.0802\n",
      "Epoch 597/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3172 - accuracy: 0.0823\n",
      "Epoch 597: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3179 - accuracy: 0.0821 - val_loss: 3.2938 - val_accuracy: 0.0851\n",
      "Epoch 598/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3180 - accuracy: 0.0824\n",
      "Epoch 598: loss did not improve from 3.31739\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3178 - accuracy: 0.0824 - val_loss: 3.2934 - val_accuracy: 0.0863\n",
      "Epoch 599/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0808\n",
      "Epoch 599: loss improved from 3.31739 to 3.31721, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3172 - accuracy: 0.0810 - val_loss: 3.2941 - val_accuracy: 0.0839\n",
      "Epoch 600/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3158 - accuracy: 0.0836\n",
      "Epoch 600: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3182 - accuracy: 0.0824 - val_loss: 3.2938 - val_accuracy: 0.0845\n",
      "Epoch 601/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3172 - accuracy: 0.0807\n",
      "Epoch 601: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3178 - accuracy: 0.0806 - val_loss: 3.2939 - val_accuracy: 0.0833\n",
      "Epoch 602/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3202 - accuracy: 0.0805\n",
      "Epoch 602: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3205 - accuracy: 0.0805 - val_loss: 3.2951 - val_accuracy: 0.0870\n",
      "Epoch 603/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3164 - accuracy: 0.0833\n",
      "Epoch 603: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3179 - accuracy: 0.0824 - val_loss: 3.2937 - val_accuracy: 0.0863\n",
      "Epoch 604/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3200 - accuracy: 0.0806\n",
      "Epoch 604: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3205 - accuracy: 0.0809 - val_loss: 3.2960 - val_accuracy: 0.0821\n",
      "Epoch 605/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3192 - accuracy: 0.0812\n",
      "Epoch 605: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3182 - accuracy: 0.0807 - val_loss: 3.2945 - val_accuracy: 0.0833\n",
      "Epoch 606/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3203 - accuracy: 0.0801\n",
      "Epoch 606: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0805 - val_loss: 3.2954 - val_accuracy: 0.0857\n",
      "Epoch 607/5000\n",
      "235/256 [==========================>...] - ETA: 0s - loss: 3.3161 - accuracy: 0.0827\n",
      "Epoch 607: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3177 - accuracy: 0.0822 - val_loss: 3.2948 - val_accuracy: 0.0821\n",
      "Epoch 608/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3197 - accuracy: 0.0784\n",
      "Epoch 608: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3186 - accuracy: 0.0782 - val_loss: 3.2944 - val_accuracy: 0.0839\n",
      "Epoch 609/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3176 - accuracy: 0.0815\n",
      "Epoch 609: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3179 - accuracy: 0.0813 - val_loss: 3.2935 - val_accuracy: 0.0851\n",
      "Epoch 610/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0812\n",
      "Epoch 610: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3182 - accuracy: 0.0805 - val_loss: 3.2942 - val_accuracy: 0.0851\n",
      "Epoch 611/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3197 - accuracy: 0.0820\n",
      "Epoch 611: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3182 - accuracy: 0.0821 - val_loss: 3.2958 - val_accuracy: 0.0857\n",
      "Epoch 612/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3174 - accuracy: 0.0801\n",
      "Epoch 612: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3183 - accuracy: 0.0806 - val_loss: 3.2948 - val_accuracy: 0.0814\n",
      "Epoch 613/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3193 - accuracy: 0.0812\n",
      "Epoch 613: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3178 - accuracy: 0.0811 - val_loss: 3.2942 - val_accuracy: 0.0863\n",
      "Epoch 614/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3184 - accuracy: 0.0837\n",
      "Epoch 614: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3172 - accuracy: 0.0839 - val_loss: 3.2951 - val_accuracy: 0.0802\n",
      "Epoch 615/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0814\n",
      "Epoch 615: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3186 - accuracy: 0.0820 - val_loss: 3.2967 - val_accuracy: 0.0821\n",
      "Epoch 616/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0812\n",
      "Epoch 616: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3182 - accuracy: 0.0806 - val_loss: 3.2945 - val_accuracy: 0.0833\n",
      "Epoch 617/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3200 - accuracy: 0.0834\n",
      "Epoch 617: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3183 - accuracy: 0.0828 - val_loss: 3.2942 - val_accuracy: 0.0814\n",
      "Epoch 618/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3186 - accuracy: 0.0818\n",
      "Epoch 618: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3192 - accuracy: 0.0817 - val_loss: 3.2947 - val_accuracy: 0.0870\n",
      "Epoch 619/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3177 - accuracy: 0.0827\n",
      "Epoch 619: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3180 - accuracy: 0.0826 - val_loss: 3.2946 - val_accuracy: 0.0839\n",
      "Epoch 620/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3177 - accuracy: 0.0819\n",
      "Epoch 620: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3180 - accuracy: 0.0818 - val_loss: 3.2943 - val_accuracy: 0.0845\n",
      "Epoch 621/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3196 - accuracy: 0.0816\n",
      "Epoch 621: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3177 - accuracy: 0.0821 - val_loss: 3.2939 - val_accuracy: 0.0876\n",
      "Epoch 622/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3172 - accuracy: 0.0807\n",
      "Epoch 622: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3185 - accuracy: 0.0806 - val_loss: 3.2957 - val_accuracy: 0.0827\n",
      "Epoch 623/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3180 - accuracy: 0.0824\n",
      "Epoch 623: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3185 - accuracy: 0.0818 - val_loss: 3.2945 - val_accuracy: 0.0851\n",
      "Epoch 624/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3185 - accuracy: 0.0820\n",
      "Epoch 624: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3175 - accuracy: 0.0823 - val_loss: 3.2941 - val_accuracy: 0.0857\n",
      "Epoch 625/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0817\n",
      "Epoch 625: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3173 - accuracy: 0.0816 - val_loss: 3.2931 - val_accuracy: 0.0863\n",
      "Epoch 626/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3199 - accuracy: 0.0795\n",
      "Epoch 626: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3179 - accuracy: 0.0815 - val_loss: 3.2931 - val_accuracy: 0.0857\n",
      "Epoch 627/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3192 - accuracy: 0.0827\n",
      "Epoch 627: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3192 - accuracy: 0.0827 - val_loss: 3.2944 - val_accuracy: 0.0882\n",
      "Epoch 628/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3177 - accuracy: 0.0811\n",
      "Epoch 628: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3173 - accuracy: 0.0804 - val_loss: 3.2944 - val_accuracy: 0.0882\n",
      "Epoch 629/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3152 - accuracy: 0.0815\n",
      "Epoch 629: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3173 - accuracy: 0.0804 - val_loss: 3.2933 - val_accuracy: 0.0814\n",
      "Epoch 630/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3181 - accuracy: 0.0800\n",
      "Epoch 630: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3178 - accuracy: 0.0802 - val_loss: 3.2950 - val_accuracy: 0.0839\n",
      "Epoch 631/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0808\n",
      "Epoch 631: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3182 - accuracy: 0.0804 - val_loss: 3.2941 - val_accuracy: 0.0888\n",
      "Epoch 632/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3187 - accuracy: 0.0809\n",
      "Epoch 632: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0810 - val_loss: 3.2943 - val_accuracy: 0.0833\n",
      "Epoch 633/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3184 - accuracy: 0.0814\n",
      "Epoch 633: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3184 - accuracy: 0.0812 - val_loss: 3.2957 - val_accuracy: 0.0845\n",
      "Epoch 634/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3179 - accuracy: 0.0816\n",
      "Epoch 634: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3179 - accuracy: 0.0816 - val_loss: 3.2934 - val_accuracy: 0.0857\n",
      "Epoch 635/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3182 - accuracy: 0.0825\n",
      "Epoch 635: loss did not improve from 3.31721\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3175 - accuracy: 0.0827 - val_loss: 3.2933 - val_accuracy: 0.0863\n",
      "Epoch 636/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3171 - accuracy: 0.0821\n",
      "Epoch 636: loss improved from 3.31721 to 3.31714, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3171 - accuracy: 0.0824 - val_loss: 3.2942 - val_accuracy: 0.0802\n",
      "Epoch 637/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0821\n",
      "Epoch 637: loss did not improve from 3.31714\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3175 - accuracy: 0.0822 - val_loss: 3.2931 - val_accuracy: 0.0857\n",
      "Epoch 638/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3183 - accuracy: 0.0809\n",
      "Epoch 638: loss did not improve from 3.31714\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3172 - accuracy: 0.0809 - val_loss: 3.2936 - val_accuracy: 0.0821\n",
      "Epoch 639/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3182 - accuracy: 0.0796\n",
      "Epoch 639: loss did not improve from 3.31714\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3180 - accuracy: 0.0794 - val_loss: 3.2937 - val_accuracy: 0.0821\n",
      "Epoch 640/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3152 - accuracy: 0.0817\n",
      "Epoch 640: loss improved from 3.31714 to 3.31692, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3169 - accuracy: 0.0815 - val_loss: 3.2939 - val_accuracy: 0.0857\n",
      "Epoch 641/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3174 - accuracy: 0.0817\n",
      "Epoch 641: loss did not improve from 3.31692\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3174 - accuracy: 0.0807 - val_loss: 3.2938 - val_accuracy: 0.0839\n",
      "Epoch 642/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3154 - accuracy: 0.0823\n",
      "Epoch 642: loss improved from 3.31692 to 3.31669, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0818 - val_loss: 3.2923 - val_accuracy: 0.0857\n",
      "Epoch 643/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3171 - accuracy: 0.0813\n",
      "Epoch 643: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3206 - accuracy: 0.0820 - val_loss: 3.2987 - val_accuracy: 0.0839\n",
      "Epoch 644/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3196 - accuracy: 0.0814\n",
      "Epoch 644: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3195 - accuracy: 0.0813 - val_loss: 3.2946 - val_accuracy: 0.0876\n",
      "Epoch 645/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3183 - accuracy: 0.0809\n",
      "Epoch 645: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3177 - accuracy: 0.0810 - val_loss: 3.2930 - val_accuracy: 0.0821\n",
      "Epoch 646/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0832\n",
      "Epoch 646: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3173 - accuracy: 0.0822 - val_loss: 3.2928 - val_accuracy: 0.0784\n",
      "Epoch 647/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3167 - accuracy: 0.0824\n",
      "Epoch 647: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3178 - accuracy: 0.0816 - val_loss: 3.2947 - val_accuracy: 0.0808\n",
      "Epoch 648/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3156 - accuracy: 0.0820\n",
      "Epoch 648: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3173 - accuracy: 0.0817 - val_loss: 3.2930 - val_accuracy: 0.0851\n",
      "Epoch 649/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3198 - accuracy: 0.0799\n",
      "Epoch 649: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3177 - accuracy: 0.0812 - val_loss: 3.2944 - val_accuracy: 0.0870\n",
      "Epoch 650/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0823\n",
      "Epoch 650: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0820 - val_loss: 3.2939 - val_accuracy: 0.0833\n",
      "Epoch 651/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3192 - accuracy: 0.0811\n",
      "Epoch 651: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3176 - accuracy: 0.0813 - val_loss: 3.2933 - val_accuracy: 0.0870\n",
      "Epoch 652/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3172 - accuracy: 0.0816\n",
      "Epoch 652: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0813 - val_loss: 3.2939 - val_accuracy: 0.0827\n",
      "Epoch 653/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3179 - accuracy: 0.0813\n",
      "Epoch 653: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3170 - accuracy: 0.0817 - val_loss: 3.2928 - val_accuracy: 0.0894\n",
      "Epoch 654/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3178 - accuracy: 0.0815\n",
      "Epoch 654: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0816 - val_loss: 3.2935 - val_accuracy: 0.0839\n",
      "Epoch 655/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3182 - accuracy: 0.0825\n",
      "Epoch 655: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3169 - accuracy: 0.0824 - val_loss: 3.2933 - val_accuracy: 0.0851\n",
      "Epoch 656/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3168 - accuracy: 0.0831\n",
      "Epoch 656: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3170 - accuracy: 0.0826 - val_loss: 3.2930 - val_accuracy: 0.0839\n",
      "Epoch 657/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3194 - accuracy: 0.0812\n",
      "Epoch 657: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3180 - accuracy: 0.0813 - val_loss: 3.2938 - val_accuracy: 0.0845\n",
      "Epoch 658/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3177 - accuracy: 0.0809\n",
      "Epoch 658: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3177 - accuracy: 0.0809 - val_loss: 3.2936 - val_accuracy: 0.0814\n",
      "Epoch 659/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3175 - accuracy: 0.0823\n",
      "Epoch 659: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3173 - accuracy: 0.0811 - val_loss: 3.2990 - val_accuracy: 0.0894\n",
      "Epoch 660/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3174 - accuracy: 0.0816\n",
      "Epoch 660: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3177 - accuracy: 0.0816 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 661/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3186 - accuracy: 0.0827\n",
      "Epoch 661: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3196 - accuracy: 0.0835 - val_loss: 3.2957 - val_accuracy: 0.0814\n",
      "Epoch 662/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0818\n",
      "Epoch 662: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0821 - val_loss: 3.2928 - val_accuracy: 0.0833\n",
      "Epoch 663/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3192 - accuracy: 0.0814\n",
      "Epoch 663: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3173 - accuracy: 0.0810 - val_loss: 3.2931 - val_accuracy: 0.0882\n",
      "Epoch 664/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3179 - accuracy: 0.0840\n",
      "Epoch 664: loss did not improve from 3.31669\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3184 - accuracy: 0.0835 - val_loss: 3.2932 - val_accuracy: 0.0870\n",
      "Epoch 665/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3212 - accuracy: 0.0805\n",
      "Epoch 665: loss improved from 3.31669 to 3.31652, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3165 - accuracy: 0.0811 - val_loss: 3.2922 - val_accuracy: 0.0876\n",
      "Epoch 666/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3188 - accuracy: 0.0831\n",
      "Epoch 666: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3168 - accuracy: 0.0834 - val_loss: 3.2937 - val_accuracy: 0.0845\n",
      "Epoch 667/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3161 - accuracy: 0.0829\n",
      "Epoch 667: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3181 - accuracy: 0.0822 - val_loss: 3.2939 - val_accuracy: 0.0912\n",
      "Epoch 668/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3169 - accuracy: 0.0806\n",
      "Epoch 668: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3172 - accuracy: 0.0813 - val_loss: 3.2924 - val_accuracy: 0.0833\n",
      "Epoch 669/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3207 - accuracy: 0.0814\n",
      "Epoch 669: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3194 - accuracy: 0.0816 - val_loss: 3.2940 - val_accuracy: 0.0863\n",
      "Epoch 670/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3173 - accuracy: 0.0810\n",
      "Epoch 670: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3173 - accuracy: 0.0810 - val_loss: 3.2933 - val_accuracy: 0.0833\n",
      "Epoch 671/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0816\n",
      "Epoch 671: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0816 - val_loss: 3.2936 - val_accuracy: 0.0888\n",
      "Epoch 672/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3183 - accuracy: 0.0833\n",
      "Epoch 672: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3183 - accuracy: 0.0833 - val_loss: 3.2938 - val_accuracy: 0.0833\n",
      "Epoch 673/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3156 - accuracy: 0.0815\n",
      "Epoch 673: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3172 - accuracy: 0.0816 - val_loss: 3.2924 - val_accuracy: 0.0827\n",
      "Epoch 674/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3139 - accuracy: 0.0810\n",
      "Epoch 674: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3168 - accuracy: 0.0806 - val_loss: 3.2923 - val_accuracy: 0.0784\n",
      "Epoch 675/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3180 - accuracy: 0.0860\n",
      "Epoch 675: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 3.3165 - accuracy: 0.0847 - val_loss: 3.2929 - val_accuracy: 0.0845\n",
      "Epoch 676/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0789\n",
      "Epoch 676: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3171 - accuracy: 0.0790 - val_loss: 3.2925 - val_accuracy: 0.0845\n",
      "Epoch 677/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3172 - accuracy: 0.0811\n",
      "Epoch 677: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3168 - accuracy: 0.0812 - val_loss: 3.2918 - val_accuracy: 0.0821\n",
      "Epoch 678/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3143 - accuracy: 0.0800\n",
      "Epoch 678: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0805 - val_loss: 3.2918 - val_accuracy: 0.0808\n",
      "Epoch 679/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3181 - accuracy: 0.0806\n",
      "Epoch 679: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0807 - val_loss: 3.2927 - val_accuracy: 0.0827\n",
      "Epoch 680/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3175 - accuracy: 0.0783\n",
      "Epoch 680: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3172 - accuracy: 0.0799 - val_loss: 3.2937 - val_accuracy: 0.0814\n",
      "Epoch 681/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3174 - accuracy: 0.0816\n",
      "Epoch 681: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3174 - accuracy: 0.0816 - val_loss: 3.2933 - val_accuracy: 0.0833\n",
      "Epoch 682/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0830\n",
      "Epoch 682: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0831 - val_loss: 3.2927 - val_accuracy: 0.0827\n",
      "Epoch 683/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3174 - accuracy: 0.0810\n",
      "Epoch 683: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0804 - val_loss: 3.2935 - val_accuracy: 0.0839\n",
      "Epoch 684/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3187 - accuracy: 0.0801\n",
      "Epoch 684: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3175 - accuracy: 0.0800 - val_loss: 3.2935 - val_accuracy: 0.0827\n",
      "Epoch 685/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0810\n",
      "Epoch 685: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3174 - accuracy: 0.0812 - val_loss: 3.2931 - val_accuracy: 0.0845\n",
      "Epoch 686/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0833\n",
      "Epoch 686: loss did not improve from 3.31652\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0834 - val_loss: 3.2922 - val_accuracy: 0.0863\n",
      "Epoch 687/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0821\n",
      "Epoch 687: loss improved from 3.31652 to 3.31634, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3163 - accuracy: 0.0822 - val_loss: 3.2931 - val_accuracy: 0.0833\n",
      "Epoch 688/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3164 - accuracy: 0.0838\n",
      "Epoch 688: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3169 - accuracy: 0.0838 - val_loss: 3.2938 - val_accuracy: 0.0857\n",
      "Epoch 689/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3176 - accuracy: 0.0807\n",
      "Epoch 689: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3166 - accuracy: 0.0805 - val_loss: 3.2931 - val_accuracy: 0.0851\n",
      "Epoch 690/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3172 - accuracy: 0.0818\n",
      "Epoch 690: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3171 - accuracy: 0.0820 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 691/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0816\n",
      "Epoch 691: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0812 - val_loss: 3.2936 - val_accuracy: 0.0876\n",
      "Epoch 692/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3170 - accuracy: 0.0815\n",
      "Epoch 692: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0815 - val_loss: 3.2941 - val_accuracy: 0.0857\n",
      "Epoch 693/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3170 - accuracy: 0.0815\n",
      "Epoch 693: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3170 - accuracy: 0.0815 - val_loss: 3.2926 - val_accuracy: 0.0882\n",
      "Epoch 694/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3144 - accuracy: 0.0795\n",
      "Epoch 694: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3170 - accuracy: 0.0794 - val_loss: 3.2931 - val_accuracy: 0.0845\n",
      "Epoch 695/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0822\n",
      "Epoch 695: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3173 - accuracy: 0.0818 - val_loss: 3.2943 - val_accuracy: 0.0863\n",
      "Epoch 696/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0828\n",
      "Epoch 696: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0824 - val_loss: 3.2934 - val_accuracy: 0.0863\n",
      "Epoch 697/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3197 - accuracy: 0.0823\n",
      "Epoch 697: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0823 - val_loss: 3.2959 - val_accuracy: 0.0827\n",
      "Epoch 698/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3184 - accuracy: 0.0806\n",
      "Epoch 698: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3174 - accuracy: 0.0811 - val_loss: 3.2939 - val_accuracy: 0.0876\n",
      "Epoch 699/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3176 - accuracy: 0.0817\n",
      "Epoch 699: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3165 - accuracy: 0.0816 - val_loss: 3.2924 - val_accuracy: 0.0839\n",
      "Epoch 700/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0808\n",
      "Epoch 700: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3169 - accuracy: 0.0807 - val_loss: 3.2923 - val_accuracy: 0.0845\n",
      "Epoch 701/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3187 - accuracy: 0.0818\n",
      "Epoch 701: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3174 - accuracy: 0.0816 - val_loss: 3.2927 - val_accuracy: 0.0851\n",
      "Epoch 702/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3184 - accuracy: 0.0804\n",
      "Epoch 702: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3178 - accuracy: 0.0801 - val_loss: 3.2973 - val_accuracy: 0.0851\n",
      "Epoch 703/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3155 - accuracy: 0.0832\n",
      "Epoch 703: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3173 - accuracy: 0.0834 - val_loss: 3.2943 - val_accuracy: 0.0814\n",
      "Epoch 704/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0816\n",
      "Epoch 704: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3165 - accuracy: 0.0815 - val_loss: 3.2930 - val_accuracy: 0.0900\n",
      "Epoch 705/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0815\n",
      "Epoch 705: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3177 - accuracy: 0.0811 - val_loss: 3.2945 - val_accuracy: 0.0827\n",
      "Epoch 706/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3194 - accuracy: 0.0781\n",
      "Epoch 706: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3166 - accuracy: 0.0778 - val_loss: 3.2922 - val_accuracy: 0.0796\n",
      "Epoch 707/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3184 - accuracy: 0.0812\n",
      "Epoch 707: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0812 - val_loss: 3.2934 - val_accuracy: 0.0827\n",
      "Epoch 708/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3161 - accuracy: 0.0817\n",
      "Epoch 708: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3164 - accuracy: 0.0821 - val_loss: 3.2920 - val_accuracy: 0.0821\n",
      "Epoch 709/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3127 - accuracy: 0.0812\n",
      "Epoch 709: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3166 - accuracy: 0.0810 - val_loss: 3.2929 - val_accuracy: 0.0863\n",
      "Epoch 710/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3188 - accuracy: 0.0832\n",
      "Epoch 710: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3188 - accuracy: 0.0832 - val_loss: 3.2936 - val_accuracy: 0.0821\n",
      "Epoch 711/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0815\n",
      "Epoch 711: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0813 - val_loss: 3.2923 - val_accuracy: 0.0870\n",
      "Epoch 712/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0810\n",
      "Epoch 712: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0809 - val_loss: 3.2923 - val_accuracy: 0.0827\n",
      "Epoch 713/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3177 - accuracy: 0.0808\n",
      "Epoch 713: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3176 - accuracy: 0.0809 - val_loss: 3.2933 - val_accuracy: 0.0851\n",
      "Epoch 714/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3166 - accuracy: 0.0800\n",
      "Epoch 714: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3176 - accuracy: 0.0798 - val_loss: 3.2945 - val_accuracy: 0.0827\n",
      "Epoch 715/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0812\n",
      "Epoch 715: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0815 - val_loss: 3.2930 - val_accuracy: 0.0870\n",
      "Epoch 716/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3135 - accuracy: 0.0816\n",
      "Epoch 716: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0810 - val_loss: 3.2928 - val_accuracy: 0.0827\n",
      "Epoch 717/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0814\n",
      "Epoch 717: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3169 - accuracy: 0.0815 - val_loss: 3.2942 - val_accuracy: 0.0857\n",
      "Epoch 718/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3165 - accuracy: 0.0831\n",
      "Epoch 718: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3170 - accuracy: 0.0827 - val_loss: 3.2934 - val_accuracy: 0.0876\n",
      "Epoch 719/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3178 - accuracy: 0.0804\n",
      "Epoch 719: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3169 - accuracy: 0.0799 - val_loss: 3.2932 - val_accuracy: 0.0857\n",
      "Epoch 720/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3166 - accuracy: 0.0839\n",
      "Epoch 720: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3168 - accuracy: 0.0839 - val_loss: 3.2926 - val_accuracy: 0.0876\n",
      "Epoch 721/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3115 - accuracy: 0.0838\n",
      "Epoch 721: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3167 - accuracy: 0.0827 - val_loss: 3.2936 - val_accuracy: 0.0821\n",
      "Epoch 722/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3160 - accuracy: 0.0802\n",
      "Epoch 722: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0809 - val_loss: 3.2925 - val_accuracy: 0.0827\n",
      "Epoch 723/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3231 - accuracy: 0.0809\n",
      "Epoch 723: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3200 - accuracy: 0.0813 - val_loss: 3.2952 - val_accuracy: 0.0839\n",
      "Epoch 724/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3181 - accuracy: 0.0828\n",
      "Epoch 724: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3180 - accuracy: 0.0828 - val_loss: 3.2931 - val_accuracy: 0.0863\n",
      "Epoch 725/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3127 - accuracy: 0.0825\n",
      "Epoch 725: loss did not improve from 3.31634\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3170 - accuracy: 0.0822 - val_loss: 3.2929 - val_accuracy: 0.0870\n",
      "Epoch 726/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0828\n",
      "Epoch 726: loss improved from 3.31634 to 3.31630, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3163 - accuracy: 0.0832 - val_loss: 3.2924 - val_accuracy: 0.0821\n",
      "Epoch 727/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3162 - accuracy: 0.0800\n",
      "Epoch 727: loss improved from 3.31630 to 3.31622, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3162 - accuracy: 0.0800 - val_loss: 3.2924 - val_accuracy: 0.0857\n",
      "Epoch 728/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3144 - accuracy: 0.0817\n",
      "Epoch 728: loss improved from 3.31622 to 3.31606, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3161 - accuracy: 0.0809 - val_loss: 3.2927 - val_accuracy: 0.0802\n",
      "Epoch 729/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0799\n",
      "Epoch 729: loss did not improve from 3.31606\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3161 - accuracy: 0.0789 - val_loss: 3.2931 - val_accuracy: 0.0894\n",
      "Epoch 730/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0827\n",
      "Epoch 730: loss did not improve from 3.31606\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3161 - accuracy: 0.0827 - val_loss: 3.2923 - val_accuracy: 0.0845\n",
      "Epoch 731/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3190 - accuracy: 0.0798\n",
      "Epoch 731: loss did not improve from 3.31606\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3181 - accuracy: 0.0804 - val_loss: 3.2942 - val_accuracy: 0.0790\n",
      "Epoch 732/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3169 - accuracy: 0.0819\n",
      "Epoch 732: loss did not improve from 3.31606\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3166 - accuracy: 0.0806 - val_loss: 3.2939 - val_accuracy: 0.0851\n",
      "Epoch 733/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0806\n",
      "Epoch 733: loss did not improve from 3.31606\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3167 - accuracy: 0.0807 - val_loss: 3.2924 - val_accuracy: 0.0833\n",
      "Epoch 734/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0825\n",
      "Epoch 734: loss improved from 3.31606 to 3.31597, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3160 - accuracy: 0.0824 - val_loss: 3.2921 - val_accuracy: 0.0863\n",
      "Epoch 735/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3178 - accuracy: 0.0813\n",
      "Epoch 735: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0813 - val_loss: 3.2933 - val_accuracy: 0.0839\n",
      "Epoch 736/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3165 - accuracy: 0.0804\n",
      "Epoch 736: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0806 - val_loss: 3.2924 - val_accuracy: 0.0845\n",
      "Epoch 737/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3172 - accuracy: 0.0817\n",
      "Epoch 737: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3165 - accuracy: 0.0817 - val_loss: 3.2921 - val_accuracy: 0.0857\n",
      "Epoch 738/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3166 - accuracy: 0.0801\n",
      "Epoch 738: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3169 - accuracy: 0.0801 - val_loss: 3.2916 - val_accuracy: 0.0888\n",
      "Epoch 739/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0821\n",
      "Epoch 739: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3161 - accuracy: 0.0826 - val_loss: 3.2913 - val_accuracy: 0.0827\n",
      "Epoch 740/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3150 - accuracy: 0.0802\n",
      "Epoch 740: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3173 - accuracy: 0.0793 - val_loss: 3.2930 - val_accuracy: 0.0845\n",
      "Epoch 741/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0815\n",
      "Epoch 741: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3164 - accuracy: 0.0816 - val_loss: 3.2923 - val_accuracy: 0.0870\n",
      "Epoch 742/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3149 - accuracy: 0.0811\n",
      "Epoch 742: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3161 - accuracy: 0.0811 - val_loss: 3.2935 - val_accuracy: 0.0851\n",
      "Epoch 743/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3191 - accuracy: 0.0819\n",
      "Epoch 743: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0820 - val_loss: 3.2922 - val_accuracy: 0.0839\n",
      "Epoch 744/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3176 - accuracy: 0.0807\n",
      "Epoch 744: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3174 - accuracy: 0.0804 - val_loss: 3.2935 - val_accuracy: 0.0876\n",
      "Epoch 745/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3146 - accuracy: 0.0822\n",
      "Epoch 745: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3163 - accuracy: 0.0815 - val_loss: 3.2928 - val_accuracy: 0.0845\n",
      "Epoch 746/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0813\n",
      "Epoch 746: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3164 - accuracy: 0.0812 - val_loss: 3.2926 - val_accuracy: 0.0870\n",
      "Epoch 747/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3174 - accuracy: 0.0811\n",
      "Epoch 747: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3165 - accuracy: 0.0812 - val_loss: 3.2934 - val_accuracy: 0.0827\n",
      "Epoch 748/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3191 - accuracy: 0.0780\n",
      "Epoch 748: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3166 - accuracy: 0.0800 - val_loss: 3.2949 - val_accuracy: 0.0870\n",
      "Epoch 749/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3166 - accuracy: 0.0815\n",
      "Epoch 749: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0816 - val_loss: 3.2941 - val_accuracy: 0.0857\n",
      "Epoch 750/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3165 - accuracy: 0.0820\n",
      "Epoch 750: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0820 - val_loss: 3.2939 - val_accuracy: 0.0863\n",
      "Epoch 751/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3191 - accuracy: 0.0803\n",
      "Epoch 751: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3180 - accuracy: 0.0809 - val_loss: 3.2930 - val_accuracy: 0.0894\n",
      "Epoch 752/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0823\n",
      "Epoch 752: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3162 - accuracy: 0.0824 - val_loss: 3.2919 - val_accuracy: 0.0839\n",
      "Epoch 753/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0828\n",
      "Epoch 753: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0824 - val_loss: 3.2924 - val_accuracy: 0.0833\n",
      "Epoch 754/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3158 - accuracy: 0.0823\n",
      "Epoch 754: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3170 - accuracy: 0.0818 - val_loss: 3.2945 - val_accuracy: 0.0870\n",
      "Epoch 755/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3193 - accuracy: 0.0816\n",
      "Epoch 755: loss did not improve from 3.31597\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3164 - accuracy: 0.0815 - val_loss: 3.2926 - val_accuracy: 0.0863\n",
      "Epoch 756/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0827\n",
      "Epoch 756: loss improved from 3.31597 to 3.31592, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3159 - accuracy: 0.0828 - val_loss: 3.2922 - val_accuracy: 0.0821\n",
      "Epoch 757/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3165 - accuracy: 0.0833\n",
      "Epoch 757: loss did not improve from 3.31592\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3165 - accuracy: 0.0833 - val_loss: 3.2923 - val_accuracy: 0.0808\n",
      "Epoch 758/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3171 - accuracy: 0.0800\n",
      "Epoch 758: loss did not improve from 3.31592\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3162 - accuracy: 0.0793 - val_loss: 3.2934 - val_accuracy: 0.0863\n",
      "Epoch 759/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3141 - accuracy: 0.0831\n",
      "Epoch 759: loss improved from 3.31592 to 3.31577, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3158 - accuracy: 0.0828 - val_loss: 3.2931 - val_accuracy: 0.0808\n",
      "Epoch 760/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3206 - accuracy: 0.0812\n",
      "Epoch 760: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3194 - accuracy: 0.0813 - val_loss: 3.2940 - val_accuracy: 0.0845\n",
      "Epoch 761/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0806\n",
      "Epoch 761: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3166 - accuracy: 0.0806 - val_loss: 3.2928 - val_accuracy: 0.0857\n",
      "Epoch 762/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3170 - accuracy: 0.0823\n",
      "Epoch 762: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3170 - accuracy: 0.0823 - val_loss: 3.2938 - val_accuracy: 0.0845\n",
      "Epoch 763/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0818\n",
      "Epoch 763: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3166 - accuracy: 0.0818 - val_loss: 3.2924 - val_accuracy: 0.0839\n",
      "Epoch 764/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3165 - accuracy: 0.0811\n",
      "Epoch 764: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3165 - accuracy: 0.0811 - val_loss: 3.2927 - val_accuracy: 0.0870\n",
      "Epoch 765/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3164 - accuracy: 0.0831\n",
      "Epoch 765: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0829 - val_loss: 3.2930 - val_accuracy: 0.0888\n",
      "Epoch 766/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0834\n",
      "Epoch 766: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0839 - val_loss: 3.2938 - val_accuracy: 0.0863\n",
      "Epoch 767/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3158 - accuracy: 0.0827\n",
      "Epoch 767: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3165 - accuracy: 0.0820 - val_loss: 3.2933 - val_accuracy: 0.0882\n",
      "Epoch 768/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0829\n",
      "Epoch 768: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0827 - val_loss: 3.2920 - val_accuracy: 0.0827\n",
      "Epoch 769/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0843\n",
      "Epoch 769: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3174 - accuracy: 0.0840 - val_loss: 3.2929 - val_accuracy: 0.0857\n",
      "Epoch 770/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0813\n",
      "Epoch 770: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3172 - accuracy: 0.0810 - val_loss: 3.2938 - val_accuracy: 0.0882\n",
      "Epoch 771/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3170 - accuracy: 0.0809\n",
      "Epoch 771: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3164 - accuracy: 0.0812 - val_loss: 3.2926 - val_accuracy: 0.0888\n",
      "Epoch 772/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3145 - accuracy: 0.0849\n",
      "Epoch 772: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3161 - accuracy: 0.0834 - val_loss: 3.2926 - val_accuracy: 0.0876\n",
      "Epoch 773/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3144 - accuracy: 0.0827\n",
      "Epoch 773: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3160 - accuracy: 0.0832 - val_loss: 3.2922 - val_accuracy: 0.0814\n",
      "Epoch 774/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0839\n",
      "Epoch 774: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3161 - accuracy: 0.0837 - val_loss: 3.2917 - val_accuracy: 0.0857\n",
      "Epoch 775/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3164 - accuracy: 0.0816\n",
      "Epoch 775: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3170 - accuracy: 0.0815 - val_loss: 3.2920 - val_accuracy: 0.0870\n",
      "Epoch 776/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0824\n",
      "Epoch 776: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3161 - accuracy: 0.0824 - val_loss: 3.2933 - val_accuracy: 0.0863\n",
      "Epoch 777/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3144 - accuracy: 0.0819\n",
      "Epoch 777: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0806 - val_loss: 3.2920 - val_accuracy: 0.0906\n",
      "Epoch 778/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0840\n",
      "Epoch 778: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3161 - accuracy: 0.0848 - val_loss: 3.2931 - val_accuracy: 0.0845\n",
      "Epoch 779/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0811\n",
      "Epoch 779: loss did not improve from 3.31577\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3159 - accuracy: 0.0828 - val_loss: 3.2930 - val_accuracy: 0.0912\n",
      "Epoch 780/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0833\n",
      "Epoch 780: loss improved from 3.31577 to 3.31553, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3155 - accuracy: 0.0834 - val_loss: 3.2924 - val_accuracy: 0.0833\n",
      "Epoch 781/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3165 - accuracy: 0.0820\n",
      "Epoch 781: loss did not improve from 3.31553\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3162 - accuracy: 0.0822 - val_loss: 3.2939 - val_accuracy: 0.0845\n",
      "Epoch 782/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0811\n",
      "Epoch 782: loss did not improve from 3.31553\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3157 - accuracy: 0.0812 - val_loss: 3.2921 - val_accuracy: 0.0790\n",
      "Epoch 783/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3179 - accuracy: 0.0802\n",
      "Epoch 783: loss did not improve from 3.31553\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3178 - accuracy: 0.0805 - val_loss: 3.2931 - val_accuracy: 0.0827\n",
      "Epoch 784/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0809\n",
      "Epoch 784: loss did not improve from 3.31553\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3156 - accuracy: 0.0809 - val_loss: 3.2918 - val_accuracy: 0.0857\n",
      "Epoch 785/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3135 - accuracy: 0.0809\n",
      "Epoch 785: loss improved from 3.31553 to 3.31546, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3155 - accuracy: 0.0806 - val_loss: 3.2922 - val_accuracy: 0.0796\n",
      "Epoch 786/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0828\n",
      "Epoch 786: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3164 - accuracy: 0.0824 - val_loss: 3.2924 - val_accuracy: 0.0814\n",
      "Epoch 787/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3185 - accuracy: 0.0812\n",
      "Epoch 787: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3156 - accuracy: 0.0817 - val_loss: 3.2920 - val_accuracy: 0.0833\n",
      "Epoch 788/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3166 - accuracy: 0.0826\n",
      "Epoch 788: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 33ms/step - loss: 3.3166 - accuracy: 0.0826 - val_loss: 3.2947 - val_accuracy: 0.0839\n",
      "Epoch 789/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0821\n",
      "Epoch 789: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3163 - accuracy: 0.0821 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 790/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3172 - accuracy: 0.0818\n",
      "Epoch 790: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3167 - accuracy: 0.0820 - val_loss: 3.2921 - val_accuracy: 0.0857\n",
      "Epoch 791/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0839\n",
      "Epoch 791: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3159 - accuracy: 0.0839 - val_loss: 3.2917 - val_accuracy: 0.0833\n",
      "Epoch 792/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3183 - accuracy: 0.0795\n",
      "Epoch 792: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3160 - accuracy: 0.0796 - val_loss: 3.2920 - val_accuracy: 0.0857\n",
      "Epoch 793/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0795\n",
      "Epoch 793: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 10s 38ms/step - loss: 3.3163 - accuracy: 0.0796 - val_loss: 3.2923 - val_accuracy: 0.0827\n",
      "Epoch 794/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0814\n",
      "Epoch 794: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3162 - accuracy: 0.0813 - val_loss: 3.2950 - val_accuracy: 0.0833\n",
      "Epoch 795/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3168 - accuracy: 0.0813\n",
      "Epoch 795: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3168 - accuracy: 0.0813 - val_loss: 3.2920 - val_accuracy: 0.0857\n",
      "Epoch 796/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0820\n",
      "Epoch 796: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 37ms/step - loss: 3.3163 - accuracy: 0.0820 - val_loss: 3.2927 - val_accuracy: 0.0833\n",
      "Epoch 797/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0809\n",
      "Epoch 797: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3158 - accuracy: 0.0810 - val_loss: 3.2913 - val_accuracy: 0.0851\n",
      "Epoch 798/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0833\n",
      "Epoch 798: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3160 - accuracy: 0.0834 - val_loss: 3.2921 - val_accuracy: 0.0845\n",
      "Epoch 799/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0800\n",
      "Epoch 799: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3158 - accuracy: 0.0798 - val_loss: 3.2918 - val_accuracy: 0.0851\n",
      "Epoch 800/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3196 - accuracy: 0.0811\n",
      "Epoch 800: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3195 - accuracy: 0.0811 - val_loss: 3.2927 - val_accuracy: 0.0839\n",
      "Epoch 801/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3167 - accuracy: 0.0809\n",
      "Epoch 801: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 10s 38ms/step - loss: 3.3166 - accuracy: 0.0812 - val_loss: 3.2930 - val_accuracy: 0.0845\n",
      "Epoch 802/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0814\n",
      "Epoch 802: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3161 - accuracy: 0.0815 - val_loss: 3.2927 - val_accuracy: 0.0863\n",
      "Epoch 803/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3170 - accuracy: 0.0805\n",
      "Epoch 803: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 33ms/step - loss: 3.3170 - accuracy: 0.0805 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 804/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3176 - accuracy: 0.0820\n",
      "Epoch 804: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3178 - accuracy: 0.0820 - val_loss: 3.2923 - val_accuracy: 0.0833\n",
      "Epoch 805/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0824\n",
      "Epoch 805: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 37ms/step - loss: 3.3161 - accuracy: 0.0826 - val_loss: 3.2923 - val_accuracy: 0.0851\n",
      "Epoch 806/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3163 - accuracy: 0.0810\n",
      "Epoch 806: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3163 - accuracy: 0.0810 - val_loss: 3.2925 - val_accuracy: 0.0845\n",
      "Epoch 807/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0831\n",
      "Epoch 807: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3157 - accuracy: 0.0831 - val_loss: 3.2911 - val_accuracy: 0.0857\n",
      "Epoch 808/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3173 - accuracy: 0.0832\n",
      "Epoch 808: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3173 - accuracy: 0.0832 - val_loss: 3.2919 - val_accuracy: 0.0839\n",
      "Epoch 809/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0821\n",
      "Epoch 809: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3162 - accuracy: 0.0823 - val_loss: 3.2934 - val_accuracy: 0.0863\n",
      "Epoch 810/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3180 - accuracy: 0.0814\n",
      "Epoch 810: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3169 - accuracy: 0.0820 - val_loss: 3.2923 - val_accuracy: 0.0827\n",
      "Epoch 811/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3182 - accuracy: 0.0814\n",
      "Epoch 811: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3165 - accuracy: 0.0813 - val_loss: 3.2924 - val_accuracy: 0.0882\n",
      "Epoch 812/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0818\n",
      "Epoch 812: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3159 - accuracy: 0.0818 - val_loss: 3.2921 - val_accuracy: 0.0863\n",
      "Epoch 813/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0821\n",
      "Epoch 813: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3160 - accuracy: 0.0818 - val_loss: 3.2918 - val_accuracy: 0.0851\n",
      "Epoch 814/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0802\n",
      "Epoch 814: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3160 - accuracy: 0.0801 - val_loss: 3.2929 - val_accuracy: 0.0827\n",
      "Epoch 815/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3152 - accuracy: 0.0801\n",
      "Epoch 815: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3171 - accuracy: 0.0805 - val_loss: 3.2934 - val_accuracy: 0.0870\n",
      "Epoch 816/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3159 - accuracy: 0.0835\n",
      "Epoch 816: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3159 - accuracy: 0.0835 - val_loss: 3.2925 - val_accuracy: 0.0863\n",
      "Epoch 817/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3138 - accuracy: 0.0807\n",
      "Epoch 817: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3167 - accuracy: 0.0813 - val_loss: 3.2923 - val_accuracy: 0.0814\n",
      "Epoch 818/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3157 - accuracy: 0.0825\n",
      "Epoch 818: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 33ms/step - loss: 3.3182 - accuracy: 0.0823 - val_loss: 3.2980 - val_accuracy: 0.0882\n",
      "Epoch 819/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0819\n",
      "Epoch 819: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 33ms/step - loss: 3.3171 - accuracy: 0.0818 - val_loss: 3.2930 - val_accuracy: 0.0845\n",
      "Epoch 820/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0806\n",
      "Epoch 820: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3163 - accuracy: 0.0805 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 821/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0792\n",
      "Epoch 821: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3165 - accuracy: 0.0793 - val_loss: 3.2948 - val_accuracy: 0.0833\n",
      "Epoch 822/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0823\n",
      "Epoch 822: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3162 - accuracy: 0.0821 - val_loss: 3.2919 - val_accuracy: 0.0851\n",
      "Epoch 823/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0771\n",
      "Epoch 823: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 10s 39ms/step - loss: 3.3158 - accuracy: 0.0771 - val_loss: 3.2923 - val_accuracy: 0.0851\n",
      "Epoch 824/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0824\n",
      "Epoch 824: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3160 - accuracy: 0.0823 - val_loss: 3.2924 - val_accuracy: 0.0833\n",
      "Epoch 825/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0814\n",
      "Epoch 825: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3167 - accuracy: 0.0816 - val_loss: 3.2949 - val_accuracy: 0.0845\n",
      "Epoch 826/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0804\n",
      "Epoch 826: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3163 - accuracy: 0.0810 - val_loss: 3.2922 - val_accuracy: 0.0863\n",
      "Epoch 827/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0810\n",
      "Epoch 827: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3160 - accuracy: 0.0810 - val_loss: 3.2928 - val_accuracy: 0.0857\n",
      "Epoch 828/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3128 - accuracy: 0.0827\n",
      "Epoch 828: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3157 - accuracy: 0.0820 - val_loss: 3.2920 - val_accuracy: 0.0870\n",
      "Epoch 829/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0830\n",
      "Epoch 829: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3162 - accuracy: 0.0829 - val_loss: 3.2927 - val_accuracy: 0.0863\n",
      "Epoch 830/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0806\n",
      "Epoch 830: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3170 - accuracy: 0.0806 - val_loss: 3.2942 - val_accuracy: 0.0882\n",
      "Epoch 831/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0817\n",
      "Epoch 831: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3161 - accuracy: 0.0817 - val_loss: 3.2929 - val_accuracy: 0.0870\n",
      "Epoch 832/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0812\n",
      "Epoch 832: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3156 - accuracy: 0.0813 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 833/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3192 - accuracy: 0.0807\n",
      "Epoch 833: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3183 - accuracy: 0.0806 - val_loss: 3.2972 - val_accuracy: 0.0882\n",
      "Epoch 834/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3165 - accuracy: 0.0822\n",
      "Epoch 834: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3171 - accuracy: 0.0824 - val_loss: 3.2929 - val_accuracy: 0.0876\n",
      "Epoch 835/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0820\n",
      "Epoch 835: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3156 - accuracy: 0.0820 - val_loss: 3.2925 - val_accuracy: 0.0870\n",
      "Epoch 836/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3158 - accuracy: 0.0805\n",
      "Epoch 836: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3158 - accuracy: 0.0805 - val_loss: 3.2923 - val_accuracy: 0.0808\n",
      "Epoch 837/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3167 - accuracy: 0.0840\n",
      "Epoch 837: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3159 - accuracy: 0.0829 - val_loss: 3.2914 - val_accuracy: 0.0857\n",
      "Epoch 838/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0805\n",
      "Epoch 838: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3158 - accuracy: 0.0807 - val_loss: 3.2926 - val_accuracy: 0.0870\n",
      "Epoch 839/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3177 - accuracy: 0.0815\n",
      "Epoch 839: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3158 - accuracy: 0.0822 - val_loss: 3.2922 - val_accuracy: 0.0814\n",
      "Epoch 840/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0816\n",
      "Epoch 840: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3155 - accuracy: 0.0818 - val_loss: 3.2921 - val_accuracy: 0.0857\n",
      "Epoch 841/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0831\n",
      "Epoch 841: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3174 - accuracy: 0.0831 - val_loss: 3.2924 - val_accuracy: 0.0863\n",
      "Epoch 842/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0824\n",
      "Epoch 842: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 10s 40ms/step - loss: 3.3155 - accuracy: 0.0824 - val_loss: 3.2929 - val_accuracy: 0.0833\n",
      "Epoch 843/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0836\n",
      "Epoch 843: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3158 - accuracy: 0.0835 - val_loss: 3.2921 - val_accuracy: 0.0827\n",
      "Epoch 844/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3151 - accuracy: 0.0803\n",
      "Epoch 844: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3164 - accuracy: 0.0809 - val_loss: 3.2932 - val_accuracy: 0.0857\n",
      "Epoch 845/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0805\n",
      "Epoch 845: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3167 - accuracy: 0.0806 - val_loss: 3.2923 - val_accuracy: 0.0876\n",
      "Epoch 846/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0830\n",
      "Epoch 846: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3158 - accuracy: 0.0829 - val_loss: 3.2917 - val_accuracy: 0.0863\n",
      "Epoch 847/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3162 - accuracy: 0.0827\n",
      "Epoch 847: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3162 - accuracy: 0.0827 - val_loss: 3.2918 - val_accuracy: 0.0833\n",
      "Epoch 848/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0800\n",
      "Epoch 848: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3158 - accuracy: 0.0800 - val_loss: 3.2913 - val_accuracy: 0.0870\n",
      "Epoch 849/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0821\n",
      "Epoch 849: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3157 - accuracy: 0.0816 - val_loss: 3.2910 - val_accuracy: 0.0882\n",
      "Epoch 850/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3194 - accuracy: 0.0805\n",
      "Epoch 850: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3179 - accuracy: 0.0817 - val_loss: 3.2920 - val_accuracy: 0.0888\n",
      "Epoch 851/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3166 - accuracy: 0.0808\n",
      "Epoch 851: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3159 - accuracy: 0.0810 - val_loss: 3.2927 - val_accuracy: 0.0870\n",
      "Epoch 852/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0830\n",
      "Epoch 852: loss did not improve from 3.31546\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3157 - accuracy: 0.0829 - val_loss: 3.2925 - val_accuracy: 0.0839\n",
      "Epoch 853/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3152 - accuracy: 0.0803\n",
      "Epoch 853: loss improved from 3.31546 to 3.31542, saving model to best_model.h5\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3154 - accuracy: 0.0806 - val_loss: 3.2921 - val_accuracy: 0.0821\n",
      "Epoch 854/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3182 - accuracy: 0.0825\n",
      "Epoch 854: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3186 - accuracy: 0.0824 - val_loss: 3.2935 - val_accuracy: 0.0857\n",
      "Epoch 855/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0803\n",
      "Epoch 855: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3160 - accuracy: 0.0802 - val_loss: 3.2928 - val_accuracy: 0.0833\n",
      "Epoch 856/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0797\n",
      "Epoch 856: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3161 - accuracy: 0.0800 - val_loss: 3.2937 - val_accuracy: 0.0821\n",
      "Epoch 857/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0797\n",
      "Epoch 857: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3157 - accuracy: 0.0799 - val_loss: 3.2917 - val_accuracy: 0.0870\n",
      "Epoch 858/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0800\n",
      "Epoch 858: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3159 - accuracy: 0.0800 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 859/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3166 - accuracy: 0.0816\n",
      "Epoch 859: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3160 - accuracy: 0.0813 - val_loss: 3.2952 - val_accuracy: 0.0845\n",
      "Epoch 860/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0833\n",
      "Epoch 860: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3162 - accuracy: 0.0831 - val_loss: 3.2926 - val_accuracy: 0.0857\n",
      "Epoch 861/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0819\n",
      "Epoch 861: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3166 - accuracy: 0.0816 - val_loss: 3.2920 - val_accuracy: 0.0821\n",
      "Epoch 862/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0800\n",
      "Epoch 862: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3157 - accuracy: 0.0799 - val_loss: 3.2915 - val_accuracy: 0.0827\n",
      "Epoch 863/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0827\n",
      "Epoch 863: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3160 - accuracy: 0.0824 - val_loss: 3.2951 - val_accuracy: 0.0839\n",
      "Epoch 864/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0814\n",
      "Epoch 864: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 10s 39ms/step - loss: 3.3172 - accuracy: 0.0813 - val_loss: 3.2921 - val_accuracy: 0.0845\n",
      "Epoch 865/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0813\n",
      "Epoch 865: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3163 - accuracy: 0.0817 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 866/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3136 - accuracy: 0.0808\n",
      "Epoch 866: loss did not improve from 3.31542\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3164 - accuracy: 0.0813 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 867/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0816\n",
      "Epoch 867: loss improved from 3.31542 to 3.31533, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3153 - accuracy: 0.0816 - val_loss: 3.2910 - val_accuracy: 0.0857\n",
      "Epoch 868/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0798\n",
      "Epoch 868: loss did not improve from 3.31533\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3161 - accuracy: 0.0810 - val_loss: 3.2920 - val_accuracy: 0.0882\n",
      "Epoch 869/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3151 - accuracy: 0.0829\n",
      "Epoch 869: loss improved from 3.31533 to 3.31507, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3151 - accuracy: 0.0828 - val_loss: 3.2922 - val_accuracy: 0.0857\n",
      "Epoch 870/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0830\n",
      "Epoch 870: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3166 - accuracy: 0.0831 - val_loss: 3.2921 - val_accuracy: 0.0814\n",
      "Epoch 871/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0805\n",
      "Epoch 871: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3161 - accuracy: 0.0813 - val_loss: 3.2919 - val_accuracy: 0.0851\n",
      "Epoch 872/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3160 - accuracy: 0.0809\n",
      "Epoch 872: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3160 - accuracy: 0.0809 - val_loss: 3.3083 - val_accuracy: 0.0876\n",
      "Epoch 873/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3138 - accuracy: 0.0810\n",
      "Epoch 873: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3175 - accuracy: 0.0807 - val_loss: 3.2934 - val_accuracy: 0.0833\n",
      "Epoch 874/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3134 - accuracy: 0.0810\n",
      "Epoch 874: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3154 - accuracy: 0.0812 - val_loss: 3.2917 - val_accuracy: 0.0870\n",
      "Epoch 875/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0808\n",
      "Epoch 875: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3156 - accuracy: 0.0809 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 876/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0826\n",
      "Epoch 876: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3153 - accuracy: 0.0823 - val_loss: 3.2919 - val_accuracy: 0.0821\n",
      "Epoch 877/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3166 - accuracy: 0.0814\n",
      "Epoch 877: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3170 - accuracy: 0.0810 - val_loss: 3.2929 - val_accuracy: 0.0876\n",
      "Epoch 878/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3142 - accuracy: 0.0806\n",
      "Epoch 878: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3157 - accuracy: 0.0807 - val_loss: 3.2919 - val_accuracy: 0.0888\n",
      "Epoch 879/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0818\n",
      "Epoch 879: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3157 - accuracy: 0.0820 - val_loss: 3.2929 - val_accuracy: 0.0900\n",
      "Epoch 880/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3159 - accuracy: 0.0789\n",
      "Epoch 880: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0800 - val_loss: 3.2942 - val_accuracy: 0.0870\n",
      "Epoch 881/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3166 - accuracy: 0.0830\n",
      "Epoch 881: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3162 - accuracy: 0.0815 - val_loss: 3.2919 - val_accuracy: 0.0851\n",
      "Epoch 882/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3163 - accuracy: 0.0817\n",
      "Epoch 882: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3153 - accuracy: 0.0812 - val_loss: 3.2929 - val_accuracy: 0.0882\n",
      "Epoch 883/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0823\n",
      "Epoch 883: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3155 - accuracy: 0.0822 - val_loss: 3.2937 - val_accuracy: 0.0888\n",
      "Epoch 884/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0809\n",
      "Epoch 884: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3164 - accuracy: 0.0804 - val_loss: 3.2916 - val_accuracy: 0.0870\n",
      "Epoch 885/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0841\n",
      "Epoch 885: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3162 - accuracy: 0.0840 - val_loss: 3.2940 - val_accuracy: 0.0863\n",
      "Epoch 886/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3168 - accuracy: 0.0829\n",
      "Epoch 886: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0829 - val_loss: 3.2925 - val_accuracy: 0.0845\n",
      "Epoch 887/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0828\n",
      "Epoch 887: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3159 - accuracy: 0.0827 - val_loss: 3.2931 - val_accuracy: 0.0857\n",
      "Epoch 888/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0835\n",
      "Epoch 888: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3159 - accuracy: 0.0834 - val_loss: 3.2914 - val_accuracy: 0.0876\n",
      "Epoch 889/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3160 - accuracy: 0.0817\n",
      "Epoch 889: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0817 - val_loss: 3.2919 - val_accuracy: 0.0851\n",
      "Epoch 890/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0828\n",
      "Epoch 890: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3157 - accuracy: 0.0826 - val_loss: 3.2919 - val_accuracy: 0.0827\n",
      "Epoch 891/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3175 - accuracy: 0.0794\n",
      "Epoch 891: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3167 - accuracy: 0.0810 - val_loss: 3.2925 - val_accuracy: 0.0851\n",
      "Epoch 892/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0806\n",
      "Epoch 892: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3154 - accuracy: 0.0801 - val_loss: 3.2915 - val_accuracy: 0.0839\n",
      "Epoch 893/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0815\n",
      "Epoch 893: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3163 - accuracy: 0.0815 - val_loss: 3.2929 - val_accuracy: 0.0790\n",
      "Epoch 894/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3178 - accuracy: 0.0793\n",
      "Epoch 894: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0793 - val_loss: 3.2913 - val_accuracy: 0.0833\n",
      "Epoch 895/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3131 - accuracy: 0.0828\n",
      "Epoch 895: loss did not improve from 3.31507\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3168 - accuracy: 0.0824 - val_loss: 3.2917 - val_accuracy: 0.0857\n",
      "Epoch 896/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0812\n",
      "Epoch 896: loss improved from 3.31507 to 3.31490, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3149 - accuracy: 0.0810 - val_loss: 3.2924 - val_accuracy: 0.0814\n",
      "Epoch 897/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3165 - accuracy: 0.0805\n",
      "Epoch 897: loss did not improve from 3.31490\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3151 - accuracy: 0.0812 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 898/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3167 - accuracy: 0.0823\n",
      "Epoch 898: loss did not improve from 3.31490\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3158 - accuracy: 0.0829 - val_loss: 3.2929 - val_accuracy: 0.0894\n",
      "Epoch 899/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0815\n",
      "Epoch 899: loss did not improve from 3.31490\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3151 - accuracy: 0.0815 - val_loss: 3.2913 - val_accuracy: 0.0863\n",
      "Epoch 900/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3173 - accuracy: 0.0830\n",
      "Epoch 900: loss did not improve from 3.31490\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3154 - accuracy: 0.0827 - val_loss: 3.2919 - val_accuracy: 0.0839\n",
      "Epoch 901/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0813\n",
      "Epoch 901: loss did not improve from 3.31490\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3154 - accuracy: 0.0815 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 902/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3162 - accuracy: 0.0828\n",
      "Epoch 902: loss did not improve from 3.31490\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3169 - accuracy: 0.0823 - val_loss: 3.2928 - val_accuracy: 0.0851\n",
      "Epoch 903/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3127 - accuracy: 0.0829\n",
      "Epoch 903: loss improved from 3.31490 to 3.31488, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3149 - accuracy: 0.0826 - val_loss: 3.2909 - val_accuracy: 0.0845\n",
      "Epoch 904/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0824\n",
      "Epoch 904: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3156 - accuracy: 0.0815 - val_loss: 3.2911 - val_accuracy: 0.0845\n",
      "Epoch 905/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0797\n",
      "Epoch 905: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3154 - accuracy: 0.0798 - val_loss: 3.2925 - val_accuracy: 0.0827\n",
      "Epoch 906/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0809\n",
      "Epoch 906: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3163 - accuracy: 0.0809 - val_loss: 3.2920 - val_accuracy: 0.0851\n",
      "Epoch 907/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0818\n",
      "Epoch 907: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3150 - accuracy: 0.0821 - val_loss: 3.2910 - val_accuracy: 0.0827\n",
      "Epoch 908/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0823\n",
      "Epoch 908: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3159 - accuracy: 0.0826 - val_loss: 3.2941 - val_accuracy: 0.0814\n",
      "Epoch 909/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3166 - accuracy: 0.0819\n",
      "Epoch 909: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3164 - accuracy: 0.0813 - val_loss: 3.2934 - val_accuracy: 0.0808\n",
      "Epoch 910/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3168 - accuracy: 0.0812\n",
      "Epoch 910: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3158 - accuracy: 0.0804 - val_loss: 3.2916 - val_accuracy: 0.0863\n",
      "Epoch 911/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0822\n",
      "Epoch 911: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3164 - accuracy: 0.0816 - val_loss: 3.2924 - val_accuracy: 0.0863\n",
      "Epoch 912/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0817\n",
      "Epoch 912: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3158 - accuracy: 0.0818 - val_loss: 3.2947 - val_accuracy: 0.0845\n",
      "Epoch 913/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3160 - accuracy: 0.0840\n",
      "Epoch 913: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3162 - accuracy: 0.0834 - val_loss: 3.2928 - val_accuracy: 0.0833\n",
      "Epoch 914/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3172 - accuracy: 0.0827\n",
      "Epoch 914: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3176 - accuracy: 0.0826 - val_loss: 3.2939 - val_accuracy: 0.0870\n",
      "Epoch 915/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3148 - accuracy: 0.0814\n",
      "Epoch 915: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3161 - accuracy: 0.0816 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 916/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3160 - accuracy: 0.0820\n",
      "Epoch 916: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0820 - val_loss: 3.2926 - val_accuracy: 0.0863\n",
      "Epoch 917/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0825\n",
      "Epoch 917: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3161 - accuracy: 0.0824 - val_loss: 3.2942 - val_accuracy: 0.0882\n",
      "Epoch 918/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0811\n",
      "Epoch 918: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3158 - accuracy: 0.0820 - val_loss: 3.2925 - val_accuracy: 0.0876\n",
      "Epoch 919/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3161 - accuracy: 0.0805\n",
      "Epoch 919: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3171 - accuracy: 0.0810 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 920/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3142 - accuracy: 0.0829\n",
      "Epoch 920: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3153 - accuracy: 0.0833 - val_loss: 3.2920 - val_accuracy: 0.0851\n",
      "Epoch 921/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0823\n",
      "Epoch 921: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3157 - accuracy: 0.0822 - val_loss: 3.2933 - val_accuracy: 0.0845\n",
      "Epoch 922/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0826\n",
      "Epoch 922: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3152 - accuracy: 0.0826 - val_loss: 3.2914 - val_accuracy: 0.0882\n",
      "Epoch 923/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3169 - accuracy: 0.0822\n",
      "Epoch 923: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3156 - accuracy: 0.0821 - val_loss: 3.2931 - val_accuracy: 0.0821\n",
      "Epoch 924/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3151 - accuracy: 0.0837\n",
      "Epoch 924: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3150 - accuracy: 0.0848 - val_loss: 3.2917 - val_accuracy: 0.0827\n",
      "Epoch 925/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3160 - accuracy: 0.0827\n",
      "Epoch 925: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3160 - accuracy: 0.0824 - val_loss: 3.2926 - val_accuracy: 0.0857\n",
      "Epoch 926/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3154 - accuracy: 0.0833\n",
      "Epoch 926: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0823 - val_loss: 3.2945 - val_accuracy: 0.0845\n",
      "Epoch 927/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0820\n",
      "Epoch 927: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3162 - accuracy: 0.0820 - val_loss: 3.2940 - val_accuracy: 0.0882\n",
      "Epoch 928/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3183 - accuracy: 0.0804\n",
      "Epoch 928: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3163 - accuracy: 0.0805 - val_loss: 3.2928 - val_accuracy: 0.0857\n",
      "Epoch 929/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0808\n",
      "Epoch 929: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3155 - accuracy: 0.0807 - val_loss: 3.2920 - val_accuracy: 0.0863\n",
      "Epoch 930/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0843\n",
      "Epoch 930: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3150 - accuracy: 0.0845 - val_loss: 3.2924 - val_accuracy: 0.0857\n",
      "Epoch 931/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0820\n",
      "Epoch 931: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3150 - accuracy: 0.0828 - val_loss: 3.2911 - val_accuracy: 0.0845\n",
      "Epoch 932/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0828\n",
      "Epoch 932: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3175 - accuracy: 0.0828 - val_loss: 3.2930 - val_accuracy: 0.0845\n",
      "Epoch 933/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0817\n",
      "Epoch 933: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3160 - accuracy: 0.0817 - val_loss: 3.2988 - val_accuracy: 0.0845\n",
      "Epoch 934/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3184 - accuracy: 0.0816\n",
      "Epoch 934: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3161 - accuracy: 0.0816 - val_loss: 3.2927 - val_accuracy: 0.0821\n",
      "Epoch 935/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0808\n",
      "Epoch 935: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3159 - accuracy: 0.0807 - val_loss: 3.2951 - val_accuracy: 0.0839\n",
      "Epoch 936/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3159 - accuracy: 0.0807\n",
      "Epoch 936: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3159 - accuracy: 0.0807 - val_loss: 3.2926 - val_accuracy: 0.0827\n",
      "Epoch 937/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0815\n",
      "Epoch 937: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3154 - accuracy: 0.0815 - val_loss: 3.2918 - val_accuracy: 0.0857\n",
      "Epoch 938/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3187 - accuracy: 0.0821\n",
      "Epoch 938: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 4s 18ms/step - loss: 3.3164 - accuracy: 0.0822 - val_loss: 3.2929 - val_accuracy: 0.0814\n",
      "Epoch 939/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0803\n",
      "Epoch 939: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3170 - accuracy: 0.0800 - val_loss: 3.2972 - val_accuracy: 0.0851\n",
      "Epoch 940/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3171 - accuracy: 0.0825\n",
      "Epoch 940: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3162 - accuracy: 0.0821 - val_loss: 3.2931 - val_accuracy: 0.0863\n",
      "Epoch 941/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0822\n",
      "Epoch 941: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3149 - accuracy: 0.0822 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 942/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3174 - accuracy: 0.0816\n",
      "Epoch 942: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3156 - accuracy: 0.0821 - val_loss: 3.2925 - val_accuracy: 0.0814\n",
      "Epoch 943/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0807\n",
      "Epoch 943: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3161 - accuracy: 0.0806 - val_loss: 3.2935 - val_accuracy: 0.0870\n",
      "Epoch 944/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3156 - accuracy: 0.0811\n",
      "Epoch 944: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3156 - accuracy: 0.0811 - val_loss: 3.2933 - val_accuracy: 0.0863\n",
      "Epoch 945/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3170 - accuracy: 0.0814\n",
      "Epoch 945: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3167 - accuracy: 0.0813 - val_loss: 3.2956 - val_accuracy: 0.0857\n",
      "Epoch 946/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0809\n",
      "Epoch 946: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3161 - accuracy: 0.0809 - val_loss: 3.2938 - val_accuracy: 0.0827\n",
      "Epoch 947/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0808\n",
      "Epoch 947: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3152 - accuracy: 0.0805 - val_loss: 3.2937 - val_accuracy: 0.0839\n",
      "Epoch 948/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0822\n",
      "Epoch 948: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3156 - accuracy: 0.0822 - val_loss: 3.2949 - val_accuracy: 0.0839\n",
      "Epoch 949/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0815\n",
      "Epoch 949: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3156 - accuracy: 0.0817 - val_loss: 3.2933 - val_accuracy: 0.0839\n",
      "Epoch 950/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0805\n",
      "Epoch 950: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3150 - accuracy: 0.0811 - val_loss: 3.2918 - val_accuracy: 0.0845\n",
      "Epoch 951/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0809\n",
      "Epoch 951: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3158 - accuracy: 0.0818 - val_loss: 3.2920 - val_accuracy: 0.0839\n",
      "Epoch 952/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0824\n",
      "Epoch 952: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3155 - accuracy: 0.0823 - val_loss: 3.2924 - val_accuracy: 0.0839\n",
      "Epoch 953/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0804\n",
      "Epoch 953: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3154 - accuracy: 0.0801 - val_loss: 3.2929 - val_accuracy: 0.0845\n",
      "Epoch 954/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0832\n",
      "Epoch 954: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3155 - accuracy: 0.0832 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 955/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0829\n",
      "Epoch 955: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3163 - accuracy: 0.0824 - val_loss: 3.2922 - val_accuracy: 0.0839\n",
      "Epoch 956/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3154 - accuracy: 0.0820\n",
      "Epoch 956: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3154 - accuracy: 0.0820 - val_loss: 3.2924 - val_accuracy: 0.0857\n",
      "Epoch 957/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3152 - accuracy: 0.0823\n",
      "Epoch 957: loss did not improve from 3.31488\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3152 - accuracy: 0.0823 - val_loss: 3.2921 - val_accuracy: 0.0857\n",
      "Epoch 958/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0822\n",
      "Epoch 958: loss improved from 3.31488 to 3.31471, saving model to best_model.h5\n",
      "256/256 [==============================] - 6s 21ms/step - loss: 3.3147 - accuracy: 0.0822 - val_loss: 3.2941 - val_accuracy: 0.0845\n",
      "Epoch 959/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0837\n",
      "Epoch 959: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3152 - accuracy: 0.0837 - val_loss: 3.2919 - val_accuracy: 0.0827\n",
      "Epoch 960/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3171 - accuracy: 0.0814\n",
      "Epoch 960: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3166 - accuracy: 0.0820 - val_loss: 3.2936 - val_accuracy: 0.0845\n",
      "Epoch 961/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3138 - accuracy: 0.0834\n",
      "Epoch 961: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3155 - accuracy: 0.0824 - val_loss: 3.2926 - val_accuracy: 0.0863\n",
      "Epoch 962/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0816\n",
      "Epoch 962: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 10s 38ms/step - loss: 3.3153 - accuracy: 0.0816 - val_loss: 3.2919 - val_accuracy: 0.0814\n",
      "Epoch 963/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0810\n",
      "Epoch 963: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3157 - accuracy: 0.0810 - val_loss: 3.2924 - val_accuracy: 0.0827\n",
      "Epoch 964/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0813\n",
      "Epoch 964: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 10s 39ms/step - loss: 3.3168 - accuracy: 0.0813 - val_loss: 3.2938 - val_accuracy: 0.0839\n",
      "Epoch 965/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0811\n",
      "Epoch 965: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3156 - accuracy: 0.0804 - val_loss: 3.2918 - val_accuracy: 0.0814\n",
      "Epoch 966/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3165 - accuracy: 0.0820\n",
      "Epoch 966: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3147 - accuracy: 0.0826 - val_loss: 3.2913 - val_accuracy: 0.0827\n",
      "Epoch 967/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0814\n",
      "Epoch 967: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 11s 41ms/step - loss: 3.3152 - accuracy: 0.0813 - val_loss: 3.2911 - val_accuracy: 0.0845\n",
      "Epoch 968/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0813\n",
      "Epoch 968: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 8s 29ms/step - loss: 3.3156 - accuracy: 0.0811 - val_loss: 3.2917 - val_accuracy: 0.0845\n",
      "Epoch 969/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0821\n",
      "Epoch 969: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3156 - accuracy: 0.0821 - val_loss: 3.2933 - val_accuracy: 0.0839\n",
      "Epoch 970/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3155 - accuracy: 0.0826\n",
      "Epoch 970: loss did not improve from 3.31471\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3155 - accuracy: 0.0826 - val_loss: 3.2918 - val_accuracy: 0.0845\n",
      "Epoch 971/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0830\n",
      "Epoch 971: loss improved from 3.31471 to 3.31468, saving model to best_model.h5\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3147 - accuracy: 0.0828 - val_loss: 3.2914 - val_accuracy: 0.0839\n",
      "Epoch 972/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3182 - accuracy: 0.0812\n",
      "Epoch 972: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3182 - accuracy: 0.0812 - val_loss: 3.2927 - val_accuracy: 0.0851\n",
      "Epoch 973/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3188 - accuracy: 0.0821\n",
      "Epoch 973: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3148 - accuracy: 0.0826 - val_loss: 3.2916 - val_accuracy: 0.0851\n",
      "Epoch 974/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0806\n",
      "Epoch 974: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3152 - accuracy: 0.0806 - val_loss: 3.2920 - val_accuracy: 0.0821\n",
      "Epoch 975/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0824\n",
      "Epoch 975: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3163 - accuracy: 0.0820 - val_loss: 3.2944 - val_accuracy: 0.0851\n",
      "Epoch 976/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0843\n",
      "Epoch 976: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3155 - accuracy: 0.0834 - val_loss: 3.2915 - val_accuracy: 0.0857\n",
      "Epoch 977/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0798\n",
      "Epoch 977: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3156 - accuracy: 0.0798 - val_loss: 3.2928 - val_accuracy: 0.0845\n",
      "Epoch 978/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0810\n",
      "Epoch 978: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3149 - accuracy: 0.0810 - val_loss: 3.2911 - val_accuracy: 0.0839\n",
      "Epoch 979/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3154 - accuracy: 0.0799\n",
      "Epoch 979: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3154 - accuracy: 0.0799 - val_loss: 3.2918 - val_accuracy: 0.0827\n",
      "Epoch 980/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0819\n",
      "Epoch 980: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3154 - accuracy: 0.0820 - val_loss: 3.2919 - val_accuracy: 0.0876\n",
      "Epoch 981/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3149 - accuracy: 0.0799\n",
      "Epoch 981: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 2s 5ms/step - loss: 3.3162 - accuracy: 0.0798 - val_loss: 3.2926 - val_accuracy: 0.0827\n",
      "Epoch 982/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0833\n",
      "Epoch 982: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3154 - accuracy: 0.0832 - val_loss: 3.2940 - val_accuracy: 0.0857\n",
      "Epoch 983/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0795\n",
      "Epoch 983: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3154 - accuracy: 0.0795 - val_loss: 3.2922 - val_accuracy: 0.0876\n",
      "Epoch 984/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3121 - accuracy: 0.0814\n",
      "Epoch 984: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3151 - accuracy: 0.0817 - val_loss: 3.2922 - val_accuracy: 0.0833\n",
      "Epoch 985/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3127 - accuracy: 0.0787\n",
      "Epoch 985: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3149 - accuracy: 0.0779 - val_loss: 3.3136 - val_accuracy: 0.0833\n",
      "Epoch 986/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0820\n",
      "Epoch 986: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3152 - accuracy: 0.0822 - val_loss: 3.2912 - val_accuracy: 0.0863\n",
      "Epoch 987/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0827\n",
      "Epoch 987: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3162 - accuracy: 0.0823 - val_loss: 3.2947 - val_accuracy: 0.0919\n",
      "Epoch 988/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0796\n",
      "Epoch 988: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3158 - accuracy: 0.0801 - val_loss: 3.2923 - val_accuracy: 0.0863\n",
      "Epoch 989/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0812\n",
      "Epoch 989: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3152 - accuracy: 0.0811 - val_loss: 3.2926 - val_accuracy: 0.0857\n",
      "Epoch 990/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0808\n",
      "Epoch 990: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3156 - accuracy: 0.0807 - val_loss: 3.2927 - val_accuracy: 0.0827\n",
      "Epoch 991/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3181 - accuracy: 0.0824\n",
      "Epoch 991: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 7s 25ms/step - loss: 3.3156 - accuracy: 0.0822 - val_loss: 3.2924 - val_accuracy: 0.0845\n",
      "Epoch 992/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3199 - accuracy: 0.0821\n",
      "Epoch 992: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3200 - accuracy: 0.0821 - val_loss: 3.2929 - val_accuracy: 0.0857\n",
      "Epoch 993/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3183 - accuracy: 0.0818\n",
      "Epoch 993: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 21ms/step - loss: 3.3152 - accuracy: 0.0828 - val_loss: 3.2921 - val_accuracy: 0.0870\n",
      "Epoch 994/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0815\n",
      "Epoch 994: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3151 - accuracy: 0.0815 - val_loss: 3.2927 - val_accuracy: 0.0882\n",
      "Epoch 995/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0811\n",
      "Epoch 995: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 10s 37ms/step - loss: 3.3155 - accuracy: 0.0811 - val_loss: 3.2943 - val_accuracy: 0.0870\n",
      "Epoch 996/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0835\n",
      "Epoch 996: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3151 - accuracy: 0.0832 - val_loss: 3.2916 - val_accuracy: 0.0876\n",
      "Epoch 997/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0827\n",
      "Epoch 997: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3154 - accuracy: 0.0827 - val_loss: 3.2924 - val_accuracy: 0.0857\n",
      "Epoch 998/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0817\n",
      "Epoch 998: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3148 - accuracy: 0.0821 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 999/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0844\n",
      "Epoch 999: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3149 - accuracy: 0.0839 - val_loss: 3.2918 - val_accuracy: 0.0814\n",
      "Epoch 1000/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3150 - accuracy: 0.0832\n",
      "Epoch 1000: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3171 - accuracy: 0.0832 - val_loss: 3.2920 - val_accuracy: 0.0833\n",
      "Epoch 1001/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3169 - accuracy: 0.0811\n",
      "Epoch 1001: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3151 - accuracy: 0.0816 - val_loss: 3.2925 - val_accuracy: 0.0870\n",
      "Epoch 1002/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0794\n",
      "Epoch 1002: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3147 - accuracy: 0.0793 - val_loss: 3.2903 - val_accuracy: 0.0882\n",
      "Epoch 1003/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3122 - accuracy: 0.0823\n",
      "Epoch 1003: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3149 - accuracy: 0.0812 - val_loss: 3.2908 - val_accuracy: 0.0876\n",
      "Epoch 1004/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3172 - accuracy: 0.0818\n",
      "Epoch 1004: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3175 - accuracy: 0.0820 - val_loss: 3.2921 - val_accuracy: 0.0876\n",
      "Epoch 1005/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0820\n",
      "Epoch 1005: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3154 - accuracy: 0.0813 - val_loss: 3.2913 - val_accuracy: 0.0827\n",
      "Epoch 1006/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0827\n",
      "Epoch 1006: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3148 - accuracy: 0.0828 - val_loss: 3.2908 - val_accuracy: 0.0821\n",
      "Epoch 1007/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0818\n",
      "Epoch 1007: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3154 - accuracy: 0.0820 - val_loss: 3.2902 - val_accuracy: 0.0863\n",
      "Epoch 1008/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0809\n",
      "Epoch 1008: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3154 - accuracy: 0.0810 - val_loss: 3.2912 - val_accuracy: 0.0870\n",
      "Epoch 1009/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3148 - accuracy: 0.0832\n",
      "Epoch 1009: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3148 - accuracy: 0.0832 - val_loss: 3.2926 - val_accuracy: 0.0863\n",
      "Epoch 1010/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3124 - accuracy: 0.0797\n",
      "Epoch 1010: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3159 - accuracy: 0.0798 - val_loss: 3.2944 - val_accuracy: 0.0802\n",
      "Epoch 1011/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0826\n",
      "Epoch 1011: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 10s 40ms/step - loss: 3.3156 - accuracy: 0.0827 - val_loss: 3.2909 - val_accuracy: 0.0833\n",
      "Epoch 1012/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0805\n",
      "Epoch 1012: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3161 - accuracy: 0.0805 - val_loss: 3.2920 - val_accuracy: 0.0845\n",
      "Epoch 1013/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0828\n",
      "Epoch 1013: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3157 - accuracy: 0.0821 - val_loss: 3.2939 - val_accuracy: 0.0863\n",
      "Epoch 1014/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0812\n",
      "Epoch 1014: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3163 - accuracy: 0.0811 - val_loss: 3.2925 - val_accuracy: 0.0845\n",
      "Epoch 1015/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3153 - accuracy: 0.0822\n",
      "Epoch 1015: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 37ms/step - loss: 3.3153 - accuracy: 0.0822 - val_loss: 3.2922 - val_accuracy: 0.0857\n",
      "Epoch 1016/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0814\n",
      "Epoch 1016: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3152 - accuracy: 0.0817 - val_loss: 3.2914 - val_accuracy: 0.0814\n",
      "Epoch 1017/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0808\n",
      "Epoch 1017: loss improved from 3.31468 to 3.31468, saving model to best_model.h5\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3147 - accuracy: 0.0807 - val_loss: 3.2916 - val_accuracy: 0.0821\n",
      "Epoch 1018/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0840\n",
      "Epoch 1018: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3157 - accuracy: 0.0838 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 1019/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0808\n",
      "Epoch 1019: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3153 - accuracy: 0.0806 - val_loss: 3.2924 - val_accuracy: 0.0845\n",
      "Epoch 1020/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3171 - accuracy: 0.0827\n",
      "Epoch 1020: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3160 - accuracy: 0.0826 - val_loss: 3.2926 - val_accuracy: 0.0876\n",
      "Epoch 1021/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3128 - accuracy: 0.0826\n",
      "Epoch 1021: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3150 - accuracy: 0.0822 - val_loss: 3.2918 - val_accuracy: 0.0827\n",
      "Epoch 1022/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0821\n",
      "Epoch 1022: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3150 - accuracy: 0.0821 - val_loss: 3.2906 - val_accuracy: 0.0845\n",
      "Epoch 1023/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0809\n",
      "Epoch 1023: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3153 - accuracy: 0.0810 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 1024/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3149 - accuracy: 0.0822\n",
      "Epoch 1024: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 11s 42ms/step - loss: 3.3149 - accuracy: 0.0822 - val_loss: 3.2909 - val_accuracy: 0.0851\n",
      "Epoch 1025/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0843\n",
      "Epoch 1025: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3153 - accuracy: 0.0844 - val_loss: 3.2944 - val_accuracy: 0.0821\n",
      "Epoch 1026/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0805\n",
      "Epoch 1026: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3151 - accuracy: 0.0805 - val_loss: 3.2917 - val_accuracy: 0.0857\n",
      "Epoch 1027/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0824\n",
      "Epoch 1027: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3152 - accuracy: 0.0823 - val_loss: 3.2921 - val_accuracy: 0.0839\n",
      "Epoch 1028/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3170 - accuracy: 0.0834\n",
      "Epoch 1028: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3155 - accuracy: 0.0837 - val_loss: 3.2925 - val_accuracy: 0.0851\n",
      "Epoch 1029/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3166 - accuracy: 0.0809\n",
      "Epoch 1029: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3166 - accuracy: 0.0809 - val_loss: 3.2933 - val_accuracy: 0.0857\n",
      "Epoch 1030/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0841\n",
      "Epoch 1030: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3149 - accuracy: 0.0840 - val_loss: 3.2918 - val_accuracy: 0.0870\n",
      "Epoch 1031/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0840\n",
      "Epoch 1031: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 33ms/step - loss: 3.3160 - accuracy: 0.0831 - val_loss: 3.2949 - val_accuracy: 0.0857\n",
      "Epoch 1032/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3141 - accuracy: 0.0822\n",
      "Epoch 1032: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3155 - accuracy: 0.0820 - val_loss: 3.2919 - val_accuracy: 0.0778\n",
      "Epoch 1033/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0812\n",
      "Epoch 1033: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 33ms/step - loss: 3.3151 - accuracy: 0.0812 - val_loss: 3.2916 - val_accuracy: 0.0882\n",
      "Epoch 1034/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3146 - accuracy: 0.0815\n",
      "Epoch 1034: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3149 - accuracy: 0.0823 - val_loss: 3.2927 - val_accuracy: 0.0827\n",
      "Epoch 1035/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3136 - accuracy: 0.0818\n",
      "Epoch 1035: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3150 - accuracy: 0.0822 - val_loss: 3.2918 - val_accuracy: 0.0894\n",
      "Epoch 1036/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0817\n",
      "Epoch 1036: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3151 - accuracy: 0.0820 - val_loss: 3.2915 - val_accuracy: 0.0845\n",
      "Epoch 1037/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3150 - accuracy: 0.0795\n",
      "Epoch 1037: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3150 - accuracy: 0.0795 - val_loss: 3.2910 - val_accuracy: 0.0814\n",
      "Epoch 1038/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3155 - accuracy: 0.0812\n",
      "Epoch 1038: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3155 - accuracy: 0.0812 - val_loss: 3.2916 - val_accuracy: 0.0833\n",
      "Epoch 1039/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0822\n",
      "Epoch 1039: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3148 - accuracy: 0.0822 - val_loss: 3.2914 - val_accuracy: 0.0857\n",
      "Epoch 1040/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3176 - accuracy: 0.0827\n",
      "Epoch 1040: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3203 - accuracy: 0.0812 - val_loss: 3.2980 - val_accuracy: 0.0876\n",
      "Epoch 1041/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0806\n",
      "Epoch 1041: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 10s 38ms/step - loss: 3.3166 - accuracy: 0.0805 - val_loss: 3.2935 - val_accuracy: 0.0888\n",
      "Epoch 1042/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0821\n",
      "Epoch 1042: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3148 - accuracy: 0.0821 - val_loss: 3.2919 - val_accuracy: 0.0870\n",
      "Epoch 1043/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0825\n",
      "Epoch 1043: loss did not improve from 3.31468\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3149 - accuracy: 0.0824 - val_loss: 3.2929 - val_accuracy: 0.0821\n",
      "Epoch 1044/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0817\n",
      "Epoch 1044: loss improved from 3.31468 to 3.31449, saving model to best_model.h5\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3145 - accuracy: 0.0817 - val_loss: 3.2927 - val_accuracy: 0.0808\n",
      "Epoch 1045/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3157 - accuracy: 0.0802\n",
      "Epoch 1045: loss did not improve from 3.31449\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3157 - accuracy: 0.0802 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 1046/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0821\n",
      "Epoch 1046: loss improved from 3.31449 to 3.31441, saving model to best_model.h5\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3144 - accuracy: 0.0821 - val_loss: 3.2917 - val_accuracy: 0.0845\n",
      "Epoch 1047/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0826\n",
      "Epoch 1047: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 33ms/step - loss: 3.3149 - accuracy: 0.0824 - val_loss: 3.2921 - val_accuracy: 0.0839\n",
      "Epoch 1048/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0806\n",
      "Epoch 1048: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3152 - accuracy: 0.0806 - val_loss: 3.2931 - val_accuracy: 0.0833\n",
      "Epoch 1049/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0817\n",
      "Epoch 1049: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3151 - accuracy: 0.0816 - val_loss: 3.2916 - val_accuracy: 0.0876\n",
      "Epoch 1050/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0787\n",
      "Epoch 1050: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3146 - accuracy: 0.0785 - val_loss: 3.2917 - val_accuracy: 0.0857\n",
      "Epoch 1051/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0795\n",
      "Epoch 1051: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 33ms/step - loss: 3.3146 - accuracy: 0.0795 - val_loss: 3.2913 - val_accuracy: 0.0863\n",
      "Epoch 1052/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0809\n",
      "Epoch 1052: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3148 - accuracy: 0.0810 - val_loss: 3.2910 - val_accuracy: 0.0870\n",
      "Epoch 1053/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3150 - accuracy: 0.0799\n",
      "Epoch 1053: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3150 - accuracy: 0.0799 - val_loss: 3.2913 - val_accuracy: 0.0839\n",
      "Epoch 1054/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0814\n",
      "Epoch 1054: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3152 - accuracy: 0.0811 - val_loss: 3.2914 - val_accuracy: 0.0839\n",
      "Epoch 1055/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3178 - accuracy: 0.0811\n",
      "Epoch 1055: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3155 - accuracy: 0.0815 - val_loss: 3.2915 - val_accuracy: 0.0827\n",
      "Epoch 1056/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0827\n",
      "Epoch 1056: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 3.3146 - accuracy: 0.0827 - val_loss: 3.2905 - val_accuracy: 0.0827\n",
      "Epoch 1057/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3154 - accuracy: 0.0812\n",
      "Epoch 1057: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3158 - accuracy: 0.0813 - val_loss: 3.2915 - val_accuracy: 0.0857\n",
      "Epoch 1058/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0821\n",
      "Epoch 1058: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3148 - accuracy: 0.0822 - val_loss: 3.2923 - val_accuracy: 0.0857\n",
      "Epoch 1059/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0814\n",
      "Epoch 1059: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3151 - accuracy: 0.0815 - val_loss: 3.2931 - val_accuracy: 0.0833\n",
      "Epoch 1060/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0824\n",
      "Epoch 1060: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3156 - accuracy: 0.0818 - val_loss: 3.2915 - val_accuracy: 0.0851\n",
      "Epoch 1061/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0824\n",
      "Epoch 1061: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3150 - accuracy: 0.0823 - val_loss: 3.2918 - val_accuracy: 0.0857\n",
      "Epoch 1062/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3152 - accuracy: 0.0809\n",
      "Epoch 1062: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 10s 40ms/step - loss: 3.3152 - accuracy: 0.0809 - val_loss: 3.2911 - val_accuracy: 0.0851\n",
      "Epoch 1063/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3145 - accuracy: 0.0845\n",
      "Epoch 1063: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3163 - accuracy: 0.0835 - val_loss: 3.2975 - val_accuracy: 0.0821\n",
      "Epoch 1064/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0829\n",
      "Epoch 1064: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3158 - accuracy: 0.0820 - val_loss: 3.2939 - val_accuracy: 0.0894\n",
      "Epoch 1065/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0810\n",
      "Epoch 1065: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3158 - accuracy: 0.0809 - val_loss: 3.2915 - val_accuracy: 0.0845\n",
      "Epoch 1066/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0830\n",
      "Epoch 1066: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 25ms/step - loss: 3.3144 - accuracy: 0.0831 - val_loss: 3.2919 - val_accuracy: 0.0827\n",
      "Epoch 1067/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0838\n",
      "Epoch 1067: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 33ms/step - loss: 3.3148 - accuracy: 0.0839 - val_loss: 3.2910 - val_accuracy: 0.0857\n",
      "Epoch 1068/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3161 - accuracy: 0.0813\n",
      "Epoch 1068: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3161 - accuracy: 0.0813 - val_loss: 3.2939 - val_accuracy: 0.0863\n",
      "Epoch 1069/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3151 - accuracy: 0.0809\n",
      "Epoch 1069: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 5s 21ms/step - loss: 3.3151 - accuracy: 0.0809 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 1070/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0836\n",
      "Epoch 1070: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3154 - accuracy: 0.0835 - val_loss: 3.2924 - val_accuracy: 0.0912\n",
      "Epoch 1071/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0830\n",
      "Epoch 1071: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 37ms/step - loss: 3.3155 - accuracy: 0.0829 - val_loss: 3.2920 - val_accuracy: 0.0882\n",
      "Epoch 1072/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0828\n",
      "Epoch 1072: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3150 - accuracy: 0.0826 - val_loss: 3.2912 - val_accuracy: 0.0876\n",
      "Epoch 1073/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3163 - accuracy: 0.0820\n",
      "Epoch 1073: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3163 - accuracy: 0.0820 - val_loss: 3.2926 - val_accuracy: 0.0839\n",
      "Epoch 1074/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3152 - accuracy: 0.0811\n",
      "Epoch 1074: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3152 - accuracy: 0.0811 - val_loss: 3.2937 - val_accuracy: 0.0882\n",
      "Epoch 1075/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3136 - accuracy: 0.0834\n",
      "Epoch 1075: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 3.3147 - accuracy: 0.0828 - val_loss: 3.2910 - val_accuracy: 0.0870\n",
      "Epoch 1076/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0828\n",
      "Epoch 1076: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3168 - accuracy: 0.0829 - val_loss: 3.2929 - val_accuracy: 0.0882\n",
      "Epoch 1077/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0835\n",
      "Epoch 1077: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3151 - accuracy: 0.0834 - val_loss: 3.2920 - val_accuracy: 0.0857\n",
      "Epoch 1078/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3121 - accuracy: 0.0808\n",
      "Epoch 1078: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3144 - accuracy: 0.0820 - val_loss: 3.2920 - val_accuracy: 0.0863\n",
      "Epoch 1079/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3145 - accuracy: 0.0817\n",
      "Epoch 1079: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3151 - accuracy: 0.0815 - val_loss: 3.2925 - val_accuracy: 0.0863\n",
      "Epoch 1080/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3133 - accuracy: 0.0815\n",
      "Epoch 1080: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3149 - accuracy: 0.0822 - val_loss: 3.2923 - val_accuracy: 0.0821\n",
      "Epoch 1081/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0819\n",
      "Epoch 1081: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3171 - accuracy: 0.0817 - val_loss: 3.2924 - val_accuracy: 0.0870\n",
      "Epoch 1082/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0805\n",
      "Epoch 1082: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3150 - accuracy: 0.0810 - val_loss: 3.2923 - val_accuracy: 0.0845\n",
      "Epoch 1083/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0814\n",
      "Epoch 1083: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3147 - accuracy: 0.0815 - val_loss: 3.2917 - val_accuracy: 0.0833\n",
      "Epoch 1084/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3147 - accuracy: 0.0811\n",
      "Epoch 1084: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3147 - accuracy: 0.0811 - val_loss: 3.2922 - val_accuracy: 0.0821\n",
      "Epoch 1085/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3179 - accuracy: 0.0806\n",
      "Epoch 1085: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3179 - accuracy: 0.0806 - val_loss: 3.2925 - val_accuracy: 0.0796\n",
      "Epoch 1086/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0815\n",
      "Epoch 1086: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3150 - accuracy: 0.0815 - val_loss: 3.2918 - val_accuracy: 0.0845\n",
      "Epoch 1087/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0817\n",
      "Epoch 1087: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3145 - accuracy: 0.0815 - val_loss: 3.2918 - val_accuracy: 0.0863\n",
      "Epoch 1088/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3162 - accuracy: 0.0818\n",
      "Epoch 1088: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 10s 37ms/step - loss: 3.3158 - accuracy: 0.0821 - val_loss: 3.2927 - val_accuracy: 0.0845\n",
      "Epoch 1089/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3149 - accuracy: 0.0849\n",
      "Epoch 1089: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 27ms/step - loss: 3.3149 - accuracy: 0.0849 - val_loss: 3.2930 - val_accuracy: 0.0863\n",
      "Epoch 1090/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3145 - accuracy: 0.0827\n",
      "Epoch 1090: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 3.3144 - accuracy: 0.0821 - val_loss: 3.2922 - val_accuracy: 0.0814\n",
      "Epoch 1091/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0810\n",
      "Epoch 1091: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 9s 36ms/step - loss: 3.3144 - accuracy: 0.0811 - val_loss: 3.2921 - val_accuracy: 0.0857\n",
      "Epoch 1092/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3139 - accuracy: 0.0808\n",
      "Epoch 1092: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 11s 45ms/step - loss: 3.3153 - accuracy: 0.0812 - val_loss: 3.2926 - val_accuracy: 0.0814\n",
      "Epoch 1093/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3142 - accuracy: 0.0811\n",
      "Epoch 1093: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3146 - accuracy: 0.0818 - val_loss: 3.2988 - val_accuracy: 0.0790\n",
      "Epoch 1094/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3131 - accuracy: 0.0802\n",
      "Epoch 1094: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3152 - accuracy: 0.0804 - val_loss: 3.2922 - val_accuracy: 0.0845\n",
      "Epoch 1095/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3195 - accuracy: 0.0806\n",
      "Epoch 1095: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3195 - accuracy: 0.0806 - val_loss: 3.2934 - val_accuracy: 0.0900\n",
      "Epoch 1096/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3176 - accuracy: 0.0826\n",
      "Epoch 1096: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 33ms/step - loss: 3.3158 - accuracy: 0.0832 - val_loss: 3.2948 - val_accuracy: 0.0906\n",
      "Epoch 1097/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0828\n",
      "Epoch 1097: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3151 - accuracy: 0.0832 - val_loss: 3.2918 - val_accuracy: 0.0851\n",
      "Epoch 1098/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0816\n",
      "Epoch 1098: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 10s 39ms/step - loss: 3.3160 - accuracy: 0.0816 - val_loss: 3.2933 - val_accuracy: 0.0839\n",
      "Epoch 1099/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3156 - accuracy: 0.0810\n",
      "Epoch 1099: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3156 - accuracy: 0.0810 - val_loss: 3.2917 - val_accuracy: 0.0833\n",
      "Epoch 1100/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0824\n",
      "Epoch 1100: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3146 - accuracy: 0.0822 - val_loss: 3.2930 - val_accuracy: 0.0863\n",
      "Epoch 1101/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3137 - accuracy: 0.0798\n",
      "Epoch 1101: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3145 - accuracy: 0.0802 - val_loss: 3.2915 - val_accuracy: 0.0851\n",
      "Epoch 1102/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0797\n",
      "Epoch 1102: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3149 - accuracy: 0.0800 - val_loss: 3.2928 - val_accuracy: 0.0863\n",
      "Epoch 1103/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0810\n",
      "Epoch 1103: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 10s 41ms/step - loss: 3.3154 - accuracy: 0.0812 - val_loss: 3.2921 - val_accuracy: 0.0876\n",
      "Epoch 1104/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0812\n",
      "Epoch 1104: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 5s 17ms/step - loss: 3.3153 - accuracy: 0.0812 - val_loss: 3.2929 - val_accuracy: 0.0839\n",
      "Epoch 1105/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3155 - accuracy: 0.0810\n",
      "Epoch 1105: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 10s 38ms/step - loss: 3.3155 - accuracy: 0.0810 - val_loss: 3.2926 - val_accuracy: 0.0882\n",
      "Epoch 1106/5000\n",
      "235/256 [==========================>...] - ETA: 0s - loss: 3.3160 - accuracy: 0.0809\n",
      "Epoch 1106: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 7s 26ms/step - loss: 3.3151 - accuracy: 0.0812 - val_loss: 3.2906 - val_accuracy: 0.0894\n",
      "Epoch 1107/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0807\n",
      "Epoch 1107: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3158 - accuracy: 0.0807 - val_loss: 3.2935 - val_accuracy: 0.0851\n",
      "Epoch 1108/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3129 - accuracy: 0.0832\n",
      "Epoch 1108: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3147 - accuracy: 0.0826 - val_loss: 3.2923 - val_accuracy: 0.0833\n",
      "Epoch 1109/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3160 - accuracy: 0.0802\n",
      "Epoch 1109: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3160 - accuracy: 0.0802 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 1110/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3148 - accuracy: 0.0798\n",
      "Epoch 1110: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3148 - accuracy: 0.0798 - val_loss: 3.2926 - val_accuracy: 0.0814\n",
      "Epoch 1111/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0816\n",
      "Epoch 1111: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3170 - accuracy: 0.0818 - val_loss: 3.2985 - val_accuracy: 0.0845\n",
      "Epoch 1112/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3174 - accuracy: 0.0822\n",
      "Epoch 1112: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3175 - accuracy: 0.0822 - val_loss: 3.2930 - val_accuracy: 0.0888\n",
      "Epoch 1113/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3141 - accuracy: 0.0825\n",
      "Epoch 1113: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3151 - accuracy: 0.0826 - val_loss: 3.2925 - val_accuracy: 0.0882\n",
      "Epoch 1114/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3130 - accuracy: 0.0842\n",
      "Epoch 1114: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3148 - accuracy: 0.0837 - val_loss: 3.2913 - val_accuracy: 0.0833\n",
      "Epoch 1115/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3184 - accuracy: 0.0804\n",
      "Epoch 1115: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3169 - accuracy: 0.0812 - val_loss: 3.2917 - val_accuracy: 0.0851\n",
      "Epoch 1116/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0820\n",
      "Epoch 1116: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3148 - accuracy: 0.0822 - val_loss: 3.2935 - val_accuracy: 0.0839\n",
      "Epoch 1117/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0817\n",
      "Epoch 1117: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3149 - accuracy: 0.0817 - val_loss: 3.2915 - val_accuracy: 0.0851\n",
      "Epoch 1118/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0826\n",
      "Epoch 1118: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3147 - accuracy: 0.0828 - val_loss: 3.2921 - val_accuracy: 0.0845\n",
      "Epoch 1119/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3147 - accuracy: 0.0819\n",
      "Epoch 1119: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3147 - accuracy: 0.0821 - val_loss: 3.2930 - val_accuracy: 0.0876\n",
      "Epoch 1120/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0817\n",
      "Epoch 1120: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3146 - accuracy: 0.0817 - val_loss: 3.2917 - val_accuracy: 0.0870\n",
      "Epoch 1121/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3152 - accuracy: 0.0820\n",
      "Epoch 1121: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3152 - accuracy: 0.0820 - val_loss: 3.2921 - val_accuracy: 0.0827\n",
      "Epoch 1122/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0821\n",
      "Epoch 1122: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3144 - accuracy: 0.0822 - val_loss: 3.2909 - val_accuracy: 0.0870\n",
      "Epoch 1123/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0827\n",
      "Epoch 1123: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3148 - accuracy: 0.0828 - val_loss: 3.2915 - val_accuracy: 0.0851\n",
      "Epoch 1124/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0830\n",
      "Epoch 1124: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3158 - accuracy: 0.0824 - val_loss: 3.2917 - val_accuracy: 0.0906\n",
      "Epoch 1125/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0806\n",
      "Epoch 1125: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3148 - accuracy: 0.0806 - val_loss: 3.2912 - val_accuracy: 0.0851\n",
      "Epoch 1126/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3132 - accuracy: 0.0821\n",
      "Epoch 1126: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3146 - accuracy: 0.0827 - val_loss: 3.2929 - val_accuracy: 0.0882\n",
      "Epoch 1127/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3187 - accuracy: 0.0820\n",
      "Epoch 1127: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3149 - accuracy: 0.0822 - val_loss: 3.2930 - val_accuracy: 0.0851\n",
      "Epoch 1128/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3167 - accuracy: 0.0819\n",
      "Epoch 1128: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3172 - accuracy: 0.0824 - val_loss: 3.2940 - val_accuracy: 0.0845\n",
      "Epoch 1129/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3166 - accuracy: 0.0809\n",
      "Epoch 1129: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3154 - accuracy: 0.0811 - val_loss: 3.2930 - val_accuracy: 0.0857\n",
      "Epoch 1130/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3142 - accuracy: 0.0826\n",
      "Epoch 1130: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 4s 13ms/step - loss: 3.3146 - accuracy: 0.0824 - val_loss: 3.2909 - val_accuracy: 0.0833\n",
      "Epoch 1131/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3149 - accuracy: 0.0823\n",
      "Epoch 1131: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3149 - accuracy: 0.0823 - val_loss: 3.2911 - val_accuracy: 0.0851\n",
      "Epoch 1132/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3230 - accuracy: 0.0809\n",
      "Epoch 1132: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3172 - accuracy: 0.0815 - val_loss: 3.2925 - val_accuracy: 0.0870\n",
      "Epoch 1133/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0836\n",
      "Epoch 1133: loss did not improve from 3.31441\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3149 - accuracy: 0.0835 - val_loss: 3.2917 - val_accuracy: 0.0851\n",
      "Epoch 1134/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0816\n",
      "Epoch 1134: loss improved from 3.31441 to 3.31425, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3143 - accuracy: 0.0816 - val_loss: 3.2923 - val_accuracy: 0.0808\n",
      "Epoch 1135/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3154 - accuracy: 0.0817\n",
      "Epoch 1135: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 9ms/step - loss: 3.3150 - accuracy: 0.0828 - val_loss: 3.2932 - val_accuracy: 0.0845\n",
      "Epoch 1136/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3117 - accuracy: 0.0807\n",
      "Epoch 1136: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3147 - accuracy: 0.0810 - val_loss: 3.2936 - val_accuracy: 0.0833\n",
      "Epoch 1137/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0826\n",
      "Epoch 1137: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3156 - accuracy: 0.0822 - val_loss: 3.2998 - val_accuracy: 0.0851\n",
      "Epoch 1138/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3163 - accuracy: 0.0803\n",
      "Epoch 1138: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3163 - accuracy: 0.0806 - val_loss: 3.2921 - val_accuracy: 0.0845\n",
      "Epoch 1139/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3137 - accuracy: 0.0834\n",
      "Epoch 1139: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3144 - accuracy: 0.0832 - val_loss: 3.2919 - val_accuracy: 0.0857\n",
      "Epoch 1140/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3145 - accuracy: 0.0815\n",
      "Epoch 1140: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3145 - accuracy: 0.0815 - val_loss: 3.2920 - val_accuracy: 0.0845\n",
      "Epoch 1141/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0820\n",
      "Epoch 1141: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3154 - accuracy: 0.0822 - val_loss: 3.2921 - val_accuracy: 0.0851\n",
      "Epoch 1142/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0837\n",
      "Epoch 1142: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3155 - accuracy: 0.0837 - val_loss: 3.2961 - val_accuracy: 0.0845\n",
      "Epoch 1143/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0832\n",
      "Epoch 1143: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3152 - accuracy: 0.0827 - val_loss: 3.2919 - val_accuracy: 0.0857\n",
      "Epoch 1144/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0833\n",
      "Epoch 1144: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3151 - accuracy: 0.0833 - val_loss: 3.2913 - val_accuracy: 0.0888\n",
      "Epoch 1145/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3178 - accuracy: 0.0813\n",
      "Epoch 1145: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3155 - accuracy: 0.0817 - val_loss: 3.2936 - val_accuracy: 0.0863\n",
      "Epoch 1146/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3150 - accuracy: 0.0826\n",
      "Epoch 1146: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3152 - accuracy: 0.0827 - val_loss: 3.2927 - val_accuracy: 0.0863\n",
      "Epoch 1147/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0819\n",
      "Epoch 1147: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3146 - accuracy: 0.0818 - val_loss: 3.2912 - val_accuracy: 0.0870\n",
      "Epoch 1148/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0828\n",
      "Epoch 1148: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3149 - accuracy: 0.0831 - val_loss: 3.2916 - val_accuracy: 0.0870\n",
      "Epoch 1149/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0834\n",
      "Epoch 1149: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3147 - accuracy: 0.0838 - val_loss: 3.2918 - val_accuracy: 0.0888\n",
      "Epoch 1150/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0838\n",
      "Epoch 1150: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3149 - accuracy: 0.0840 - val_loss: 3.2927 - val_accuracy: 0.0845\n",
      "Epoch 1151/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0826\n",
      "Epoch 1151: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 9ms/step - loss: 3.3162 - accuracy: 0.0824 - val_loss: 3.2966 - val_accuracy: 0.0827\n",
      "Epoch 1152/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3138 - accuracy: 0.0814\n",
      "Epoch 1152: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3154 - accuracy: 0.0811 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 1153/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0807\n",
      "Epoch 1153: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3146 - accuracy: 0.0807 - val_loss: 3.2925 - val_accuracy: 0.0863\n",
      "Epoch 1154/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0804\n",
      "Epoch 1154: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3146 - accuracy: 0.0809 - val_loss: 3.2911 - val_accuracy: 0.0839\n",
      "Epoch 1155/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3128 - accuracy: 0.0824\n",
      "Epoch 1155: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3147 - accuracy: 0.0823 - val_loss: 3.2925 - val_accuracy: 0.0845\n",
      "Epoch 1156/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3175 - accuracy: 0.0827\n",
      "Epoch 1156: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3143 - accuracy: 0.0829 - val_loss: 3.2908 - val_accuracy: 0.0845\n",
      "Epoch 1157/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3146 - accuracy: 0.0811\n",
      "Epoch 1157: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3152 - accuracy: 0.0812 - val_loss: 3.2911 - val_accuracy: 0.0827\n",
      "Epoch 1158/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3184 - accuracy: 0.0815\n",
      "Epoch 1158: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3180 - accuracy: 0.0817 - val_loss: 3.2921 - val_accuracy: 0.0888\n",
      "Epoch 1159/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3137 - accuracy: 0.0809\n",
      "Epoch 1159: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3153 - accuracy: 0.0809 - val_loss: 3.2920 - val_accuracy: 0.0827\n",
      "Epoch 1160/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0841\n",
      "Epoch 1160: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3154 - accuracy: 0.0837 - val_loss: 3.2922 - val_accuracy: 0.0839\n",
      "Epoch 1161/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0826\n",
      "Epoch 1161: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 11s 44ms/step - loss: 3.3148 - accuracy: 0.0829 - val_loss: 3.2912 - val_accuracy: 0.0827\n",
      "Epoch 1162/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0822\n",
      "Epoch 1162: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 3.3149 - accuracy: 0.0824 - val_loss: 3.2915 - val_accuracy: 0.0839\n",
      "Epoch 1163/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3107 - accuracy: 0.0826\n",
      "Epoch 1163: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3144 - accuracy: 0.0818 - val_loss: 3.2909 - val_accuracy: 0.0870\n",
      "Epoch 1164/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3193 - accuracy: 0.0809\n",
      "Epoch 1164: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3168 - accuracy: 0.0811 - val_loss: 3.2937 - val_accuracy: 0.0857\n",
      "Epoch 1165/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0839\n",
      "Epoch 1165: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3159 - accuracy: 0.0831 - val_loss: 3.2923 - val_accuracy: 0.0845\n",
      "Epoch 1166/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3150 - accuracy: 0.0820\n",
      "Epoch 1166: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3150 - accuracy: 0.0820 - val_loss: 3.2912 - val_accuracy: 0.0802\n",
      "Epoch 1167/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3110 - accuracy: 0.0825\n",
      "Epoch 1167: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3146 - accuracy: 0.0817 - val_loss: 3.2915 - val_accuracy: 0.0870\n",
      "Epoch 1168/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3125 - accuracy: 0.0808\n",
      "Epoch 1168: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3148 - accuracy: 0.0802 - val_loss: 3.2907 - val_accuracy: 0.0863\n",
      "Epoch 1169/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3143 - accuracy: 0.0806\n",
      "Epoch 1169: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3143 - accuracy: 0.0806 - val_loss: 3.2911 - val_accuracy: 0.0857\n",
      "Epoch 1170/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3130 - accuracy: 0.0826\n",
      "Epoch 1170: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3155 - accuracy: 0.0824 - val_loss: 3.2908 - val_accuracy: 0.0857\n",
      "Epoch 1171/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0835\n",
      "Epoch 1171: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3153 - accuracy: 0.0833 - val_loss: 3.2919 - val_accuracy: 0.0888\n",
      "Epoch 1172/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3150 - accuracy: 0.0820\n",
      "Epoch 1172: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 10s 38ms/step - loss: 3.3150 - accuracy: 0.0820 - val_loss: 3.2925 - val_accuracy: 0.0876\n",
      "Epoch 1173/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3152 - accuracy: 0.0823\n",
      "Epoch 1173: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3154 - accuracy: 0.0824 - val_loss: 3.2930 - val_accuracy: 0.0870\n",
      "Epoch 1174/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3109 - accuracy: 0.0827\n",
      "Epoch 1174: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3150 - accuracy: 0.0821 - val_loss: 3.2918 - val_accuracy: 0.0870\n",
      "Epoch 1175/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3158 - accuracy: 0.0812\n",
      "Epoch 1175: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3158 - accuracy: 0.0812 - val_loss: 3.2945 - val_accuracy: 0.0857\n",
      "Epoch 1176/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3138 - accuracy: 0.0827\n",
      "Epoch 1176: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3149 - accuracy: 0.0824 - val_loss: 3.2913 - val_accuracy: 0.0851\n",
      "Epoch 1177/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0830\n",
      "Epoch 1177: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3152 - accuracy: 0.0821 - val_loss: 3.2915 - val_accuracy: 0.0863\n",
      "Epoch 1178/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0834\n",
      "Epoch 1178: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 10s 38ms/step - loss: 3.3145 - accuracy: 0.0834 - val_loss: 3.2910 - val_accuracy: 0.0833\n",
      "Epoch 1179/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3127 - accuracy: 0.0839\n",
      "Epoch 1179: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3143 - accuracy: 0.0835 - val_loss: 3.2918 - val_accuracy: 0.0870\n",
      "Epoch 1180/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0825\n",
      "Epoch 1180: loss did not improve from 3.31425\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 3.3151 - accuracy: 0.0827 - val_loss: 3.2911 - val_accuracy: 0.0863\n",
      "Epoch 1181/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3127 - accuracy: 0.0835\n",
      "Epoch 1181: loss improved from 3.31425 to 3.31422, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3142 - accuracy: 0.0826 - val_loss: 3.2910 - val_accuracy: 0.0876\n",
      "Epoch 1182/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3177 - accuracy: 0.0815\n",
      "Epoch 1182: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3149 - accuracy: 0.0820 - val_loss: 3.2921 - val_accuracy: 0.0851\n",
      "Epoch 1183/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0830\n",
      "Epoch 1183: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.3147 - accuracy: 0.0838 - val_loss: 3.2905 - val_accuracy: 0.0857\n",
      "Epoch 1184/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0814\n",
      "Epoch 1184: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3149 - accuracy: 0.0815 - val_loss: 3.2909 - val_accuracy: 0.0839\n",
      "Epoch 1185/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3135 - accuracy: 0.0816\n",
      "Epoch 1185: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3147 - accuracy: 0.0812 - val_loss: 3.2907 - val_accuracy: 0.0833\n",
      "Epoch 1186/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3123 - accuracy: 0.0828\n",
      "Epoch 1186: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 3.3162 - accuracy: 0.0818 - val_loss: 3.2915 - val_accuracy: 0.0863\n",
      "Epoch 1187/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0842\n",
      "Epoch 1187: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3147 - accuracy: 0.0840 - val_loss: 3.2919 - val_accuracy: 0.0882\n",
      "Epoch 1188/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0835\n",
      "Epoch 1188: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3145 - accuracy: 0.0828 - val_loss: 3.2914 - val_accuracy: 0.0821\n",
      "Epoch 1189/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0847\n",
      "Epoch 1189: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3173 - accuracy: 0.0847 - val_loss: 3.2928 - val_accuracy: 0.0851\n",
      "Epoch 1190/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0837\n",
      "Epoch 1190: loss did not improve from 3.31422\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3146 - accuracy: 0.0835 - val_loss: 3.2927 - val_accuracy: 0.0839\n",
      "Epoch 1191/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0800\n",
      "Epoch 1191: loss improved from 3.31422 to 3.31420, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3142 - accuracy: 0.0800 - val_loss: 3.2943 - val_accuracy: 0.0827\n",
      "Epoch 1192/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3149 - accuracy: 0.0808\n",
      "Epoch 1192: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3152 - accuracy: 0.0804 - val_loss: 3.2937 - val_accuracy: 0.0839\n",
      "Epoch 1193/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3134 - accuracy: 0.0836\n",
      "Epoch 1193: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3148 - accuracy: 0.0838 - val_loss: 3.2912 - val_accuracy: 0.0870\n",
      "Epoch 1194/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3128 - accuracy: 0.0806\n",
      "Epoch 1194: loss improved from 3.31420 to 3.31420, saving model to best_model.h5\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3142 - accuracy: 0.0794 - val_loss: 3.2915 - val_accuracy: 0.0845\n",
      "Epoch 1195/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0812\n",
      "Epoch 1195: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3151 - accuracy: 0.0806 - val_loss: 3.2924 - val_accuracy: 0.0857\n",
      "Epoch 1196/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0829\n",
      "Epoch 1196: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3159 - accuracy: 0.0826 - val_loss: 3.2942 - val_accuracy: 0.0894\n",
      "Epoch 1197/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3147 - accuracy: 0.0834\n",
      "Epoch 1197: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3147 - accuracy: 0.0834 - val_loss: 3.2937 - val_accuracy: 0.0888\n",
      "Epoch 1198/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3146 - accuracy: 0.0824\n",
      "Epoch 1198: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3159 - accuracy: 0.0823 - val_loss: 3.2916 - val_accuracy: 0.0863\n",
      "Epoch 1199/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3122 - accuracy: 0.0825\n",
      "Epoch 1199: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3144 - accuracy: 0.0824 - val_loss: 3.2916 - val_accuracy: 0.0845\n",
      "Epoch 1200/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3206 - accuracy: 0.0818\n",
      "Epoch 1200: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3162 - accuracy: 0.0833 - val_loss: 3.2932 - val_accuracy: 0.0894\n",
      "Epoch 1201/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3142 - accuracy: 0.0811\n",
      "Epoch 1201: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3148 - accuracy: 0.0809 - val_loss: 3.2911 - val_accuracy: 0.0876\n",
      "Epoch 1202/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0809\n",
      "Epoch 1202: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3150 - accuracy: 0.0809 - val_loss: 3.2941 - val_accuracy: 0.0839\n",
      "Epoch 1203/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0835\n",
      "Epoch 1203: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3149 - accuracy: 0.0834 - val_loss: 3.2927 - val_accuracy: 0.0876\n",
      "Epoch 1204/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3151 - accuracy: 0.0824\n",
      "Epoch 1204: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3148 - accuracy: 0.0826 - val_loss: 3.2921 - val_accuracy: 0.0845\n",
      "Epoch 1205/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3137 - accuracy: 0.0826\n",
      "Epoch 1205: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3143 - accuracy: 0.0828 - val_loss: 3.2912 - val_accuracy: 0.0857\n",
      "Epoch 1206/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0806\n",
      "Epoch 1206: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3157 - accuracy: 0.0810 - val_loss: 3.2923 - val_accuracy: 0.0863\n",
      "Epoch 1207/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3138 - accuracy: 0.0816\n",
      "Epoch 1207: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3144 - accuracy: 0.0818 - val_loss: 3.2911 - val_accuracy: 0.0808\n",
      "Epoch 1208/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3134 - accuracy: 0.0816\n",
      "Epoch 1208: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3154 - accuracy: 0.0810 - val_loss: 3.2918 - val_accuracy: 0.0863\n",
      "Epoch 1209/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3119 - accuracy: 0.0799\n",
      "Epoch 1209: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3144 - accuracy: 0.0796 - val_loss: 3.2912 - val_accuracy: 0.0857\n",
      "Epoch 1210/5000\n",
      "235/256 [==========================>...] - ETA: 0s - loss: 3.3168 - accuracy: 0.0811\n",
      "Epoch 1210: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3148 - accuracy: 0.0810 - val_loss: 3.2915 - val_accuracy: 0.0845\n",
      "Epoch 1211/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3131 - accuracy: 0.0822\n",
      "Epoch 1211: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3149 - accuracy: 0.0818 - val_loss: 3.2930 - val_accuracy: 0.0857\n",
      "Epoch 1212/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3178 - accuracy: 0.0819\n",
      "Epoch 1212: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3146 - accuracy: 0.0821 - val_loss: 3.2920 - val_accuracy: 0.0857\n",
      "Epoch 1213/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3145 - accuracy: 0.0815\n",
      "Epoch 1213: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3173 - accuracy: 0.0813 - val_loss: 3.2932 - val_accuracy: 0.0888\n",
      "Epoch 1214/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0837\n",
      "Epoch 1214: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3151 - accuracy: 0.0834 - val_loss: 3.2924 - val_accuracy: 0.0882\n",
      "Epoch 1215/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3133 - accuracy: 0.0825\n",
      "Epoch 1215: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3145 - accuracy: 0.0822 - val_loss: 3.2927 - val_accuracy: 0.0851\n",
      "Epoch 1216/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3133 - accuracy: 0.0814\n",
      "Epoch 1216: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3150 - accuracy: 0.0813 - val_loss: 3.2943 - val_accuracy: 0.0833\n",
      "Epoch 1217/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3116 - accuracy: 0.0809\n",
      "Epoch 1217: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3147 - accuracy: 0.0813 - val_loss: 3.2921 - val_accuracy: 0.0857\n",
      "Epoch 1218/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0826\n",
      "Epoch 1218: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3160 - accuracy: 0.0824 - val_loss: 3.2925 - val_accuracy: 0.0863\n",
      "Epoch 1219/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0818\n",
      "Epoch 1219: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3152 - accuracy: 0.0828 - val_loss: 3.2927 - val_accuracy: 0.0888\n",
      "Epoch 1220/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3164 - accuracy: 0.0822\n",
      "Epoch 1220: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3152 - accuracy: 0.0817 - val_loss: 3.2920 - val_accuracy: 0.0863\n",
      "Epoch 1221/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0777\n",
      "Epoch 1221: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3147 - accuracy: 0.0785 - val_loss: 3.2920 - val_accuracy: 0.0851\n",
      "Epoch 1222/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3127 - accuracy: 0.0809\n",
      "Epoch 1222: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3156 - accuracy: 0.0817 - val_loss: 3.2920 - val_accuracy: 0.0863\n",
      "Epoch 1223/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0803\n",
      "Epoch 1223: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3148 - accuracy: 0.0801 - val_loss: 3.2906 - val_accuracy: 0.0900\n",
      "Epoch 1224/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0827\n",
      "Epoch 1224: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3146 - accuracy: 0.0828 - val_loss: 3.2920 - val_accuracy: 0.0839\n",
      "Epoch 1225/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3140 - accuracy: 0.0833\n",
      "Epoch 1225: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3144 - accuracy: 0.0828 - val_loss: 3.2917 - val_accuracy: 0.0870\n",
      "Epoch 1226/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3137 - accuracy: 0.0804\n",
      "Epoch 1226: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3143 - accuracy: 0.0805 - val_loss: 3.2915 - val_accuracy: 0.0845\n",
      "Epoch 1227/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3178 - accuracy: 0.0819\n",
      "Epoch 1227: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 14ms/step - loss: 3.3164 - accuracy: 0.0812 - val_loss: 3.2926 - val_accuracy: 0.0845\n",
      "Epoch 1228/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0810\n",
      "Epoch 1228: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3144 - accuracy: 0.0810 - val_loss: 3.2914 - val_accuracy: 0.0863\n",
      "Epoch 1229/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0834\n",
      "Epoch 1229: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3148 - accuracy: 0.0833 - val_loss: 3.2938 - val_accuracy: 0.0814\n",
      "Epoch 1230/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0810\n",
      "Epoch 1230: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3146 - accuracy: 0.0813 - val_loss: 3.2910 - val_accuracy: 0.0863\n",
      "Epoch 1231/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3135 - accuracy: 0.0824\n",
      "Epoch 1231: loss did not improve from 3.31420\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3145 - accuracy: 0.0827 - val_loss: 3.2918 - val_accuracy: 0.0821\n",
      "Epoch 1232/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0821\n",
      "Epoch 1232: loss improved from 3.31420 to 3.31396, saving model to best_model.h5\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.3140 - accuracy: 0.0822 - val_loss: 3.2920 - val_accuracy: 0.0845\n",
      "Epoch 1233/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0804\n",
      "Epoch 1233: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3144 - accuracy: 0.0812 - val_loss: 3.2920 - val_accuracy: 0.0876\n",
      "Epoch 1234/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0822\n",
      "Epoch 1234: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3142 - accuracy: 0.0831 - val_loss: 3.2916 - val_accuracy: 0.0888\n",
      "Epoch 1235/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3188 - accuracy: 0.0806\n",
      "Epoch 1235: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3152 - accuracy: 0.0816 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 1236/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3141 - accuracy: 0.0843\n",
      "Epoch 1236: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3141 - accuracy: 0.0843 - val_loss: 3.2916 - val_accuracy: 0.0821\n",
      "Epoch 1237/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3191 - accuracy: 0.0806\n",
      "Epoch 1237: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3148 - accuracy: 0.0812 - val_loss: 3.2911 - val_accuracy: 0.0857\n",
      "Epoch 1238/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3129 - accuracy: 0.0842\n",
      "Epoch 1238: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3147 - accuracy: 0.0829 - val_loss: 3.2926 - val_accuracy: 0.0845\n",
      "Epoch 1239/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3158 - accuracy: 0.0807\n",
      "Epoch 1239: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3157 - accuracy: 0.0810 - val_loss: 3.2938 - val_accuracy: 0.0857\n",
      "Epoch 1240/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0855\n",
      "Epoch 1240: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3145 - accuracy: 0.0856 - val_loss: 3.2914 - val_accuracy: 0.0833\n",
      "Epoch 1241/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0818\n",
      "Epoch 1241: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3150 - accuracy: 0.0821 - val_loss: 3.2914 - val_accuracy: 0.0857\n",
      "Epoch 1242/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0800\n",
      "Epoch 1242: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3150 - accuracy: 0.0804 - val_loss: 3.2920 - val_accuracy: 0.0882\n",
      "Epoch 1243/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0833\n",
      "Epoch 1243: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3143 - accuracy: 0.0834 - val_loss: 3.2919 - val_accuracy: 0.0876\n",
      "Epoch 1244/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3173 - accuracy: 0.0822\n",
      "Epoch 1244: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3173 - accuracy: 0.0826 - val_loss: 3.2931 - val_accuracy: 0.0876\n",
      "Epoch 1245/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3138 - accuracy: 0.0831\n",
      "Epoch 1245: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3148 - accuracy: 0.0827 - val_loss: 3.2918 - val_accuracy: 0.0857\n",
      "Epoch 1246/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0832\n",
      "Epoch 1246: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3150 - accuracy: 0.0831 - val_loss: 3.2920 - val_accuracy: 0.0870\n",
      "Epoch 1247/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0801\n",
      "Epoch 1247: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3145 - accuracy: 0.0807 - val_loss: 3.2913 - val_accuracy: 0.0845\n",
      "Epoch 1248/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3115 - accuracy: 0.0842\n",
      "Epoch 1248: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3149 - accuracy: 0.0826 - val_loss: 3.2916 - val_accuracy: 0.0876\n",
      "Epoch 1249/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0845\n",
      "Epoch 1249: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3142 - accuracy: 0.0844 - val_loss: 3.2911 - val_accuracy: 0.0870\n",
      "Epoch 1250/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3123 - accuracy: 0.0799\n",
      "Epoch 1250: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3142 - accuracy: 0.0801 - val_loss: 3.2916 - val_accuracy: 0.0882\n",
      "Epoch 1251/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0822\n",
      "Epoch 1251: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3153 - accuracy: 0.0826 - val_loss: 3.2920 - val_accuracy: 0.0827\n",
      "Epoch 1252/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0842\n",
      "Epoch 1252: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3159 - accuracy: 0.0840 - val_loss: 3.2935 - val_accuracy: 0.0876\n",
      "Epoch 1253/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0825\n",
      "Epoch 1253: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3147 - accuracy: 0.0823 - val_loss: 3.2915 - val_accuracy: 0.0857\n",
      "Epoch 1254/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0806\n",
      "Epoch 1254: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3148 - accuracy: 0.0812 - val_loss: 3.2925 - val_accuracy: 0.0888\n",
      "Epoch 1255/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0812\n",
      "Epoch 1255: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3142 - accuracy: 0.0813 - val_loss: 3.2908 - val_accuracy: 0.0876\n",
      "Epoch 1256/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0843\n",
      "Epoch 1256: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3147 - accuracy: 0.0835 - val_loss: 3.2917 - val_accuracy: 0.0876\n",
      "Epoch 1257/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0814\n",
      "Epoch 1257: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3152 - accuracy: 0.0815 - val_loss: 3.2922 - val_accuracy: 0.0839\n",
      "Epoch 1258/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0818\n",
      "Epoch 1258: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3148 - accuracy: 0.0816 - val_loss: 3.2951 - val_accuracy: 0.0900\n",
      "Epoch 1259/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0827\n",
      "Epoch 1259: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3150 - accuracy: 0.0821 - val_loss: 3.2921 - val_accuracy: 0.0845\n",
      "Epoch 1260/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0803\n",
      "Epoch 1260: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3156 - accuracy: 0.0807 - val_loss: 3.2922 - val_accuracy: 0.0876\n",
      "Epoch 1261/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0826\n",
      "Epoch 1261: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3146 - accuracy: 0.0826 - val_loss: 3.2913 - val_accuracy: 0.0888\n",
      "Epoch 1262/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3132 - accuracy: 0.0829\n",
      "Epoch 1262: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3151 - accuracy: 0.0821 - val_loss: 3.2918 - val_accuracy: 0.0839\n",
      "Epoch 1263/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3181 - accuracy: 0.0819\n",
      "Epoch 1263: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3167 - accuracy: 0.0824 - val_loss: 3.2944 - val_accuracy: 0.0870\n",
      "Epoch 1264/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0805\n",
      "Epoch 1264: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3148 - accuracy: 0.0802 - val_loss: 3.2916 - val_accuracy: 0.0833\n",
      "Epoch 1265/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0834\n",
      "Epoch 1265: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3140 - accuracy: 0.0833 - val_loss: 3.2911 - val_accuracy: 0.0827\n",
      "Epoch 1266/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0808\n",
      "Epoch 1266: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3147 - accuracy: 0.0811 - val_loss: 3.2922 - val_accuracy: 0.0857\n",
      "Epoch 1267/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0814\n",
      "Epoch 1267: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3142 - accuracy: 0.0813 - val_loss: 3.2933 - val_accuracy: 0.0857\n",
      "Epoch 1268/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3117 - accuracy: 0.0823\n",
      "Epoch 1268: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3142 - accuracy: 0.0823 - val_loss: 3.2928 - val_accuracy: 0.0845\n",
      "Epoch 1269/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3159 - accuracy: 0.0831\n",
      "Epoch 1269: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3143 - accuracy: 0.0821 - val_loss: 3.2937 - val_accuracy: 0.0839\n",
      "Epoch 1270/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0808\n",
      "Epoch 1270: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0809 - val_loss: 3.3261 - val_accuracy: 0.0784\n",
      "Epoch 1271/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0819\n",
      "Epoch 1271: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3152 - accuracy: 0.0824 - val_loss: 3.2914 - val_accuracy: 0.0870\n",
      "Epoch 1272/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0814\n",
      "Epoch 1272: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3140 - accuracy: 0.0818 - val_loss: 3.2910 - val_accuracy: 0.0876\n",
      "Epoch 1273/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0840\n",
      "Epoch 1273: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3141 - accuracy: 0.0842 - val_loss: 3.2927 - val_accuracy: 0.0833\n",
      "Epoch 1274/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0809\n",
      "Epoch 1274: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3147 - accuracy: 0.0820 - val_loss: 3.2916 - val_accuracy: 0.0839\n",
      "Epoch 1275/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3172 - accuracy: 0.0829\n",
      "Epoch 1275: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 12s 47ms/step - loss: 3.3141 - accuracy: 0.0824 - val_loss: 3.2921 - val_accuracy: 0.0833\n",
      "Epoch 1276/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3111 - accuracy: 0.0824\n",
      "Epoch 1276: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3142 - accuracy: 0.0820 - val_loss: 3.2913 - val_accuracy: 0.0863\n",
      "Epoch 1277/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0818\n",
      "Epoch 1277: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3147 - accuracy: 0.0822 - val_loss: 3.2927 - val_accuracy: 0.0839\n",
      "Epoch 1278/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0810\n",
      "Epoch 1278: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3150 - accuracy: 0.0810 - val_loss: 3.2917 - val_accuracy: 0.0851\n",
      "Epoch 1279/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3154 - accuracy: 0.0812\n",
      "Epoch 1279: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 3.3148 - accuracy: 0.0810 - val_loss: 3.2932 - val_accuracy: 0.0845\n",
      "Epoch 1280/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0828\n",
      "Epoch 1280: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3149 - accuracy: 0.0827 - val_loss: 3.2916 - val_accuracy: 0.0808\n",
      "Epoch 1281/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3165 - accuracy: 0.0832\n",
      "Epoch 1281: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3165 - accuracy: 0.0832 - val_loss: 3.2928 - val_accuracy: 0.0870\n",
      "Epoch 1282/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0833\n",
      "Epoch 1282: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3147 - accuracy: 0.0827 - val_loss: 3.2915 - val_accuracy: 0.0833\n",
      "Epoch 1283/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0803\n",
      "Epoch 1283: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3157 - accuracy: 0.0801 - val_loss: 3.2936 - val_accuracy: 0.0882\n",
      "Epoch 1284/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0824\n",
      "Epoch 1284: loss did not improve from 3.31396\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3150 - accuracy: 0.0827 - val_loss: 3.2923 - val_accuracy: 0.0876\n",
      "Epoch 1285/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3132 - accuracy: 0.0820\n",
      "Epoch 1285: loss improved from 3.31396 to 3.31386, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0823 - val_loss: 3.2923 - val_accuracy: 0.0863\n",
      "Epoch 1286/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3123 - accuracy: 0.0802\n",
      "Epoch 1286: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3141 - accuracy: 0.0799 - val_loss: 3.2911 - val_accuracy: 0.0845\n",
      "Epoch 1287/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3143 - accuracy: 0.0816\n",
      "Epoch 1287: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3143 - accuracy: 0.0816 - val_loss: 3.2912 - val_accuracy: 0.0882\n",
      "Epoch 1288/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0818\n",
      "Epoch 1288: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3145 - accuracy: 0.0818 - val_loss: 3.2919 - val_accuracy: 0.0814\n",
      "Epoch 1289/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3133 - accuracy: 0.0842\n",
      "Epoch 1289: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3144 - accuracy: 0.0840 - val_loss: 3.2919 - val_accuracy: 0.0863\n",
      "Epoch 1290/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3139 - accuracy: 0.0828\n",
      "Epoch 1290: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0826 - val_loss: 3.2923 - val_accuracy: 0.0814\n",
      "Epoch 1291/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3124 - accuracy: 0.0806\n",
      "Epoch 1291: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3148 - accuracy: 0.0801 - val_loss: 3.2930 - val_accuracy: 0.0845\n",
      "Epoch 1292/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3110 - accuracy: 0.0819\n",
      "Epoch 1292: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3146 - accuracy: 0.0823 - val_loss: 3.2908 - val_accuracy: 0.0851\n",
      "Epoch 1293/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3141 - accuracy: 0.0833\n",
      "Epoch 1293: loss did not improve from 3.31386\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3151 - accuracy: 0.0826 - val_loss: 3.2916 - val_accuracy: 0.0833\n",
      "Epoch 1294/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3155 - accuracy: 0.0819\n",
      "Epoch 1294: loss improved from 3.31386 to 3.31383, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3138 - accuracy: 0.0817 - val_loss: 3.2909 - val_accuracy: 0.0851\n",
      "Epoch 1295/5000\n",
      "234/256 [==========================>...] - ETA: 0s - loss: 3.3135 - accuracy: 0.0851\n",
      "Epoch 1295: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0827 - val_loss: 3.2918 - val_accuracy: 0.0845\n",
      "Epoch 1296/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3125 - accuracy: 0.0826\n",
      "Epoch 1296: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3161 - accuracy: 0.0809 - val_loss: 3.2927 - val_accuracy: 0.0870\n",
      "Epoch 1297/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0812\n",
      "Epoch 1297: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3166 - accuracy: 0.0820 - val_loss: 3.2958 - val_accuracy: 0.0870\n",
      "Epoch 1298/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3169 - accuracy: 0.0844\n",
      "Epoch 1298: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3155 - accuracy: 0.0837 - val_loss: 3.2921 - val_accuracy: 0.0900\n",
      "Epoch 1299/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3135 - accuracy: 0.0817\n",
      "Epoch 1299: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3145 - accuracy: 0.0818 - val_loss: 3.2924 - val_accuracy: 0.0851\n",
      "Epoch 1300/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0828\n",
      "Epoch 1300: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3150 - accuracy: 0.0832 - val_loss: 3.2952 - val_accuracy: 0.0863\n",
      "Epoch 1301/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3161 - accuracy: 0.0827\n",
      "Epoch 1301: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3148 - accuracy: 0.0827 - val_loss: 3.2923 - val_accuracy: 0.0845\n",
      "Epoch 1302/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3140 - accuracy: 0.0801\n",
      "Epoch 1302: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3139 - accuracy: 0.0801 - val_loss: 3.2915 - val_accuracy: 0.0876\n",
      "Epoch 1303/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3171 - accuracy: 0.0825\n",
      "Epoch 1303: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3141 - accuracy: 0.0824 - val_loss: 3.2910 - val_accuracy: 0.0882\n",
      "Epoch 1304/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0813\n",
      "Epoch 1304: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3147 - accuracy: 0.0810 - val_loss: 3.2920 - val_accuracy: 0.0839\n",
      "Epoch 1305/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0808\n",
      "Epoch 1305: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3152 - accuracy: 0.0811 - val_loss: 3.2922 - val_accuracy: 0.0851\n",
      "Epoch 1306/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3155 - accuracy: 0.0848\n",
      "Epoch 1306: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3155 - accuracy: 0.0848 - val_loss: 3.2948 - val_accuracy: 0.0870\n",
      "Epoch 1307/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3135 - accuracy: 0.0799\n",
      "Epoch 1307: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3145 - accuracy: 0.0799 - val_loss: 3.2917 - val_accuracy: 0.0845\n",
      "Epoch 1308/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3137 - accuracy: 0.0813\n",
      "Epoch 1308: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3145 - accuracy: 0.0815 - val_loss: 3.2914 - val_accuracy: 0.0863\n",
      "Epoch 1309/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3129 - accuracy: 0.0819\n",
      "Epoch 1309: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3142 - accuracy: 0.0818 - val_loss: 3.2918 - val_accuracy: 0.0833\n",
      "Epoch 1310/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3178 - accuracy: 0.0842\n",
      "Epoch 1310: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3166 - accuracy: 0.0845 - val_loss: 3.2923 - val_accuracy: 0.0833\n",
      "Epoch 1311/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3150 - accuracy: 0.0805\n",
      "Epoch 1311: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3143 - accuracy: 0.0813 - val_loss: 3.2915 - val_accuracy: 0.0814\n",
      "Epoch 1312/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0840\n",
      "Epoch 1312: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3142 - accuracy: 0.0832 - val_loss: 3.2909 - val_accuracy: 0.0821\n",
      "Epoch 1313/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3142 - accuracy: 0.0809\n",
      "Epoch 1313: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3142 - accuracy: 0.0806 - val_loss: 3.2917 - val_accuracy: 0.0821\n",
      "Epoch 1314/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3119 - accuracy: 0.0796\n",
      "Epoch 1314: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0800 - val_loss: 3.2926 - val_accuracy: 0.0845\n",
      "Epoch 1315/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3137 - accuracy: 0.0813\n",
      "Epoch 1315: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3149 - accuracy: 0.0818 - val_loss: 3.2927 - val_accuracy: 0.0833\n",
      "Epoch 1316/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3149 - accuracy: 0.0830\n",
      "Epoch 1316: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3144 - accuracy: 0.0828 - val_loss: 3.2915 - val_accuracy: 0.0845\n",
      "Epoch 1317/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3142 - accuracy: 0.0817\n",
      "Epoch 1317: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3161 - accuracy: 0.0820 - val_loss: 3.2922 - val_accuracy: 0.0851\n",
      "Epoch 1318/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0821\n",
      "Epoch 1318: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 3.3149 - accuracy: 0.0820 - val_loss: 3.2937 - val_accuracy: 0.0857\n",
      "Epoch 1319/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3165 - accuracy: 0.0802\n",
      "Epoch 1319: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0807 - val_loss: 3.2916 - val_accuracy: 0.0821\n",
      "Epoch 1320/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0821\n",
      "Epoch 1320: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3151 - accuracy: 0.0818 - val_loss: 3.2926 - val_accuracy: 0.0833\n",
      "Epoch 1321/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0816\n",
      "Epoch 1321: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3144 - accuracy: 0.0816 - val_loss: 3.2925 - val_accuracy: 0.0839\n",
      "Epoch 1322/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3145 - accuracy: 0.0837\n",
      "Epoch 1322: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3164 - accuracy: 0.0826 - val_loss: 3.2955 - val_accuracy: 0.0863\n",
      "Epoch 1323/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0810\n",
      "Epoch 1323: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3158 - accuracy: 0.0811 - val_loss: 3.2922 - val_accuracy: 0.0851\n",
      "Epoch 1324/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3099 - accuracy: 0.0846\n",
      "Epoch 1324: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3143 - accuracy: 0.0847 - val_loss: 3.2915 - val_accuracy: 0.0888\n",
      "Epoch 1325/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0841\n",
      "Epoch 1325: loss did not improve from 3.31383\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0838 - val_loss: 3.2932 - val_accuracy: 0.0870\n",
      "Epoch 1326/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3121 - accuracy: 0.0835\n",
      "Epoch 1326: loss improved from 3.31383 to 3.31378, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3138 - accuracy: 0.0831 - val_loss: 3.3873 - val_accuracy: 0.0821\n",
      "Epoch 1327/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3210 - accuracy: 0.0835\n",
      "Epoch 1327: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3212 - accuracy: 0.0832 - val_loss: 3.2929 - val_accuracy: 0.0821\n",
      "Epoch 1328/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0823\n",
      "Epoch 1328: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0822 - val_loss: 3.2923 - val_accuracy: 0.0814\n",
      "Epoch 1329/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3142 - accuracy: 0.0812\n",
      "Epoch 1329: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3145 - accuracy: 0.0815 - val_loss: 3.2940 - val_accuracy: 0.0821\n",
      "Epoch 1330/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3113 - accuracy: 0.0846\n",
      "Epoch 1330: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3144 - accuracy: 0.0828 - val_loss: 3.2925 - val_accuracy: 0.0851\n",
      "Epoch 1331/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.0824\n",
      "Epoch 1331: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3164 - accuracy: 0.0826 - val_loss: 3.2919 - val_accuracy: 0.0876\n",
      "Epoch 1332/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0834\n",
      "Epoch 1332: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3140 - accuracy: 0.0834 - val_loss: 3.2937 - val_accuracy: 0.0882\n",
      "Epoch 1333/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3116 - accuracy: 0.0835\n",
      "Epoch 1333: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0832 - val_loss: 3.2916 - val_accuracy: 0.0851\n",
      "Epoch 1334/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0842\n",
      "Epoch 1334: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3156 - accuracy: 0.0842 - val_loss: 3.2935 - val_accuracy: 0.0827\n",
      "Epoch 1335/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0835\n",
      "Epoch 1335: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3151 - accuracy: 0.0835 - val_loss: 3.2920 - val_accuracy: 0.0857\n",
      "Epoch 1336/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0834\n",
      "Epoch 1336: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3147 - accuracy: 0.0837 - val_loss: 3.2932 - val_accuracy: 0.0796\n",
      "Epoch 1337/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3142 - accuracy: 0.0833\n",
      "Epoch 1337: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3141 - accuracy: 0.0833 - val_loss: 3.2916 - val_accuracy: 0.0876\n",
      "Epoch 1338/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3136 - accuracy: 0.0821\n",
      "Epoch 1338: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3143 - accuracy: 0.0820 - val_loss: 3.2920 - val_accuracy: 0.0857\n",
      "Epoch 1339/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0823\n",
      "Epoch 1339: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0828 - val_loss: 3.2926 - val_accuracy: 0.0833\n",
      "Epoch 1340/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3142 - accuracy: 0.0824\n",
      "Epoch 1340: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3142 - accuracy: 0.0824 - val_loss: 3.2920 - val_accuracy: 0.0833\n",
      "Epoch 1341/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3130 - accuracy: 0.0827\n",
      "Epoch 1341: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3142 - accuracy: 0.0821 - val_loss: 3.2914 - val_accuracy: 0.0857\n",
      "Epoch 1342/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0823\n",
      "Epoch 1342: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3154 - accuracy: 0.0823 - val_loss: 3.2931 - val_accuracy: 0.0833\n",
      "Epoch 1343/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3128 - accuracy: 0.0843\n",
      "Epoch 1343: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3145 - accuracy: 0.0847 - val_loss: 3.2932 - val_accuracy: 0.0796\n",
      "Epoch 1344/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3132 - accuracy: 0.0824\n",
      "Epoch 1344: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3144 - accuracy: 0.0818 - val_loss: 3.2917 - val_accuracy: 0.0870\n",
      "Epoch 1345/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3150 - accuracy: 0.0821\n",
      "Epoch 1345: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3140 - accuracy: 0.0824 - val_loss: 3.2911 - val_accuracy: 0.0876\n",
      "Epoch 1346/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3190 - accuracy: 0.0832\n",
      "Epoch 1346: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3175 - accuracy: 0.0828 - val_loss: 3.2944 - val_accuracy: 0.0863\n",
      "Epoch 1347/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3126 - accuracy: 0.0839\n",
      "Epoch 1347: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3153 - accuracy: 0.0829 - val_loss: 3.2924 - val_accuracy: 0.0827\n",
      "Epoch 1348/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3130 - accuracy: 0.0822\n",
      "Epoch 1348: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3142 - accuracy: 0.0821 - val_loss: 3.2923 - val_accuracy: 0.0882\n",
      "Epoch 1349/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3175 - accuracy: 0.0822\n",
      "Epoch 1349: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3151 - accuracy: 0.0817 - val_loss: 3.2937 - val_accuracy: 0.0845\n",
      "Epoch 1350/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3146 - accuracy: 0.0809\n",
      "Epoch 1350: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3146 - accuracy: 0.0824 - val_loss: 3.2918 - val_accuracy: 0.0876\n",
      "Epoch 1351/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3157 - accuracy: 0.0832\n",
      "Epoch 1351: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3161 - accuracy: 0.0834 - val_loss: 3.2931 - val_accuracy: 0.0919\n",
      "Epoch 1352/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3150 - accuracy: 0.0824\n",
      "Epoch 1352: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0826 - val_loss: 3.2930 - val_accuracy: 0.0814\n",
      "Epoch 1353/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3136 - accuracy: 0.0809\n",
      "Epoch 1353: loss did not improve from 3.31378\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0809 - val_loss: 3.2918 - val_accuracy: 0.0857\n",
      "Epoch 1354/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0804\n",
      "Epoch 1354: loss improved from 3.31378 to 3.31367, saving model to best_model.h5\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3137 - accuracy: 0.0807 - val_loss: 3.3017 - val_accuracy: 0.0888\n",
      "Epoch 1355/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3124 - accuracy: 0.0808\n",
      "Epoch 1355: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0812 - val_loss: 3.2953 - val_accuracy: 0.0851\n",
      "Epoch 1356/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0817\n",
      "Epoch 1356: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3149 - accuracy: 0.0817 - val_loss: 3.2915 - val_accuracy: 0.0821\n",
      "Epoch 1357/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0824\n",
      "Epoch 1357: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3140 - accuracy: 0.0824 - val_loss: 3.2913 - val_accuracy: 0.0839\n",
      "Epoch 1358/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3165 - accuracy: 0.0846\n",
      "Epoch 1358: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3148 - accuracy: 0.0843 - val_loss: 3.2930 - val_accuracy: 0.0839\n",
      "Epoch 1359/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3175 - accuracy: 0.0818\n",
      "Epoch 1359: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3180 - accuracy: 0.0815 - val_loss: 3.2958 - val_accuracy: 0.0845\n",
      "Epoch 1360/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0837\n",
      "Epoch 1360: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3148 - accuracy: 0.0839 - val_loss: 3.2913 - val_accuracy: 0.0900\n",
      "Epoch 1361/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3143 - accuracy: 0.0817\n",
      "Epoch 1361: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3141 - accuracy: 0.0833 - val_loss: 3.2915 - val_accuracy: 0.0870\n",
      "Epoch 1362/5000\n",
      "234/256 [==========================>...] - ETA: 0s - loss: 3.3166 - accuracy: 0.0809\n",
      "Epoch 1362: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3144 - accuracy: 0.0826 - val_loss: 3.2913 - val_accuracy: 0.0851\n",
      "Epoch 1363/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0821\n",
      "Epoch 1363: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3152 - accuracy: 0.0826 - val_loss: 3.2914 - val_accuracy: 0.0851\n",
      "Epoch 1364/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3144 - accuracy: 0.0815\n",
      "Epoch 1364: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3147 - accuracy: 0.0807 - val_loss: 3.2925 - val_accuracy: 0.0876\n",
      "Epoch 1365/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3124 - accuracy: 0.0815\n",
      "Epoch 1365: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3147 - accuracy: 0.0815 - val_loss: 3.2942 - val_accuracy: 0.0863\n",
      "Epoch 1366/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3164 - accuracy: 0.0818\n",
      "Epoch 1366: loss did not improve from 3.31367\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3148 - accuracy: 0.0820 - val_loss: 3.2923 - val_accuracy: 0.0876\n",
      "Epoch 1367/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3131 - accuracy: 0.0817\n",
      "Epoch 1367: loss improved from 3.31367 to 3.31361, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3136 - accuracy: 0.0815 - val_loss: 3.2922 - val_accuracy: 0.0870\n",
      "Epoch 1368/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3159 - accuracy: 0.0804\n",
      "Epoch 1368: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3154 - accuracy: 0.0801 - val_loss: 3.2946 - val_accuracy: 0.0876\n",
      "Epoch 1369/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0830\n",
      "Epoch 1369: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0828 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 1370/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0821\n",
      "Epoch 1370: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0817 - val_loss: 3.2926 - val_accuracy: 0.0845\n",
      "Epoch 1371/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3172 - accuracy: 0.0796\n",
      "Epoch 1371: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3140 - accuracy: 0.0802 - val_loss: 3.2924 - val_accuracy: 0.0802\n",
      "Epoch 1372/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0820\n",
      "Epoch 1372: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3147 - accuracy: 0.0822 - val_loss: 3.2946 - val_accuracy: 0.0876\n",
      "Epoch 1373/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3174 - accuracy: 0.0818\n",
      "Epoch 1373: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3151 - accuracy: 0.0818 - val_loss: 3.2910 - val_accuracy: 0.0839\n",
      "Epoch 1374/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0821\n",
      "Epoch 1374: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3151 - accuracy: 0.0824 - val_loss: 3.2918 - val_accuracy: 0.0833\n",
      "Epoch 1375/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3123 - accuracy: 0.0818\n",
      "Epoch 1375: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3153 - accuracy: 0.0815 - val_loss: 3.2943 - val_accuracy: 0.0857\n",
      "Epoch 1376/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0817\n",
      "Epoch 1376: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3145 - accuracy: 0.0815 - val_loss: 3.2912 - val_accuracy: 0.0876\n",
      "Epoch 1377/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3137 - accuracy: 0.0837\n",
      "Epoch 1377: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3142 - accuracy: 0.0837 - val_loss: 3.2923 - val_accuracy: 0.0851\n",
      "Epoch 1378/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0840\n",
      "Epoch 1378: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3138 - accuracy: 0.0840 - val_loss: 3.2911 - val_accuracy: 0.0857\n",
      "Epoch 1379/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3161 - accuracy: 0.0810\n",
      "Epoch 1379: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3149 - accuracy: 0.0813 - val_loss: 3.2918 - val_accuracy: 0.0882\n",
      "Epoch 1380/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3171 - accuracy: 0.0810\n",
      "Epoch 1380: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0807 - val_loss: 3.2917 - val_accuracy: 0.0845\n",
      "Epoch 1381/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3138 - accuracy: 0.0817\n",
      "Epoch 1381: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3138 - accuracy: 0.0817 - val_loss: 3.2919 - val_accuracy: 0.0876\n",
      "Epoch 1382/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3193 - accuracy: 0.0808\n",
      "Epoch 1382: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0811 - val_loss: 3.2910 - val_accuracy: 0.0821\n",
      "Epoch 1383/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3142 - accuracy: 0.0820\n",
      "Epoch 1383: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3155 - accuracy: 0.0813 - val_loss: 3.2928 - val_accuracy: 0.0894\n",
      "Epoch 1384/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0831\n",
      "Epoch 1384: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3147 - accuracy: 0.0828 - val_loss: 3.2914 - val_accuracy: 0.0845\n",
      "Epoch 1385/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3163 - accuracy: 0.0822\n",
      "Epoch 1385: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3163 - accuracy: 0.0822 - val_loss: 3.2917 - val_accuracy: 0.0845\n",
      "Epoch 1386/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3177 - accuracy: 0.0827\n",
      "Epoch 1386: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3165 - accuracy: 0.0824 - val_loss: 3.2943 - val_accuracy: 0.0888\n",
      "Epoch 1387/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3161 - accuracy: 0.0819\n",
      "Epoch 1387: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3152 - accuracy: 0.0818 - val_loss: 3.2922 - val_accuracy: 0.0870\n",
      "Epoch 1388/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3133 - accuracy: 0.0829\n",
      "Epoch 1388: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0827 - val_loss: 3.2931 - val_accuracy: 0.0821\n",
      "Epoch 1389/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3144 - accuracy: 0.0821\n",
      "Epoch 1389: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3157 - accuracy: 0.0815 - val_loss: 3.2922 - val_accuracy: 0.0857\n",
      "Epoch 1390/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0818\n",
      "Epoch 1390: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3146 - accuracy: 0.0820 - val_loss: 3.2910 - val_accuracy: 0.0827\n",
      "Epoch 1391/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3127 - accuracy: 0.0819\n",
      "Epoch 1391: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3140 - accuracy: 0.0816 - val_loss: 3.2916 - val_accuracy: 0.0870\n",
      "Epoch 1392/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3121 - accuracy: 0.0810\n",
      "Epoch 1392: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3153 - accuracy: 0.0810 - val_loss: 3.2925 - val_accuracy: 0.0851\n",
      "Epoch 1393/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0824\n",
      "Epoch 1393: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3141 - accuracy: 0.0823 - val_loss: 3.2917 - val_accuracy: 0.0808\n",
      "Epoch 1394/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3163 - accuracy: 0.0833\n",
      "Epoch 1394: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3156 - accuracy: 0.0832 - val_loss: 3.2942 - val_accuracy: 0.0857\n",
      "Epoch 1395/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0830\n",
      "Epoch 1395: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3147 - accuracy: 0.0826 - val_loss: 3.2916 - val_accuracy: 0.0845\n",
      "Epoch 1396/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0819\n",
      "Epoch 1396: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3137 - accuracy: 0.0818 - val_loss: 3.2917 - val_accuracy: 0.0857\n",
      "Epoch 1397/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0802\n",
      "Epoch 1397: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3147 - accuracy: 0.0800 - val_loss: 3.2928 - val_accuracy: 0.0857\n",
      "Epoch 1398/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0803\n",
      "Epoch 1398: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3149 - accuracy: 0.0804 - val_loss: 3.2932 - val_accuracy: 0.0845\n",
      "Epoch 1399/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3140 - accuracy: 0.0831\n",
      "Epoch 1399: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3141 - accuracy: 0.0828 - val_loss: 3.2920 - val_accuracy: 0.0876\n",
      "Epoch 1400/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3176 - accuracy: 0.0812\n",
      "Epoch 1400: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3147 - accuracy: 0.0812 - val_loss: 3.2924 - val_accuracy: 0.0882\n",
      "Epoch 1401/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3146 - accuracy: 0.0828\n",
      "Epoch 1401: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0818 - val_loss: 3.2945 - val_accuracy: 0.0870\n",
      "Epoch 1402/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0823\n",
      "Epoch 1402: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0829 - val_loss: 3.2921 - val_accuracy: 0.0876\n",
      "Epoch 1403/5000\n",
      "236/256 [==========================>...] - ETA: 0s - loss: 3.3169 - accuracy: 0.0825\n",
      "Epoch 1403: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3166 - accuracy: 0.0820 - val_loss: 3.2951 - val_accuracy: 0.0876\n",
      "Epoch 1404/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3136 - accuracy: 0.0822\n",
      "Epoch 1404: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3154 - accuracy: 0.0820 - val_loss: 3.2926 - val_accuracy: 0.0882\n",
      "Epoch 1405/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0832\n",
      "Epoch 1405: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 10ms/step - loss: 3.3155 - accuracy: 0.0832 - val_loss: 3.2927 - val_accuracy: 0.0845\n",
      "Epoch 1406/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0811\n",
      "Epoch 1406: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3149 - accuracy: 0.0811 - val_loss: 3.2923 - val_accuracy: 0.0857\n",
      "Epoch 1407/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3141 - accuracy: 0.0822\n",
      "Epoch 1407: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0821 - val_loss: 3.2926 - val_accuracy: 0.0870\n",
      "Epoch 1408/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0836\n",
      "Epoch 1408: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 3.3150 - accuracy: 0.0835 - val_loss: 3.2916 - val_accuracy: 0.0900\n",
      "Epoch 1409/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0833\n",
      "Epoch 1409: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 3.3160 - accuracy: 0.0832 - val_loss: 3.2941 - val_accuracy: 0.0857\n",
      "Epoch 1410/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3151 - accuracy: 0.0835\n",
      "Epoch 1410: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3145 - accuracy: 0.0833 - val_loss: 3.2913 - val_accuracy: 0.0888\n",
      "Epoch 1411/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3134 - accuracy: 0.0825\n",
      "Epoch 1411: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3143 - accuracy: 0.0826 - val_loss: 3.2928 - val_accuracy: 0.0863\n",
      "Epoch 1412/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0814\n",
      "Epoch 1412: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3149 - accuracy: 0.0813 - val_loss: 3.2924 - val_accuracy: 0.0857\n",
      "Epoch 1413/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3124 - accuracy: 0.0817\n",
      "Epoch 1413: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3142 - accuracy: 0.0816 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 1414/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3125 - accuracy: 0.0812\n",
      "Epoch 1414: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3138 - accuracy: 0.0824 - val_loss: 3.2933 - val_accuracy: 0.0851\n",
      "Epoch 1415/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0814\n",
      "Epoch 1415: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3144 - accuracy: 0.0810 - val_loss: 3.2939 - val_accuracy: 0.0857\n",
      "Epoch 1416/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0808\n",
      "Epoch 1416: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3149 - accuracy: 0.0812 - val_loss: 3.2937 - val_accuracy: 0.0845\n",
      "Epoch 1417/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3109 - accuracy: 0.0815\n",
      "Epoch 1417: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3146 - accuracy: 0.0806 - val_loss: 3.2919 - val_accuracy: 0.0839\n",
      "Epoch 1418/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0821\n",
      "Epoch 1418: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0820 - val_loss: 3.2911 - val_accuracy: 0.0827\n",
      "Epoch 1419/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3155 - accuracy: 0.0840\n",
      "Epoch 1419: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3148 - accuracy: 0.0839 - val_loss: 3.2918 - val_accuracy: 0.0857\n",
      "Epoch 1420/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3133 - accuracy: 0.0843\n",
      "Epoch 1420: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3142 - accuracy: 0.0842 - val_loss: 3.2927 - val_accuracy: 0.0882\n",
      "Epoch 1421/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0814\n",
      "Epoch 1421: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3136 - accuracy: 0.0818 - val_loss: 3.2911 - val_accuracy: 0.0845\n",
      "Epoch 1422/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3167 - accuracy: 0.0818\n",
      "Epoch 1422: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 3.3170 - accuracy: 0.0817 - val_loss: 3.2914 - val_accuracy: 0.0821\n",
      "Epoch 1423/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0808\n",
      "Epoch 1423: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3146 - accuracy: 0.0817 - val_loss: 3.2914 - val_accuracy: 0.0839\n",
      "Epoch 1424/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0814\n",
      "Epoch 1424: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3162 - accuracy: 0.0810 - val_loss: 3.2930 - val_accuracy: 0.0857\n",
      "Epoch 1425/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3122 - accuracy: 0.0801\n",
      "Epoch 1425: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3148 - accuracy: 0.0807 - val_loss: 3.2917 - val_accuracy: 0.0833\n",
      "Epoch 1426/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0823\n",
      "Epoch 1426: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3144 - accuracy: 0.0822 - val_loss: 3.2914 - val_accuracy: 0.0839\n",
      "Epoch 1427/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0813\n",
      "Epoch 1427: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0822 - val_loss: 3.2920 - val_accuracy: 0.0827\n",
      "Epoch 1428/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0824\n",
      "Epoch 1428: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3160 - accuracy: 0.0823 - val_loss: 3.2935 - val_accuracy: 0.0857\n",
      "Epoch 1429/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3151 - accuracy: 0.0837\n",
      "Epoch 1429: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0822 - val_loss: 3.2920 - val_accuracy: 0.0827\n",
      "Epoch 1430/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3189 - accuracy: 0.0834\n",
      "Epoch 1430: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3159 - accuracy: 0.0835 - val_loss: 3.2938 - val_accuracy: 0.0839\n",
      "Epoch 1431/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0827\n",
      "Epoch 1431: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3147 - accuracy: 0.0827 - val_loss: 3.2914 - val_accuracy: 0.0839\n",
      "Epoch 1432/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0815\n",
      "Epoch 1432: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3142 - accuracy: 0.0817 - val_loss: 3.2916 - val_accuracy: 0.0894\n",
      "Epoch 1433/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3135 - accuracy: 0.0833\n",
      "Epoch 1433: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3144 - accuracy: 0.0834 - val_loss: 3.2923 - val_accuracy: 0.0839\n",
      "Epoch 1434/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3135 - accuracy: 0.0817\n",
      "Epoch 1434: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3146 - accuracy: 0.0826 - val_loss: 3.2915 - val_accuracy: 0.0857\n",
      "Epoch 1435/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0826\n",
      "Epoch 1435: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3142 - accuracy: 0.0826 - val_loss: 3.2921 - val_accuracy: 0.0851\n",
      "Epoch 1436/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3160 - accuracy: 0.0803\n",
      "Epoch 1436: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 5ms/step - loss: 3.3145 - accuracy: 0.0811 - val_loss: 3.2921 - val_accuracy: 0.0870\n",
      "Epoch 1437/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3164 - accuracy: 0.0803\n",
      "Epoch 1437: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3148 - accuracy: 0.0807 - val_loss: 3.2941 - val_accuracy: 0.0845\n",
      "Epoch 1438/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3140 - accuracy: 0.0823\n",
      "Epoch 1438: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3155 - accuracy: 0.0826 - val_loss: 3.2922 - val_accuracy: 0.0821\n",
      "Epoch 1439/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0806\n",
      "Epoch 1439: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3138 - accuracy: 0.0811 - val_loss: 3.2920 - val_accuracy: 0.0851\n",
      "Epoch 1440/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0833\n",
      "Epoch 1440: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3155 - accuracy: 0.0831 - val_loss: 3.2921 - val_accuracy: 0.0851\n",
      "Epoch 1441/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3144 - accuracy: 0.0805\n",
      "Epoch 1441: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 9ms/step - loss: 3.3144 - accuracy: 0.0805 - val_loss: 3.2919 - val_accuracy: 0.0821\n",
      "Epoch 1442/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0836\n",
      "Epoch 1442: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3139 - accuracy: 0.0835 - val_loss: 3.2915 - val_accuracy: 0.0882\n",
      "Epoch 1443/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3109 - accuracy: 0.0828\n",
      "Epoch 1443: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0820 - val_loss: 3.2910 - val_accuracy: 0.0845\n",
      "Epoch 1444/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0828\n",
      "Epoch 1444: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3143 - accuracy: 0.0822 - val_loss: 3.2908 - val_accuracy: 0.0814\n",
      "Epoch 1445/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0794\n",
      "Epoch 1445: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3160 - accuracy: 0.0798 - val_loss: 3.2912 - val_accuracy: 0.0876\n",
      "Epoch 1446/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0818\n",
      "Epoch 1446: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3137 - accuracy: 0.0812 - val_loss: 3.2917 - val_accuracy: 0.0845\n",
      "Epoch 1447/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3161 - accuracy: 0.0840\n",
      "Epoch 1447: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3186 - accuracy: 0.0832 - val_loss: 3.2935 - val_accuracy: 0.0839\n",
      "Epoch 1448/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0828\n",
      "Epoch 1448: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3148 - accuracy: 0.0828 - val_loss: 3.2914 - val_accuracy: 0.0876\n",
      "Epoch 1449/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0820\n",
      "Epoch 1449: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3140 - accuracy: 0.0823 - val_loss: 3.2914 - val_accuracy: 0.0821\n",
      "Epoch 1450/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0799\n",
      "Epoch 1450: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0804 - val_loss: 3.2919 - val_accuracy: 0.0839\n",
      "Epoch 1451/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0822\n",
      "Epoch 1451: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0823 - val_loss: 3.2919 - val_accuracy: 0.0833\n",
      "Epoch 1452/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0818\n",
      "Epoch 1452: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3149 - accuracy: 0.0817 - val_loss: 3.2918 - val_accuracy: 0.0814\n",
      "Epoch 1453/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3093 - accuracy: 0.0818\n",
      "Epoch 1453: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3140 - accuracy: 0.0817 - val_loss: 3.2912 - val_accuracy: 0.0851\n",
      "Epoch 1454/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0801\n",
      "Epoch 1454: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3158 - accuracy: 0.0793 - val_loss: 3.2929 - val_accuracy: 0.0839\n",
      "Epoch 1455/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3133 - accuracy: 0.0820\n",
      "Epoch 1455: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3142 - accuracy: 0.0816 - val_loss: 3.2912 - val_accuracy: 0.0827\n",
      "Epoch 1456/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0828\n",
      "Epoch 1456: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3151 - accuracy: 0.0828 - val_loss: 3.2935 - val_accuracy: 0.0882\n",
      "Epoch 1457/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0798\n",
      "Epoch 1457: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3148 - accuracy: 0.0800 - val_loss: 3.2927 - val_accuracy: 0.0851\n",
      "Epoch 1458/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3155 - accuracy: 0.0833\n",
      "Epoch 1458: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3141 - accuracy: 0.0826 - val_loss: 3.2924 - val_accuracy: 0.0870\n",
      "Epoch 1459/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3131 - accuracy: 0.0840\n",
      "Epoch 1459: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0838 - val_loss: 3.2917 - val_accuracy: 0.0876\n",
      "Epoch 1460/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3133 - accuracy: 0.0819\n",
      "Epoch 1460: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0820 - val_loss: 3.2929 - val_accuracy: 0.0857\n",
      "Epoch 1461/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3125 - accuracy: 0.0823\n",
      "Epoch 1461: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3144 - accuracy: 0.0821 - val_loss: 3.2917 - val_accuracy: 0.0833\n",
      "Epoch 1462/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3158 - accuracy: 0.0810\n",
      "Epoch 1462: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3142 - accuracy: 0.0813 - val_loss: 3.2954 - val_accuracy: 0.0863\n",
      "Epoch 1463/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3130 - accuracy: 0.0845\n",
      "Epoch 1463: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 3.3144 - accuracy: 0.0835 - val_loss: 3.2915 - val_accuracy: 0.0821\n",
      "Epoch 1464/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3185 - accuracy: 0.0812\n",
      "Epoch 1464: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3162 - accuracy: 0.0822 - val_loss: 3.2925 - val_accuracy: 0.0857\n",
      "Epoch 1465/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0838\n",
      "Epoch 1465: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3147 - accuracy: 0.0838 - val_loss: 3.2938 - val_accuracy: 0.0851\n",
      "Epoch 1466/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0826\n",
      "Epoch 1466: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0827 - val_loss: 3.2939 - val_accuracy: 0.0876\n",
      "Epoch 1467/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3150 - accuracy: 0.0828\n",
      "Epoch 1467: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3146 - accuracy: 0.0824 - val_loss: 3.2928 - val_accuracy: 0.0839\n",
      "Epoch 1468/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3160 - accuracy: 0.0844\n",
      "Epoch 1468: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3160 - accuracy: 0.0844 - val_loss: 3.2927 - val_accuracy: 0.0857\n",
      "Epoch 1469/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.0832\n",
      "Epoch 1469: loss did not improve from 3.31361\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3140 - accuracy: 0.0832 - val_loss: 3.2916 - val_accuracy: 0.0821\n",
      "Epoch 1470/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3135 - accuracy: 0.0832\n",
      "Epoch 1470: loss improved from 3.31361 to 3.31360, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3136 - accuracy: 0.0833 - val_loss: 3.2932 - val_accuracy: 0.0845\n",
      "Epoch 1471/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0814\n",
      "Epoch 1471: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3143 - accuracy: 0.0813 - val_loss: 3.2929 - val_accuracy: 0.0827\n",
      "Epoch 1472/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0805\n",
      "Epoch 1472: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3146 - accuracy: 0.0809 - val_loss: 3.2936 - val_accuracy: 0.0814\n",
      "Epoch 1473/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3119 - accuracy: 0.0827\n",
      "Epoch 1473: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0828 - val_loss: 3.2912 - val_accuracy: 0.0821\n",
      "Epoch 1474/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0809\n",
      "Epoch 1474: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3155 - accuracy: 0.0805 - val_loss: 3.2989 - val_accuracy: 0.0857\n",
      "Epoch 1475/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3155 - accuracy: 0.0835\n",
      "Epoch 1475: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3156 - accuracy: 0.0834 - val_loss: 3.2919 - val_accuracy: 0.0863\n",
      "Epoch 1476/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3162 - accuracy: 0.0793\n",
      "Epoch 1476: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3142 - accuracy: 0.0809 - val_loss: 3.2917 - val_accuracy: 0.0851\n",
      "Epoch 1477/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0822\n",
      "Epoch 1477: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3140 - accuracy: 0.0816 - val_loss: 3.2921 - val_accuracy: 0.0821\n",
      "Epoch 1478/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0808\n",
      "Epoch 1478: loss did not improve from 3.31360\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3145 - accuracy: 0.0810 - val_loss: 3.2928 - val_accuracy: 0.0845\n",
      "Epoch 1479/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3143 - accuracy: 0.0820\n",
      "Epoch 1479: loss improved from 3.31360 to 3.31353, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3135 - accuracy: 0.0815 - val_loss: 3.2916 - val_accuracy: 0.0821\n",
      "Epoch 1480/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3149 - accuracy: 0.0837\n",
      "Epoch 1480: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3153 - accuracy: 0.0827 - val_loss: 3.2920 - val_accuracy: 0.0845\n",
      "Epoch 1481/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0809\n",
      "Epoch 1481: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3150 - accuracy: 0.0815 - val_loss: 3.2939 - val_accuracy: 0.0827\n",
      "Epoch 1482/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3078 - accuracy: 0.0816\n",
      "Epoch 1482: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3138 - accuracy: 0.0815 - val_loss: 3.2917 - val_accuracy: 0.0851\n",
      "Epoch 1483/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3106 - accuracy: 0.0814\n",
      "Epoch 1483: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3138 - accuracy: 0.0820 - val_loss: 3.2941 - val_accuracy: 0.0821\n",
      "Epoch 1484/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0826\n",
      "Epoch 1484: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3146 - accuracy: 0.0823 - val_loss: 3.2920 - val_accuracy: 0.0870\n",
      "Epoch 1485/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3149 - accuracy: 0.0802\n",
      "Epoch 1485: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3141 - accuracy: 0.0804 - val_loss: 3.2915 - val_accuracy: 0.0821\n",
      "Epoch 1486/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3155 - accuracy: 0.0830\n",
      "Epoch 1486: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3162 - accuracy: 0.0824 - val_loss: 3.2923 - val_accuracy: 0.0870\n",
      "Epoch 1487/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3137 - accuracy: 0.0829\n",
      "Epoch 1487: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3142 - accuracy: 0.0821 - val_loss: 3.2929 - val_accuracy: 0.0839\n",
      "Epoch 1488/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3135 - accuracy: 0.0818\n",
      "Epoch 1488: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0815 - val_loss: 3.2913 - val_accuracy: 0.0857\n",
      "Epoch 1489/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0817\n",
      "Epoch 1489: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3149 - accuracy: 0.0818 - val_loss: 3.2921 - val_accuracy: 0.0900\n",
      "Epoch 1490/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3157 - accuracy: 0.0825\n",
      "Epoch 1490: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3141 - accuracy: 0.0821 - val_loss: 3.2915 - val_accuracy: 0.0851\n",
      "Epoch 1491/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0822\n",
      "Epoch 1491: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3179 - accuracy: 0.0823 - val_loss: 3.2952 - val_accuracy: 0.0814\n",
      "Epoch 1492/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0814\n",
      "Epoch 1492: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3151 - accuracy: 0.0813 - val_loss: 3.2917 - val_accuracy: 0.0821\n",
      "Epoch 1493/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3105 - accuracy: 0.0809\n",
      "Epoch 1493: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3140 - accuracy: 0.0810 - val_loss: 3.2913 - val_accuracy: 0.0808\n",
      "Epoch 1494/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3160 - accuracy: 0.0818\n",
      "Epoch 1494: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3145 - accuracy: 0.0824 - val_loss: 3.2956 - val_accuracy: 0.0882\n",
      "Epoch 1495/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3133 - accuracy: 0.0811\n",
      "Epoch 1495: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0812 - val_loss: 3.2916 - val_accuracy: 0.0870\n",
      "Epoch 1496/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3169 - accuracy: 0.0817\n",
      "Epoch 1496: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3144 - accuracy: 0.0817 - val_loss: 3.2920 - val_accuracy: 0.0802\n",
      "Epoch 1497/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3135 - accuracy: 0.0809\n",
      "Epoch 1497: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3151 - accuracy: 0.0812 - val_loss: 3.2922 - val_accuracy: 0.0857\n",
      "Epoch 1498/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3165 - accuracy: 0.0834\n",
      "Epoch 1498: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3168 - accuracy: 0.0824 - val_loss: 3.2964 - val_accuracy: 0.0882\n",
      "Epoch 1499/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3158 - accuracy: 0.0831\n",
      "Epoch 1499: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3153 - accuracy: 0.0828 - val_loss: 3.2934 - val_accuracy: 0.0833\n",
      "Epoch 1500/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3146 - accuracy: 0.0832\n",
      "Epoch 1500: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0832 - val_loss: 3.2911 - val_accuracy: 0.0821\n",
      "Epoch 1501/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3136 - accuracy: 0.0829\n",
      "Epoch 1501: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3149 - accuracy: 0.0822 - val_loss: 3.2906 - val_accuracy: 0.0851\n",
      "Epoch 1502/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3122 - accuracy: 0.0814\n",
      "Epoch 1502: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3140 - accuracy: 0.0806 - val_loss: 3.2910 - val_accuracy: 0.0888\n",
      "Epoch 1503/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3134 - accuracy: 0.0821\n",
      "Epoch 1503: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3149 - accuracy: 0.0818 - val_loss: 3.2939 - val_accuracy: 0.0876\n",
      "Epoch 1504/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0821\n",
      "Epoch 1504: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3143 - accuracy: 0.0821 - val_loss: 3.2909 - val_accuracy: 0.0839\n",
      "Epoch 1505/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3105 - accuracy: 0.0835\n",
      "Epoch 1505: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3135 - accuracy: 0.0831 - val_loss: 3.2909 - val_accuracy: 0.0808\n",
      "Epoch 1506/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3115 - accuracy: 0.0829\n",
      "Epoch 1506: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3138 - accuracy: 0.0827 - val_loss: 3.2910 - val_accuracy: 0.0857\n",
      "Epoch 1507/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0828\n",
      "Epoch 1507: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3153 - accuracy: 0.0831 - val_loss: 3.2952 - val_accuracy: 0.0839\n",
      "Epoch 1508/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0801\n",
      "Epoch 1508: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3147 - accuracy: 0.0801 - val_loss: 3.2910 - val_accuracy: 0.0863\n",
      "Epoch 1509/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.0817\n",
      "Epoch 1509: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3150 - accuracy: 0.0815 - val_loss: 3.2915 - val_accuracy: 0.0845\n",
      "Epoch 1510/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0809\n",
      "Epoch 1510: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0809 - val_loss: 3.2916 - val_accuracy: 0.0796\n",
      "Epoch 1511/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3173 - accuracy: 0.0803\n",
      "Epoch 1511: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3152 - accuracy: 0.0805 - val_loss: 3.2934 - val_accuracy: 0.0814\n",
      "Epoch 1512/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0808\n",
      "Epoch 1512: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3144 - accuracy: 0.0806 - val_loss: 3.2918 - val_accuracy: 0.0827\n",
      "Epoch 1513/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3094 - accuracy: 0.0823\n",
      "Epoch 1513: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0817 - val_loss: 3.2921 - val_accuracy: 0.0839\n",
      "Epoch 1514/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0820\n",
      "Epoch 1514: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3145 - accuracy: 0.0813 - val_loss: 3.2913 - val_accuracy: 0.0863\n",
      "Epoch 1515/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3151 - accuracy: 0.0815\n",
      "Epoch 1515: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0818 - val_loss: 3.2945 - val_accuracy: 0.0814\n",
      "Epoch 1516/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3131 - accuracy: 0.0811\n",
      "Epoch 1516: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0813 - val_loss: 3.2919 - val_accuracy: 0.0808\n",
      "Epoch 1517/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0823\n",
      "Epoch 1517: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3140 - accuracy: 0.0818 - val_loss: 3.2936 - val_accuracy: 0.0857\n",
      "Epoch 1518/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3144 - accuracy: 0.0834\n",
      "Epoch 1518: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3145 - accuracy: 0.0828 - val_loss: 3.2931 - val_accuracy: 0.0870\n",
      "Epoch 1519/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3132 - accuracy: 0.0848\n",
      "Epoch 1519: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3142 - accuracy: 0.0840 - val_loss: 3.2933 - val_accuracy: 0.0808\n",
      "Epoch 1520/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0798\n",
      "Epoch 1520: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3143 - accuracy: 0.0798 - val_loss: 3.2910 - val_accuracy: 0.0851\n",
      "Epoch 1521/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0845\n",
      "Epoch 1521: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3139 - accuracy: 0.0845 - val_loss: 3.2912 - val_accuracy: 0.0876\n",
      "Epoch 1522/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3139 - accuracy: 0.0822\n",
      "Epoch 1522: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0823 - val_loss: 3.2913 - val_accuracy: 0.0821\n",
      "Epoch 1523/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3152 - accuracy: 0.0837\n",
      "Epoch 1523: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3152 - accuracy: 0.0837 - val_loss: 3.2947 - val_accuracy: 0.0808\n",
      "Epoch 1524/5000\n",
      "249/256 [============================>.] - ETA: 0s - loss: 3.3142 - accuracy: 0.0820\n",
      "Epoch 1524: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0818 - val_loss: 3.2919 - val_accuracy: 0.0814\n",
      "Epoch 1525/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3151 - accuracy: 0.0834\n",
      "Epoch 1525: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3143 - accuracy: 0.0828 - val_loss: 3.2913 - val_accuracy: 0.0857\n",
      "Epoch 1526/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3185 - accuracy: 0.0824\n",
      "Epoch 1526: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3155 - accuracy: 0.0817 - val_loss: 3.2975 - val_accuracy: 0.0857\n",
      "Epoch 1527/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0815\n",
      "Epoch 1527: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3149 - accuracy: 0.0813 - val_loss: 3.2915 - val_accuracy: 0.0827\n",
      "Epoch 1528/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3138 - accuracy: 0.0803\n",
      "Epoch 1528: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3136 - accuracy: 0.0807 - val_loss: 3.2961 - val_accuracy: 0.0851\n",
      "Epoch 1529/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3168 - accuracy: 0.0798\n",
      "Epoch 1529: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3153 - accuracy: 0.0804 - val_loss: 3.2949 - val_accuracy: 0.0833\n",
      "Epoch 1530/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3147 - accuracy: 0.0816\n",
      "Epoch 1530: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3151 - accuracy: 0.0811 - val_loss: 3.2920 - val_accuracy: 0.0827\n",
      "Epoch 1531/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3143 - accuracy: 0.0855\n",
      "Epoch 1531: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3143 - accuracy: 0.0849 - val_loss: 3.2913 - val_accuracy: 0.0888\n",
      "Epoch 1532/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3116 - accuracy: 0.0829\n",
      "Epoch 1532: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3142 - accuracy: 0.0816 - val_loss: 3.2911 - val_accuracy: 0.0833\n",
      "Epoch 1533/5000\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 3.3126 - accuracy: 0.0832\n",
      "Epoch 1533: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3158 - accuracy: 0.0824 - val_loss: 3.2945 - val_accuracy: 0.0894\n",
      "Epoch 1534/5000\n",
      "235/256 [==========================>...] - ETA: 0s - loss: 3.3152 - accuracy: 0.0842\n",
      "Epoch 1534: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3156 - accuracy: 0.0847 - val_loss: 3.2931 - val_accuracy: 0.0857\n",
      "Epoch 1535/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0854\n",
      "Epoch 1535: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3141 - accuracy: 0.0855 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 1536/5000\n",
      "242/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0799\n",
      "Epoch 1536: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 3.3140 - accuracy: 0.0820 - val_loss: 3.2908 - val_accuracy: 0.0857\n",
      "Epoch 1537/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0805\n",
      "Epoch 1537: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3140 - accuracy: 0.0807 - val_loss: 3.2923 - val_accuracy: 0.0845\n",
      "Epoch 1538/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3153 - accuracy: 0.0816\n",
      "Epoch 1538: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3143 - accuracy: 0.0820 - val_loss: 3.2932 - val_accuracy: 0.0857\n",
      "Epoch 1539/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3160 - accuracy: 0.0826\n",
      "Epoch 1539: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3160 - accuracy: 0.0826 - val_loss: 3.2936 - val_accuracy: 0.0857\n",
      "Epoch 1540/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3142 - accuracy: 0.0812\n",
      "Epoch 1540: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3141 - accuracy: 0.0811 - val_loss: 3.2916 - val_accuracy: 0.0833\n",
      "Epoch 1541/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3128 - accuracy: 0.0823\n",
      "Epoch 1541: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3146 - accuracy: 0.0824 - val_loss: 3.2938 - val_accuracy: 0.0857\n",
      "Epoch 1542/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3105 - accuracy: 0.0836\n",
      "Epoch 1542: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3145 - accuracy: 0.0828 - val_loss: 3.2913 - val_accuracy: 0.0857\n",
      "Epoch 1543/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3098 - accuracy: 0.0841\n",
      "Epoch 1543: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3146 - accuracy: 0.0837 - val_loss: 3.2914 - val_accuracy: 0.0870\n",
      "Epoch 1544/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3141 - accuracy: 0.0830\n",
      "Epoch 1544: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3141 - accuracy: 0.0829 - val_loss: 3.2932 - val_accuracy: 0.0827\n",
      "Epoch 1545/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3102 - accuracy: 0.0828\n",
      "Epoch 1545: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3135 - accuracy: 0.0827 - val_loss: 3.2913 - val_accuracy: 0.0876\n",
      "Epoch 1546/5000\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 3.3134 - accuracy: 0.0843\n",
      "Epoch 1546: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3151 - accuracy: 0.0842 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 1547/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3144 - accuracy: 0.0822\n",
      "Epoch 1547: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0822 - val_loss: 3.2918 - val_accuracy: 0.0857\n",
      "Epoch 1548/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0815\n",
      "Epoch 1548: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3158 - accuracy: 0.0815 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 1549/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3154 - accuracy: 0.0829\n",
      "Epoch 1549: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3150 - accuracy: 0.0827 - val_loss: 3.2922 - val_accuracy: 0.0870\n",
      "Epoch 1550/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3131 - accuracy: 0.0822\n",
      "Epoch 1550: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0820 - val_loss: 3.2932 - val_accuracy: 0.0863\n",
      "Epoch 1551/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0822\n",
      "Epoch 1551: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3141 - accuracy: 0.0818 - val_loss: 3.2914 - val_accuracy: 0.0839\n",
      "Epoch 1552/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0817\n",
      "Epoch 1552: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3142 - accuracy: 0.0817 - val_loss: 3.2917 - val_accuracy: 0.0845\n",
      "Epoch 1553/5000\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 3.3179 - accuracy: 0.0817\n",
      "Epoch 1553: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3139 - accuracy: 0.0823 - val_loss: 3.2919 - val_accuracy: 0.0845\n",
      "Epoch 1554/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3155 - accuracy: 0.0797\n",
      "Epoch 1554: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3140 - accuracy: 0.0815 - val_loss: 3.2916 - val_accuracy: 0.0857\n",
      "Epoch 1555/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3151 - accuracy: 0.0814\n",
      "Epoch 1555: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3148 - accuracy: 0.0815 - val_loss: 3.2906 - val_accuracy: 0.0882\n",
      "Epoch 1556/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3136 - accuracy: 0.0818\n",
      "Epoch 1556: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 1s 6ms/step - loss: 3.3141 - accuracy: 0.0813 - val_loss: 3.2928 - val_accuracy: 0.0857\n",
      "Epoch 1557/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3142 - accuracy: 0.0816\n",
      "Epoch 1557: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3137 - accuracy: 0.0824 - val_loss: 3.2924 - val_accuracy: 0.0863\n",
      "Epoch 1558/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3153 - accuracy: 0.0828\n",
      "Epoch 1558: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3151 - accuracy: 0.0828 - val_loss: 3.2920 - val_accuracy: 0.0876\n",
      "Epoch 1559/5000\n",
      "239/256 [===========================>..] - ETA: 0s - loss: 3.3131 - accuracy: 0.0822\n",
      "Epoch 1559: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3138 - accuracy: 0.0832 - val_loss: 3.2928 - val_accuracy: 0.0882\n",
      "Epoch 1560/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3140 - accuracy: 0.0812\n",
      "Epoch 1560: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3151 - accuracy: 0.0816 - val_loss: 3.2941 - val_accuracy: 0.0870\n",
      "Epoch 1561/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3134 - accuracy: 0.0797\n",
      "Epoch 1561: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0806 - val_loss: 3.2911 - val_accuracy: 0.0906\n",
      "Epoch 1562/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3159 - accuracy: 0.0819\n",
      "Epoch 1562: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3147 - accuracy: 0.0815 - val_loss: 3.2932 - val_accuracy: 0.0839\n",
      "Epoch 1563/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0817\n",
      "Epoch 1563: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3145 - accuracy: 0.0817 - val_loss: 3.2960 - val_accuracy: 0.0888\n",
      "Epoch 1564/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0827\n",
      "Epoch 1564: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3146 - accuracy: 0.0827 - val_loss: 3.2909 - val_accuracy: 0.0863\n",
      "Epoch 1565/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3162 - accuracy: 0.0795\n",
      "Epoch 1565: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3153 - accuracy: 0.0799 - val_loss: 3.2926 - val_accuracy: 0.0833\n",
      "Epoch 1566/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3138 - accuracy: 0.0837\n",
      "Epoch 1566: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3138 - accuracy: 0.0837 - val_loss: 3.2905 - val_accuracy: 0.0863\n",
      "Epoch 1567/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0830\n",
      "Epoch 1567: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 3.3141 - accuracy: 0.0823 - val_loss: 3.2901 - val_accuracy: 0.0882\n",
      "Epoch 1568/5000\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 3.3172 - accuracy: 0.0789\n",
      "Epoch 1568: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0802 - val_loss: 3.2901 - val_accuracy: 0.0845\n",
      "Epoch 1569/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0819\n",
      "Epoch 1569: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3156 - accuracy: 0.0824 - val_loss: 3.2912 - val_accuracy: 0.0857\n",
      "Epoch 1570/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3140 - accuracy: 0.0829\n",
      "Epoch 1570: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3140 - accuracy: 0.0829 - val_loss: 3.2925 - val_accuracy: 0.0814\n",
      "Epoch 1571/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3096 - accuracy: 0.0837\n",
      "Epoch 1571: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3143 - accuracy: 0.0820 - val_loss: 3.2923 - val_accuracy: 0.0821\n",
      "Epoch 1572/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0808\n",
      "Epoch 1572: loss did not improve from 3.31353\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3145 - accuracy: 0.0805 - val_loss: 3.2901 - val_accuracy: 0.0888\n",
      "Epoch 1573/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3123 - accuracy: 0.0817\n",
      "Epoch 1573: loss improved from 3.31353 to 3.31329, saving model to best_model.h5\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3133 - accuracy: 0.0818 - val_loss: 3.2901 - val_accuracy: 0.0870\n",
      "Epoch 1574/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0819\n",
      "Epoch 1574: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3159 - accuracy: 0.0827 - val_loss: 3.2920 - val_accuracy: 0.0888\n",
      "Epoch 1575/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3140 - accuracy: 0.0822\n",
      "Epoch 1575: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3139 - accuracy: 0.0826 - val_loss: 3.2900 - val_accuracy: 0.0888\n",
      "Epoch 1576/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3149 - accuracy: 0.0836\n",
      "Epoch 1576: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3150 - accuracy: 0.0828 - val_loss: 3.2917 - val_accuracy: 0.0876\n",
      "Epoch 1577/5000\n",
      "253/256 [============================>.] - ETA: 0s - loss: 3.3133 - accuracy: 0.0828\n",
      "Epoch 1577: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 3.3137 - accuracy: 0.0827 - val_loss: 3.3450 - val_accuracy: 0.0839\n",
      "Epoch 1578/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3175 - accuracy: 0.0826\n",
      "Epoch 1578: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3169 - accuracy: 0.0826 - val_loss: 3.2915 - val_accuracy: 0.0894\n",
      "Epoch 1579/5000\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 3.3157 - accuracy: 0.0814\n",
      "Epoch 1579: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3148 - accuracy: 0.0817 - val_loss: 3.2925 - val_accuracy: 0.0845\n",
      "Epoch 1580/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3143 - accuracy: 0.0842\n",
      "Epoch 1580: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3137 - accuracy: 0.0837 - val_loss: 3.2914 - val_accuracy: 0.0876\n",
      "Epoch 1581/5000\n",
      "250/256 [============================>.] - ETA: 0s - loss: 3.3123 - accuracy: 0.0801\n",
      "Epoch 1581: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3136 - accuracy: 0.0798 - val_loss: 3.2914 - val_accuracy: 0.0876\n",
      "Epoch 1582/5000\n",
      "248/256 [============================>.] - ETA: 0s - loss: 3.3156 - accuracy: 0.0829\n",
      "Epoch 1582: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3144 - accuracy: 0.0821 - val_loss: 3.2917 - val_accuracy: 0.0882\n",
      "Epoch 1583/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3140 - accuracy: 0.0818\n",
      "Epoch 1583: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3140 - accuracy: 0.0818 - val_loss: 3.2915 - val_accuracy: 0.0851\n",
      "Epoch 1584/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3143 - accuracy: 0.0810\n",
      "Epoch 1584: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 3.3149 - accuracy: 0.0817 - val_loss: 3.2951 - val_accuracy: 0.0851\n",
      "Epoch 1585/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3138 - accuracy: 0.0830\n",
      "Epoch 1585: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 3.3145 - accuracy: 0.0831 - val_loss: 3.2911 - val_accuracy: 0.0863\n",
      "Epoch 1586/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3135 - accuracy: 0.0821\n",
      "Epoch 1586: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 3.3135 - accuracy: 0.0821 - val_loss: 3.2929 - val_accuracy: 0.0851\n",
      "Epoch 1587/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3163 - accuracy: 0.0830\n",
      "Epoch 1587: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.3154 - accuracy: 0.0837 - val_loss: 3.2951 - val_accuracy: 0.0876\n",
      "Epoch 1588/5000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0823\n",
      "Epoch 1588: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3146 - accuracy: 0.0824 - val_loss: 3.2916 - val_accuracy: 0.0833\n",
      "Epoch 1589/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3145 - accuracy: 0.0841\n",
      "Epoch 1589: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 10s 40ms/step - loss: 3.3144 - accuracy: 0.0840 - val_loss: 3.2926 - val_accuracy: 0.0851\n",
      "Epoch 1590/5000\n",
      "252/256 [============================>.] - ETA: 0s - loss: 3.3149 - accuracy: 0.0841\n",
      "Epoch 1590: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 3.3158 - accuracy: 0.0839 - val_loss: 3.2948 - val_accuracy: 0.0839\n",
      "Epoch 1591/5000\n",
      "256/256 [==============================] - ETA: 0s - loss: 3.3146 - accuracy: 0.0812\n",
      "Epoch 1591: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 9s 35ms/step - loss: 3.3146 - accuracy: 0.0812 - val_loss: 3.2921 - val_accuracy: 0.0839\n",
      "Epoch 1592/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3144 - accuracy: 0.0816\n",
      "Epoch 1592: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 9s 34ms/step - loss: 3.3148 - accuracy: 0.0816 - val_loss: 3.2981 - val_accuracy: 0.0851\n",
      "Epoch 1593/5000\n",
      "251/256 [============================>.] - ETA: 0s - loss: 3.3148 - accuracy: 0.0829\n",
      "Epoch 1593: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 3.3147 - accuracy: 0.0826 - val_loss: 3.2918 - val_accuracy: 0.0833\n",
      "Epoch 1594/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3110 - accuracy: 0.0837\n",
      "Epoch 1594: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3144 - accuracy: 0.0824 - val_loss: 3.2916 - val_accuracy: 0.0851\n",
      "Epoch 1595/5000\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 3.3161 - accuracy: 0.0818\n",
      "Epoch 1595: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3148 - accuracy: 0.0821 - val_loss: 3.2915 - val_accuracy: 0.0900\n",
      "Epoch 1596/5000\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 3.3139 - accuracy: 0.0829\n",
      "Epoch 1596: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3141 - accuracy: 0.0832 - val_loss: 3.2935 - val_accuracy: 0.0906\n",
      "Epoch 1597/5000\n",
      "241/256 [===========================>..] - ETA: 0s - loss: 3.3139 - accuracy: 0.0801\n",
      "Epoch 1597: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 6ms/step - loss: 3.3150 - accuracy: 0.0800 - val_loss: 3.2932 - val_accuracy: 0.0894\n",
      "Epoch 1598/5000\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 3.3175 - accuracy: 0.0837\n",
      "Epoch 1598: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 7ms/step - loss: 3.3141 - accuracy: 0.0834 - val_loss: 3.2912 - val_accuracy: 0.0857\n",
      "Epoch 1599/5000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.3157 - accuracy: 0.0814\n",
      "Epoch 1599: loss did not improve from 3.31329\n",
      "256/256 [==============================] - 2s 8ms/step - loss: 3.3158 - accuracy: 0.0815 - val_loss: 3.2933 - val_accuracy: 0.0857\n",
      "Epoch 1600/5000\n",
      " 65/256 [======>.......................] - ETA: 0s - loss: 3.3220 - accuracy: 0.0827"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\delat\\Documents\\Statistics\\Project\\CNN.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/delat/Documents/Statistics/Project/CNN.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/delat/Documents/Statistics/Project/CNN.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/delat/Documents/Statistics/Project/CNN.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m               loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/delat/Documents/Statistics/Project/CNN.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/delat/Documents/Statistics/Project/CNN.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/delat/Documents/Statistics/Project/CNN.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, callbacks \u001b[39m=\u001b[39;49m [model_cp])\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\delat\\miniconda3\\envs\\MLenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We cut out unnecesary samples\n",
    "X_train_null, X_test, y_train_null, y_test = train_test_split(data, beta_hot, test_size=0.2)\n",
    "# We train with the whole dataset\n",
    "X_train, y_train = data, beta_hot\n",
    "print('training with the whole dataset: {}'.format(y_train.shape))\n",
    "\n",
    "#es = EarlyStopping(monitor = 'loss', min_delta = 0.01, patience = 250, verbose = 1)\n",
    "model_cp = ModelCheckpoint(filepath = 'best_model{epoch}.h5', monitor = 'loss', save_best_only = True, verbose = 1, save_freq=100)\n",
    "\n",
    "model = models.Sequential()\n",
    "# one filter of 3x3\n",
    "model.add(layers.Conv2D(1, (3, 3), (2,2), padding='same', activation='relu', input_shape=(8, 8, 1)))\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(100, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5000, \n",
    "                    validation_data=(X_test, y_test), verbose = 1, callbacks = [model_cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filters, biases = model.layers[0].get_weights()\n",
    "print(filters.shape)\n",
    "file = open('filters.txt', 'w')\n",
    "for row in filters[:,:,0,0]:\n",
    "  np.savetxt(file, row)\n",
    "file.close()\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "im = ax.imshow(filters[:,:,0,0], cmap=mpl.colormaps['OrRd'])\n",
    "ax.axis('off')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.figtext(0.5, 0.01, \"$\\sum_{ij}F_{ij}=$\"+\"{:.5f}\".format(np.sum(filters[:,:,0,0])), ha=\"center\", fontsize=18, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filters_m, biases_m = model.layers[2].get_weights()\n",
    "print(filters_m.shape)\n",
    "file = open('weights.txt', 'w')\n",
    "for row in filters_m[:,:]:\n",
    "  np.savetxt(file, row)\n",
    "file.close()\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "im = ax.imshow(filters_m[:,20:], cmap=mpl.colormaps['gray'], origin='lower', aspect='auto') #clase 19 es el inicio del rango de temperatura y loanterior no hay samples \n",
    "ax.set_ylabel(r'$m$', fontsize=18)\n",
    "ax.set_xlabel(r'$I$', fontsize=18)\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8163,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import special\n",
    "W_sum = np.matmul(filters_m, beta_hot.transpose()).sum(axis=0)*10**-2\n",
    "print(W_sum.shape)\n",
    "def mag(bet, a, b, c, betacnn):\n",
    "  return a*np.tanh(c*(bet-betacnn))-b\n",
    "def erf(beta, a, b, c, betacnn):\n",
    "  return b + 0.5*a*(1 + special.erf(c*(beta-betacnn)))\n",
    "def exp(bet, a, b, c, betacnn):\n",
    "  return b + a*np.exp(c*(bet-betacnn))\n",
    "popt1, pcov1 = curve_fit(mag, beta[1837:], W_sum)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(beta[1837:], mag(beta[1837:], *popt1), 'r--',\n",
    "         label=r'$W_{sum}fit$: $\\beta_{cnn}$=%5.3f' % popt1[3])\n",
    "ax.scatter(beta[1837:], W_sum, marker='x', s=30, c='black', linewidths=0.09, label=r'$W_{sum}, L=$%i' % data.shape[1])\n",
    "ax.set_ylabel(r'$W_{sum}$', fontsize=18)\n",
    "ax.set_xlabel(r'$\\beta$', fontsize=18)\n",
    "plt.legend()\n",
    "#plt.ylim([-100, 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], c='b', label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], c = 'r', label='Val_accuracy')\n",
    "plt.title('Model accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], c='b', label='Loss')\n",
    "plt.plot(history.history['val_loss'], c = 'r', label='Val_loss')\n",
    "plt.title('Model loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os.path\n",
    "#if os.path.isfile('ising_model_10000_L8.h5') is False:\n",
    "#    model.save('ising_model_10000_L8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('MLenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34776b5c179a3366956eba0bd084d5567ae536015d6fb5aea4949c85288509e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
